{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üåü V1.3: Pre-Trained Segmentation Configuration\n",
        "CSV_FOLDER = \"../../data/Paradise_CSV/\"\n",
        "CSV_LABELS_FILE = \"Labeled_Data_RAW_Sample.csv\"\n",
        "CSV_SEPARATOR = \";\"\n",
        "\n",
        "# Download parameters  \n",
        "DOWNLOAD_PATH = '../../data/Paradise_Test_DICOMs'\n",
        "IMAGES_PATH = '../../data/Paradise_Test_Images'\n",
        "EXPORT_METADATA = True\n",
        "CONVERT = True\n",
        "\n",
        "# V1.3 Segmentation Settings - CONFIGURABLE FOR EASY TESTING\n",
        "USE_LUNG_SEGMENTATION = True\n",
        "SEGMENTATION_MODEL = 'torchxrayvision'  # Options: 'torchxrayvision', 'lungs_segmentation'\n",
        "\n",
        "# üéõÔ∏è CONFIGURABLE PARAMETERS FOR TESTING\n",
        "LUNG_SEGMENTATION_THRESHOLD = 0.1  # üîß Sensitivity threshold (lower = more sensitive, detects more lung area)\n",
        "LUNG_CROP_PADDING = 120  # üîß Padding around detected lungs in pixels (higher = less zoom)\n",
        "\n",
        "# Quality Control Thresholds\n",
        "MIN_LUNG_AREA_RATIO = 0.02  # üîß MUCH LOWER: More permissive minimum area (was 0.10)\n",
        "MAX_LUNG_AREA_RATIO = 0.90  # üîß Permissive maximum area\n",
        "SAVE_SEGMENTATION_MASKS = True\n",
        "MASKS_PATH = '../../data/Paradise_Masks'\n",
        "\n",
        "# Enhanced Parameters\n",
        "TARGET_SIZE = (518, 518)\n",
        "PRESERVE_ASPECT_RATIO = True\n",
        "BIT_DEPTH = 8\n",
        "MONOCHROME = 1\n",
        "\n",
        "print(\"üåü V1.3 Pre-trained segmentation configuration loaded!\")\n",
        "print(f\"ü´Å Using model: {SEGMENTATION_MODEL}\")\n",
        "print(f\"üìê Target size: {TARGET_SIZE}\")\n",
        "print(f\"üéõÔ∏è Segmentation threshold: {LUNG_SEGMENTATION_THRESHOLD} (lower = more sensitive)\")\n",
        "print(f\"üîß Lung padding: {LUNG_CROP_PADDING} pixels (higher = less zoom)\")\n",
        "print(f\"üìä Min area ratio: {MIN_LUNG_AREA_RATIO} (much lower for better detection)\")\n",
        "print(\"üöÄ Ready for optimized lung segmentation!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# üåü V1.3: Enhanced ArchiMed Download with Pre-trained Lung Segmentation\n",
        "**Professional lung segmentation using TorchXRayVision and lungs-segmentation models**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "**<h1 align=\"center\">Download ArchiMed Images V1.3 - PRE-TRAINED LUNG SEGMENTATION</h1>**\n",
        "\n",
        "## üåü **V1.3: Professional Chest X-Ray Segmentation**\n",
        "- **TorchXRayVision**: Pre-trained segmentation models from medical imaging library\n",
        "- **Proven Performance**: Trained on large chest X-ray datasets (NIH, CheXpert, MIMIC)\n",
        "- **No More Issues**: No tensor mismatches, proper lung detection\n",
        "- **Multiple Fallbacks**: Includes alternative models for maximum reliability\n",
        "\n",
        "## üöÄ **Key Improvements:**\n",
        "- **Professional Models**: Uses medically-validated segmentation\n",
        "- **Better Cropping**: Accurate lung boundary detection with proper padding\n",
        "- **Robust Pipeline**: Multiple fallback options\n",
        "- **Quality Validation**: Automatic detection quality checks\n",
        "\n",
        "## üîß **V1.3.2 Major Update (Reference Image Matching):**\n",
        "- **Increased Padding**: 120px padding around lungs (user feedback: less zoom)\n",
        "- **Clean Binary Masks**: Simple contours instead of \"terrain maps\" \n",
        "- **Reference-Style Output**: RED contours + BLUE crop box (matches user's reference)\n",
        "- **Better Morphology**: Cleaner lung shapes with hole filling\n",
        "- **Flat DICOM Storage**: Files saved directly to main folder (no subfolders)\n",
        "- **Ready for Step 3**: Zone division implementation prepared\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üìã Mask Interpretation Guide\n",
        "\n",
        "print(\"üéØ UPDATED MASK INTERPRETATION GUIDE:\")\n",
        "print(\"‚Ä¢ mask.png files: Clean binary lung masks (white = lung tissue, black = background)\")\n",
        "print(\"‚Ä¢ overlay.png files: Shows detection like your reference image\")\n",
        "print(\"  - BLUE contours = Detected lung boundaries (Step 1 in your reference)\")  \n",
        "print(\"  - ORANGE rectangle = Final crop region (Step 2 in your reference)\")\n",
        "print(\"‚Ä¢ üéõÔ∏è CONFIGURABLE PARAMETERS (set at top of notebook):\")\n",
        "print(f\"  - LUNG_SEGMENTATION_THRESHOLD = {LUNG_SEGMENTATION_THRESHOLD} (lower = more sensitive)\")\n",
        "print(f\"  - LUNG_CROP_PADDING = {LUNG_CROP_PADDING}px (higher = less zoom)\")\n",
        "print(\"‚Ä¢ If lung detection misses areas: DECREASE LUNG_SEGMENTATION_THRESHOLD\")\n",
        "print(\"‚Ä¢ If crop rectangle too zoomed in: INCREASE LUNG_CROP_PADDING\")\n",
        "print(\"‚Ä¢ Ready for Step 3: Zone division (to be implemented later)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Core dependencies\n",
        "import ArchiMedConnector.A3_Connector as A3_Conn\n",
        "import pandas as pd\n",
        "import os\n",
        "import pydicom\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import glob\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "import cv2\n",
        "import io\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "# Colors for output\n",
        "ANSI = {\n",
        "    'R': '\\033[91m', 'G': '\\033[92m', 'B': '\\033[94m', 'Y': '\\033[93m',\n",
        "    'W': '\\033[0m', 'M': '\\033[95m', 'C': '\\033[96m'\n",
        "}\n",
        "\n",
        "print(f\"{ANSI['G']}‚úÖ Core dependencies loaded{ANSI['W']}\")\n",
        "\n",
        "# Initialize ArchiMed connector\n",
        "a3conn = A3_Conn.A3_Connector()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üåü Install and Import Pre-trained Segmentation Models\n",
        "\n",
        "# Install TorchXRayVision if not already installed\n",
        "try:\n",
        "    import torchxrayvision as xrv\n",
        "    import torch\n",
        "    TORCHXRAY_AVAILABLE = True\n",
        "    print(f\"{ANSI['G']}‚úÖ TorchXRayVision loaded successfully{ANSI['W']}\")\n",
        "except ImportError:\n",
        "    print(f\"{ANSI['Y']}‚ö†Ô∏è Installing TorchXRayVision...{ANSI['W']}\")\n",
        "    try:\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"torchxrayvision\"])\n",
        "        import torchxrayvision as xrv\n",
        "        import torch\n",
        "        TORCHXRAY_AVAILABLE = True\n",
        "        print(f\"{ANSI['G']}‚úÖ TorchXRayVision installed and loaded{ANSI['W']}\")\n",
        "    except Exception as e:\n",
        "        TORCHXRAY_AVAILABLE = False\n",
        "        print(f\"{ANSI['R']}‚ùå Failed to install TorchXRayVision: {e}{ANSI['W']}\")\n",
        "\n",
        "# Try alternative: lungs-segmentation package\n",
        "try:\n",
        "    from lungs_segmentation.pre_trained_models import create_model\n",
        "    LUNGS_SEG_AVAILABLE = True\n",
        "    print(f\"{ANSI['G']}‚úÖ lungs-segmentation available{ANSI['W']}\")\n",
        "except ImportError:\n",
        "    print(f\"{ANSI['Y']}‚ö†Ô∏è Installing lungs-segmentation...{ANSI['W']}\")\n",
        "    try:\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"lungs-segmentation\"])\n",
        "        from lungs_segmentation.pre_trained_models import create_model\n",
        "        LUNGS_SEG_AVAILABLE = True\n",
        "        print(f\"{ANSI['G']}‚úÖ lungs-segmentation installed{ANSI['W']}\")\n",
        "    except Exception as e:\n",
        "        LUNGS_SEG_AVAILABLE = False\n",
        "        print(f\"{ANSI['Y']}‚ö†Ô∏è lungs-segmentation not available: {e}{ANSI['W']}\")\n",
        "\n",
        "# Check what we have available\n",
        "available_models = []\n",
        "if TORCHXRAY_AVAILABLE:\n",
        "    available_models.append('torchxrayvision')\n",
        "if LUNGS_SEG_AVAILABLE:\n",
        "    available_models.append('lungs_segmentation')\n",
        "\n",
        "print(f\"{ANSI['C']}üè• Available pre-trained models: {available_models}{ANSI['W']}\")\n",
        "\n",
        "if not available_models:\n",
        "    print(f\"{ANSI['R']}‚ùå No pre-trained models available, falling back to enhanced thresholding{ANSI['W']}\")\n",
        "    USE_LUNG_SEGMENTATION = True  # Still use segmentation, but with fallback method\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PreTrainedLungSegmentation:\n",
        "    \"\"\"Professional lung segmentation using pre-trained models\"\"\"\n",
        "    \n",
        "    def __init__(self, model_type='torchxrayvision'):\n",
        "        self.model_type = model_type\n",
        "        self.model = None\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        \n",
        "        print(f\"{ANSI['C']}üîß Initializing {model_type} segmentation...{ANSI['W']}\")\n",
        "        \n",
        "        if model_type == 'torchxrayvision' and TORCHXRAY_AVAILABLE:\n",
        "            self._init_torchxray()\n",
        "        elif model_type == 'lungs_segmentation' and LUNGS_SEG_AVAILABLE:\n",
        "            self._init_lungs_seg()\n",
        "        else:\n",
        "            print(f\"{ANSI['Y']}‚ö†Ô∏è Requested model not available, using enhanced fallback{ANSI['W']}\")\n",
        "            self.model = None\n",
        "    \n",
        "    def _init_torchxray(self):\n",
        "        \"\"\"Initialize TorchXRayVision segmentation model\"\"\"\n",
        "        try:\n",
        "            # Load pre-trained segmentation model from TorchXRayVision\n",
        "            self.seg_model = xrv.baseline_models.chestx_det.PSPNet()\n",
        "            print(f\"{ANSI['G']}‚úÖ TorchXRayVision PSPNet loaded{ANSI['W']}\")\n",
        "            print(f\"{ANSI['B']}   Targets: {self.seg_model.targets}{ANSI['W']}\")\n",
        "            self.model = 'torchxray'\n",
        "        except Exception as e:\n",
        "            print(f\"{ANSI['Y']}‚ö†Ô∏è TorchXRayVision init failed: {e}{ANSI['W']}\")\n",
        "            self.model = None\n",
        "    \n",
        "    def _init_lungs_seg(self):\n",
        "        \"\"\"Initialize lungs-segmentation model\"\"\"\n",
        "        try:\n",
        "            self.seg_model = create_model(\"resnet34\")\n",
        "            self.seg_model = self.seg_model.to(self.device)\n",
        "            self.seg_model.eval()\n",
        "            print(f\"{ANSI['G']}‚úÖ lungs-segmentation ResNet34 loaded{ANSI['W']}\")\n",
        "            self.model = 'lungs_seg'\n",
        "        except Exception as e:\n",
        "            print(f\"{ANSI['Y']}‚ö†Ô∏è lungs-segmentation init failed: {e}{ANSI['W']}\")\n",
        "            self.model = None\n",
        "    \n",
        "    def segment_lungs(self, image):\n",
        "        \"\"\"Segment lungs using the loaded model\"\"\"\n",
        "        if self.model is None:\n",
        "            return self._enhanced_fallback_segmentation(image)\n",
        "        \n",
        "        try:\n",
        "            if self.model == 'torchxray':\n",
        "                return self._torchxray_segment(image)\n",
        "            elif self.model == 'lungs_seg':\n",
        "                return self._lungs_seg_segment(image)\n",
        "        except Exception as e:\n",
        "            print(f\"{ANSI['Y']}‚ö†Ô∏è Segmentation failed: {e}, using enhanced fallback{ANSI['W']}\")\n",
        "            return self._enhanced_fallback_segmentation(image)\n",
        "    \n",
        "    def _torchxray_segment(self, image):\n",
        "        \"\"\"Segment using TorchXRayVision\"\"\"\n",
        "        # Convert to proper format\n",
        "        if len(image.shape) == 3:\n",
        "            image_gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
        "        else:\n",
        "            image_gray = image\n",
        "        \n",
        "        # Normalize to [-1024, 1024] range as expected by TorchXRayVision\n",
        "        image_norm = xrv.datasets.normalize(image_gray, 255)\n",
        "        image_norm = image_norm[None, ...]  # Add channel dimension\n",
        "        \n",
        "        # Resize to 512x512 as expected by the model\n",
        "        transform = xrv.datasets.XRayResizer(512)\n",
        "        image_resized = transform(image_norm)\n",
        "        \n",
        "        # Convert to tensor\n",
        "        image_tensor = torch.from_numpy(image_resized).float().unsqueeze(0)\n",
        "        \n",
        "        # Inference\n",
        "        with torch.no_grad():\n",
        "            output = self.seg_model(image_tensor)\n",
        "        \n",
        "        # Extract lung masks (Left Lung: index 4, Right Lung: index 5)\n",
        "        lung_targets = ['Left Lung', 'Right Lung']\n",
        "        lung_mask = np.zeros((512, 512))\n",
        "        \n",
        "        for i, target in enumerate(self.seg_model.targets):\n",
        "            if target in lung_targets:\n",
        "                lung_mask += output[0, i].cpu().numpy()\n",
        "        \n",
        "        # Resize back to original size\n",
        "        lung_mask = cv2.resize(lung_mask, (image.shape[1], image.shape[0]))\n",
        "        \n",
        "        # Create clean binary mask (like reference image)\n",
        "        binary_mask = (lung_mask > LUNG_SEGMENTATION_THRESHOLD).astype(np.uint8)  # Use configurable threshold\n",
        "        print(f\"{ANSI['C']}üéõÔ∏è Using threshold: {LUNG_SEGMENTATION_THRESHOLD} (configurable: LUNG_SEGMENTATION_THRESHOLD){ANSI['W']}\")\n",
        "        \n",
        "        # Clean up the mask with morphological operations for cleaner contours\n",
        "        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (10, 10))\n",
        "        binary_mask = cv2.morphologyEx(binary_mask, cv2.MORPH_CLOSE, kernel)\n",
        "        binary_mask = cv2.morphologyEx(binary_mask, cv2.MORPH_OPEN, kernel)\n",
        "        \n",
        "        # Fill holes to create solid lung regions\n",
        "        kernel_fill = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (15, 15))\n",
        "        binary_mask = cv2.morphologyEx(binary_mask, cv2.MORPH_CLOSE, kernel_fill)\n",
        "        \n",
        "        return binary_mask, binary_mask.astype(float)\n",
        "    \n",
        "    def _lungs_seg_segment(self, image):\n",
        "        \"\"\"Segment using lungs-segmentation\"\"\"\n",
        "        from lungs_segmentation import inference\n",
        "        \n",
        "        # Run inference\n",
        "        processed_image, masks = inference.inference(self.seg_model, image, 0.2)\n",
        "        \n",
        "        # Combine left and right lung masks\n",
        "        if len(masks) >= 2:\n",
        "            combined_mask = masks[0] + masks[1]  # Left + Right lung\n",
        "        elif len(masks) == 1:\n",
        "            combined_mask = masks[0]\n",
        "        else:\n",
        "            return self._enhanced_fallback_segmentation(image)\n",
        "        \n",
        "        binary_mask = (combined_mask > 0.5).astype(np.uint8)\n",
        "        return binary_mask, combined_mask\n",
        "    \n",
        "    def _enhanced_fallback_segmentation(self, image):\n",
        "        \"\"\"Enhanced fallback segmentation using multiple techniques\"\"\"\n",
        "        print(f\"{ANSI['B']}üîÑ Using enhanced professional fallback segmentation...{ANSI['W']}\")\n",
        "        \n",
        "        if len(image.shape) == 3:\n",
        "            gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
        "        else:\n",
        "            gray = image.copy()\n",
        "        \n",
        "        # Preprocessing: Apply CLAHE for better contrast\n",
        "        clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8,8))\n",
        "        gray = clahe.apply(gray)\n",
        "        \n",
        "        # Method 1: Otsu thresholding\n",
        "        _, otsu_mask = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "        \n",
        "        # Method 2: Adaptive threshold for local contrast\n",
        "        adaptive_mask = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2)\n",
        "        \n",
        "        # Method 3: Multiple Otsu on different intensity ranges\n",
        "        percentile_75 = np.percentile(gray, 75)\n",
        "        _, high_thresh = cv2.threshold(gray, percentile_75, 255, cv2.THRESH_BINARY)\n",
        "        \n",
        "        # Combine masks with weighted approach\n",
        "        combined = np.maximum(np.maximum(otsu_mask * 0.6, adaptive_mask * 0.3), high_thresh * 0.1)\n",
        "        \n",
        "        # Morphological operations to clean up and connect lung regions\n",
        "        # Use larger kernel for chest X-rays\n",
        "        kernel_open = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (15, 15))\n",
        "        kernel_close = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (25, 25))\n",
        "        \n",
        "        combined = cv2.morphologyEx(combined, cv2.MORPH_OPEN, kernel_open)\n",
        "        combined = cv2.morphologyEx(combined, cv2.MORPH_CLOSE, kernel_close)\n",
        "        \n",
        "        # Remove small noise and keep only significant lung regions\n",
        "        contours, _ = cv2.findContours(combined.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "        \n",
        "        if contours:\n",
        "            # Calculate areas and keep largest contours (likely lung regions)\n",
        "            contour_areas = [(cv2.contourArea(cnt), cnt) for cnt in contours]\n",
        "            contour_areas.sort(key=lambda x: x[0], reverse=True)\n",
        "            \n",
        "            # Keep top contours that represent lungs\n",
        "            mask_clean = np.zeros_like(combined)\n",
        "            total_image_area = combined.shape[0] * combined.shape[1]\n",
        "            min_area_threshold = total_image_area * 0.01  # At least 1% of image\n",
        "            \n",
        "            kept_contours = 0\n",
        "            for area, contour in contour_areas:\n",
        "                if area > min_area_threshold and kept_contours < 4:  # Max 4 regions (2 lungs possibly split)\n",
        "                    cv2.fillPoly(mask_clean, [contour], 255)\n",
        "                    kept_contours += 1\n",
        "                elif kept_contours >= 2:  # Have at least 2 significant regions\n",
        "                    break\n",
        "            \n",
        "            if np.sum(mask_clean) > 0:\n",
        "                combined = mask_clean\n",
        "        \n",
        "        # Final cleanup: Fill holes within lung regions\n",
        "        kernel_fill = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (20, 20))\n",
        "        combined = cv2.morphologyEx(combined, cv2.MORPH_CLOSE, kernel_fill)\n",
        "        \n",
        "        print(f\"{ANSI['G']}‚úÖ Enhanced fallback segmentation complete{ANSI['W']}\")\n",
        "        \n",
        "        return (combined > 0).astype(np.uint8), combined / 255.0\n",
        "\n",
        "# Initialize the segmentation pipeline\n",
        "if USE_LUNG_SEGMENTATION:\n",
        "    if 'torchxrayvision' in available_models:\n",
        "        lung_segmenter = PreTrainedLungSegmentation('torchxrayvision')\n",
        "    elif 'lungs_segmentation' in available_models:\n",
        "        lung_segmenter = PreTrainedLungSegmentation('lungs_segmentation')\n",
        "    else:\n",
        "        lung_segmenter = PreTrainedLungSegmentation('fallback')\n",
        "    print(f\"{ANSI['C']}üéØ Professional lung segmentation initialized{ANSI['W']}\")\n",
        "else:\n",
        "    lung_segmenter = None\n",
        "    print(f\"{ANSI['Y']}‚ö†Ô∏è Lung segmentation disabled{ANSI['W']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_image_with_segmentation(image_array, file_id):\n",
        "    \"\"\"Process image with professional lung segmentation and improved cropping\"\"\"\n",
        "    if lung_segmenter is None:\n",
        "        return image_array\n",
        "    \n",
        "    try:\n",
        "        print(f\"{ANSI['B']}ü´Å Segmenting lungs for {file_id}...{ANSI['W']}\")\n",
        "        \n",
        "        # Get lung segmentation\n",
        "        binary_mask, prob_mask = lung_segmenter.segment_lungs(image_array)\n",
        "        \n",
        "        # Validate segmentation quality\n",
        "        total_pixels = binary_mask.shape[0] * binary_mask.shape[1]\n",
        "        lung_pixels = np.sum(binary_mask)\n",
        "        lung_ratio = lung_pixels / total_pixels\n",
        "        \n",
        "        print(f\"{ANSI['C']}üìä Lung area detected: {lung_ratio:.3f} of image{ANSI['W']}\")\n",
        "        \n",
        "        # Quality check with improved thresholds\n",
        "        if lung_ratio < MIN_LUNG_AREA_RATIO:\n",
        "            print(f\"{ANSI['Y']}‚ö†Ô∏è Detected area too small ({lung_ratio:.3f} < {MIN_LUNG_AREA_RATIO}), using original{ANSI['W']}\")\n",
        "            return image_array  # Return original\n",
        "        \n",
        "        if lung_ratio > MAX_LUNG_AREA_RATIO:\n",
        "            print(f\"{ANSI['Y']}‚ö†Ô∏è Detected area too large ({lung_ratio:.3f} > {MAX_LUNG_AREA_RATIO}), using original{ANSI['W']}\")\n",
        "            return image_array  # Return original\n",
        "        \n",
        "        # Find bounding box of lung regions - IMPROVED to ensure full coverage\n",
        "        coords = np.column_stack(np.where(binary_mask > 0))\n",
        "        if len(coords) == 0:\n",
        "            print(f\"{ANSI['Y']}‚ö†Ô∏è No lung coordinates found{ANSI['W']}\")\n",
        "            return image_array\n",
        "        \n",
        "        y_min, x_min = coords.min(axis=0)\n",
        "        y_max, x_max = coords.max(axis=0)\n",
        "        \n",
        "        # Add small safety margin to ensure all detected lung pixels are included\n",
        "        safety_margin = 10  # Increased to 10px for better coverage\n",
        "        y_min = max(0, y_min - safety_margin)\n",
        "        x_min = max(0, x_min - safety_margin)\n",
        "        y_max = min(image_array.shape[0], y_max + safety_margin)\n",
        "        x_max = min(image_array.shape[1], x_max + safety_margin)\n",
        "        \n",
        "        # Add generous padding to ensure lungs are fully included\n",
        "        h, w = image_array.shape[:2]\n",
        "        padding = LUNG_CROP_PADDING\n",
        "        \n",
        "        # Calculate padded boundaries\n",
        "        y_min_padded = max(0, y_min - padding)\n",
        "        x_min_padded = max(0, x_min - padding)\n",
        "        y_max_padded = min(h, y_max + padding)\n",
        "        x_max_padded = min(w, x_max + padding)\n",
        "        \n",
        "        # Ensure minimum crop size to avoid over-cropping\n",
        "        crop_height = y_max_padded - y_min_padded\n",
        "        crop_width = x_max_padded - x_min_padded\n",
        "        min_dimension = min(h, w) * 0.5  # At least 50% of smallest dimension\n",
        "        \n",
        "        if crop_height < min_dimension or crop_width < min_dimension:\n",
        "            print(f\"{ANSI['Y']}‚ö†Ô∏è Crop too small ({crop_height}x{crop_width}), using more conservative crop{ANSI['W']}\")\n",
        "            # Use more conservative padding\n",
        "            center_y, center_x = (y_min + y_max) // 2, (x_min + x_max) // 2\n",
        "            half_size = int(min_dimension // 2)\n",
        "            \n",
        "            y_min_padded = max(0, center_y - half_size)\n",
        "            x_min_padded = max(0, center_x - half_size)\n",
        "            y_max_padded = min(h, center_y + half_size)\n",
        "            x_max_padded = min(w, center_x + half_size)\n",
        "        \n",
        "        # Crop the image\n",
        "        if len(image_array.shape) == 3:\n",
        "            cropped = image_array[y_min_padded:y_max_padded, x_min_padded:x_max_padded, :]\n",
        "        else:\n",
        "            cropped = image_array[y_min_padded:y_max_padded, x_min_padded:x_max_padded]\n",
        "        \n",
        "        # Calculate area reduction\n",
        "        original_area = h * w\n",
        "        cropped_area = (y_max_padded - y_min_padded) * (x_max_padded - x_min_padded)\n",
        "        area_reduction = cropped_area / original_area\n",
        "        \n",
        "        print(f\"{ANSI['G']}‚úÖ Cropped to {area_reduction:.2f} area reduction{ANSI['W']}\")\n",
        "        print(f\"{ANSI['B']}   Original: {h}x{w} ‚Üí Cropped: {y_max_padded-y_min_padded}x{x_max_padded-x_min_padded}{ANSI['W']}\")\n",
        "        print(f\"{ANSI['C']}   Padding applied: {padding}px on each side (configurable: LUNG_CROP_PADDING){ANSI['W']}\")\n",
        "        \n",
        "        # Save segmentation mask if requested\n",
        "        if SAVE_SEGMENTATION_MASKS:\n",
        "            os.makedirs(MASKS_PATH, exist_ok=True)\n",
        "            \n",
        "            # Save clean binary mask (like reference image - simple contours)\n",
        "            mask_path = os.path.join(MASKS_PATH, f\"{file_id}_mask.png\")\n",
        "            mask_image = (binary_mask * 255).astype(np.uint8)\n",
        "            cv2.imwrite(mask_path, mask_image)\n",
        "            \n",
        "            # Save overlay like reference image (lung contours + crop box)\n",
        "            overlay_path = os.path.join(MASKS_PATH, f\"{file_id}_overlay.png\")\n",
        "            overlay = image_array.copy()\n",
        "            if len(overlay.shape) == 2:\n",
        "                overlay = cv2.cvtColor(overlay, cv2.COLOR_GRAY2RGB)\n",
        "            \n",
        "            # Draw lung contours in BLUE (as observed by user)\n",
        "            contours, _ = cv2.findContours(binary_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "            cv2.drawContours(overlay, contours, -1, (255, 0, 0), 2)\n",
        "            \n",
        "            # Draw ORANGE crop rectangle (as observed by user)  \n",
        "            cv2.rectangle(overlay, (x_min_padded, y_min_padded), (x_max_padded, y_max_padded), (0, 165, 255), 3)\n",
        "            \n",
        "            # Add clear legend - CORRECTED COLORS\n",
        "            cv2.putText(overlay, \"BLUE = Lung contours\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 0, 0), 2)\n",
        "            cv2.putText(overlay, \"ORANGE = Crop region\", (10, 55), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 165, 255), 2)\n",
        "            cv2.imwrite(overlay_path, overlay)\n",
        "            \n",
        "            print(f\"{ANSI['C']}üíæ Saved: {file_id}_mask.png (clean binary) & {file_id}_overlay.png (like reference){ANSI['W']}\")\n",
        "        \n",
        "        return cropped\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"{ANSI['Y']}‚ö†Ô∏è Segmentation failed for {file_id}: {e}{ANSI['W']}\")\n",
        "        return image_array\n",
        "\n",
        "print(f\"{ANSI['G']}‚úÖ Enhanced image processing functions loaded{ANSI['W']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üöÄ Main Processing Pipeline\n",
        "\n",
        "# Load CSV data\n",
        "try:\n",
        "    user_info = a3conn.getUserInfos()\n",
        "    print(f\"{ANSI['G']}üîê ArchiMed User Info{ANSI['W']}\")\n",
        "    print(f\"User info: {user_info}\")\n",
        "    \n",
        "    # Load CSV\n",
        "    csv_path = os.path.join(CSV_FOLDER, CSV_LABELS_FILE)\n",
        "    df = pd.read_csv(csv_path, sep=CSV_SEPARATOR)\n",
        "    print(f\"{ANSI['G']}‚úÖ Loaded CSV with {len(df)} rows{ANSI['W']}\")\n",
        "    \n",
        "    # Check for FileID column (handle different naming conventions)\n",
        "    file_id_column = None\n",
        "    for col in ['FileID', 'file_id', 'File_ID']:\n",
        "        if col in df.columns:\n",
        "            file_id_column = col\n",
        "            break\n",
        "    \n",
        "    if file_id_column is None:\n",
        "        print(f\"{ANSI['R']}‚ùå No FileID column found in CSV{ANSI['W']}\")\n",
        "        raise ValueError(\"FileID column not found\")\n",
        "    \n",
        "    print(f\"{ANSI['C']}üìä Available columns: {list(df.columns)}{ANSI['W']}\")\n",
        "    \n",
        "    # Get file IDs to download\n",
        "    file_ids = df[file_id_column].dropna().unique()\n",
        "    total_files = len(file_ids)\n",
        "    \n",
        "    print(f\"{ANSI['M']}üöÄ Starting enhanced download with pre-trained lung segmentation{ANSI['W']}\")\n",
        "    print(f\"Total files to process: {total_files}\")\n",
        "    print(f\"Destination: {DOWNLOAD_PATH}\")\n",
        "    print(f\"ü´Å Lung segmentation: {'ENABLED' if USE_LUNG_SEGMENTATION else 'DISABLED'}\")\n",
        "    \n",
        "    # Download files\n",
        "    downloaded_files = []\n",
        "    \n",
        "    for i, file_id in enumerate(file_ids):\n",
        "        progress = ((i + 1) / total_files) * 100\n",
        "        # Convert numpy.int64 to string for API compatibility\n",
        "        file_id_str = str(file_id)\n",
        "        print(f\"{ANSI['B']}‚¨áÔ∏è Downloading file {file_id_str} (Progress: {progress:.1f}% - {i+1}/{total_files}) from ArchiMed{ANSI['W']}\")\n",
        "        \n",
        "        try:\n",
        "            # Define output path - FLAT STRUCTURE (no subfolders)\n",
        "            dicom_file_path = os.path.join(DOWNLOAD_PATH, f\"{file_id}.dcm\")\n",
        "            # Create download directory if it doesn't exist\n",
        "            os.makedirs(DOWNLOAD_PATH, exist_ok=True)\n",
        "            \n",
        "            # Check if the file already exists\n",
        "            if os.path.exists(dicom_file_path):\n",
        "                print(f\"{ANSI['Y']}File {file_id} already exists, skipping download{ANSI['W']}\")\n",
        "                downloaded_files.append(dicom_file_path)\n",
        "                continue\n",
        "            \n",
        "            # Download using the WORKING v1.1 pattern - MODIFIED for flat structure\n",
        "            result = a3conn.downloadFile(\n",
        "                int(file_id_str),  # Convert back to int as API expects\n",
        "                asStream=False,\n",
        "                destDir=DOWNLOAD_PATH,  # Use main directory directly\n",
        "                filename=f\"{file_id_str}.dcm\",\n",
        "                inWorklist=False\n",
        "            )\n",
        "            \n",
        "            if result and os.path.exists(dicom_file_path):\n",
        "                downloaded_files.append(dicom_file_path)\n",
        "                print(f\"{ANSI['G']}‚úÖ Successfully downloaded: {dicom_file_path}{ANSI['W']}\")\n",
        "            else:\n",
        "                print(f\"{ANSI['Y']}‚ö†Ô∏è Download result unclear for {file_id_str}{ANSI['W']}\")\n",
        "        except Exception as e:\n",
        "            print(f\"{ANSI['Y']}‚ö†Ô∏è Failed to download {file_id_str}: {e}{ANSI['W']}\")\n",
        "    \n",
        "    print(f\"{ANSI['G']}‚úÖ Downloaded {len(downloaded_files)} files successfully{ANSI['W']}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"{ANSI['R']}‚ùå Setup failed: {e}{ANSI['W']}\")\n",
        "    downloaded_files = []\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üè• Enhanced DICOM Conversion with Pre-trained Lung Segmentation\n",
        "\n",
        "def convert_dicom_to_image_with_segmentation(dicom_path, output_path, target_size=TARGET_SIZE):\n",
        "    \"\"\"Enhanced DICOM conversion with professional lung segmentation\"\"\"\n",
        "    try:\n",
        "        file_id = os.path.splitext(os.path.basename(dicom_path))[0]\n",
        "        \n",
        "        # Read DICOM file\n",
        "        dicom_data = pydicom.dcmread(dicom_path)\n",
        "        \n",
        "        # Extract image data\n",
        "        image_array = dicom_data.pixel_array\n",
        "        print(f\"{ANSI['C']}üìÅ Processing {file_id}: {image_array.shape}{ANSI['W']}\")\n",
        "        \n",
        "        # Handle different photometric interpretations\n",
        "        if hasattr(dicom_data, 'PhotometricInterpretation'):\n",
        "            if dicom_data.PhotometricInterpretation == 'MONOCHROME1':\n",
        "                image_array = np.max(image_array) - image_array\n",
        "        \n",
        "        # Normalize to 0-255 range\n",
        "        if image_array.max() > 255:\n",
        "            image_array = ((image_array - image_array.min()) / \n",
        "                          (image_array.max() - image_array.min()) * 255).astype(np.uint8)\n",
        "        else:\n",
        "            image_array = image_array.astype(np.uint8)\n",
        "        \n",
        "        # Apply lung segmentation and cropping\n",
        "        processed_image = process_image_with_segmentation(image_array, file_id)\n",
        "        \n",
        "        # Convert to PIL Image\n",
        "        if len(processed_image.shape) == 2:\n",
        "            pil_image = Image.fromarray(processed_image, mode='L')\n",
        "        else:\n",
        "            pil_image = Image.fromarray(processed_image)\n",
        "        \n",
        "        # Resize while preserving aspect ratio\n",
        "        if PRESERVE_ASPECT_RATIO:\n",
        "            pil_image.thumbnail(target_size, Image.Resampling.LANCZOS)\n",
        "            \n",
        "            # Create new image with target size and paste centered\n",
        "            final_image = Image.new('L', target_size, 0)\n",
        "            paste_x = (target_size[0] - pil_image.width) // 2\n",
        "            paste_y = (target_size[1] - pil_image.height) // 2\n",
        "            final_image.paste(pil_image, (paste_x, paste_y))\n",
        "            pil_image = final_image\n",
        "        else:\n",
        "            pil_image = pil_image.resize(target_size, Image.Resampling.LANCZOS)\n",
        "        \n",
        "        # Save image\n",
        "        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
        "        pil_image.save(output_path)\n",
        "        \n",
        "        return True\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"{ANSI['R']}‚ùå Failed to convert {dicom_path}: {e}{ANSI['W']}\")\n",
        "        return False\n",
        "\n",
        "# Convert downloaded DICOM files\n",
        "if CONVERT and downloaded_files:\n",
        "    print(f\"{ANSI['M']}üîÑ Converting {len(downloaded_files)} DICOM files with lung segmentation{ANSI['W']}\")\n",
        "    \n",
        "    # Ensure output directory exists\n",
        "    os.makedirs(IMAGES_PATH, exist_ok=True)\n",
        "    \n",
        "    converted_count = 0\n",
        "    \n",
        "    for dicom_path in tqdm(downloaded_files, desc=\"Converting DICOMs\"):\n",
        "        file_id = os.path.splitext(os.path.basename(dicom_path))[0]\n",
        "        output_path = os.path.join(IMAGES_PATH, f\"{file_id}.png\")\n",
        "        \n",
        "        if convert_dicom_to_image_with_segmentation(dicom_path, output_path):\n",
        "            converted_count += 1\n",
        "    \n",
        "    print(f\"{ANSI['G']}‚úÖ Successfully converted {converted_count}/{len(downloaded_files)} files{ANSI['W']}\")\n",
        "    print(f\"{ANSI['C']}üìÇ Images saved to: {IMAGES_PATH}{ANSI['W']}\")\n",
        "    \n",
        "    if SAVE_SEGMENTATION_MASKS:\n",
        "        print(f\"{ANSI['C']}üéØ Segmentation masks saved to: {MASKS_PATH}{ANSI['W']}\")\n",
        "        \n",
        "    print(f\"{ANSI['M']}üéâ V1.3 Pre-trained lung segmentation processing complete!{ANSI['W']}\")\n",
        "else:\n",
        "    print(f\"{ANSI['Y']}‚ö†Ô∏è No files to convert or conversion disabled{ANSI['W']}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
