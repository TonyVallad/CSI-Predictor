**<h1 align="center">Weekly Report</h1>**

<p align="center"><i>2025-06-04 Summary of how the app works and what needs to be done</i></p>

This document contains topics discussed with my tutor that I need to get some information on and that I must study.

## Metric used to evaluate the model

### What metric is used ?

The model uses **F1-score** as the primary evaluation metric, specifically:
- **F1 Macro**: Average F1-score across all 6 CSI zones (treats all zones equally)
- **F1 Weighted Macro**: F1-score weighted by class frequency (recommended for imbalanced medical data)
- **Per-zone F1**: Individual F1-score for each of the 6 chest zones
- **Overall F1**: F1-score computed across all zones flattened together

Additional metrics include:
- **Accuracy**: Per-zone and overall accuracy
- **Precision/Recall**: Computed for each zone and overall

### How exactly it is used ?

The F1 metrics are implemented using **pure PyTorch** (no scikit-learn dependencies) in `src/metrics.py`:

1. **During Training**: F1 macro is computed at the end of each epoch to track model performance
2. **During Validation**: Both F1 macro and F1 weighted are computed for model selection
3. **Ignore Strategy**: Class 4 ("ungradable") is ignored by default (`ignore_index=4`) to focus on gradable cases for medical interpretation
4. **Confusion Matrix**: Per-zone confusion matrices are computed using PyTorch's `bincount` function
5. **Hyperparameter Optimization**: F1 weighted macro is used as the target metric for Optuna/WandB sweeps

### How to improve ?

Current improvements already implemented:
- **Weighted F1**: Accounts for class imbalance in medical data
- **Per-class metrics**: Detailed analysis of each CSI score (0-4)
- **Zone-specific evaluation**: Individual assessment of each lung zone

One way to improve it would be to measure how far off the prediction is from reality.
For example, if the model predicts a score of 0 when it should be 3, it's a big mistake.
But if it predicts a score of 2 instead of 3, it's not so bad.
**This would require implementing ordinal regression or a custom loss function that penalizes based on the distance between predicted and true CSI scores.**
This will need to be implemented at some later point in time. (not for now)

## ResNet50 Model

### How many output fields does this model have ?

The ResNet50 model has **30 output fields** total:
- **6 zones** × **5 classes per zone** = **30 logit outputs**
- Output shape: `[batch_size, 6, 5]`

The 6 zones correspond to different regions of the chest X-ray:
1. `right_sup` - Patient Right Superior
2. `left_sup` - Patient Left Superior  
3. `right_mid` - Patient Right Middle
4. `left_mid` - Patient Left Middle
5. `right_inf` - Patient Right Inferior
6. `left_inf` - Patient Left Inferior

The 5 classes per zone represent CSI scores:
- **0**: Normal
- **1**: Mild congestion
- **2**: Moderate congestion  
- **3**: Severe congestion
- **4**: Unknown/Ungradable

### What metric does it use to optimize ? Loss ?

The model uses **Weighted Cross-Entropy Loss** for optimization:
- **Loss Function**: `WeightedCSILoss` class in `src/train.py`
- **Weighting Strategy**: Classes 0-3 get full weight (1.0), class 4 gets reduced weight (0.3 by default)
- **Purpose**: Reduces importance of "ungradable" class while still learning to predict it
- **Implementation**: Uses `F.cross_entropy()` with class weights

The loss is computed by:
1. Flattening predictions and targets: `[batch_size × 6, 5]` and `[batch_size × 6]`
2. Applying weighted cross-entropy with class weights: `[1.0, 1.0, 1.0, 1.0, 0.3]`
3. Averaging across all samples and zones

### Does it give out a Congestion Score Index for the 6 different zones we are interested in ?

**Yes**, the model specifically predicts CSI scores for exactly the 6 zones we are interested in:

**Architecture Details**:
- **Backbone**: ResNet50 extracts features → `[batch_size, 2048]`
- **CSI Head**: 6 parallel linear classifiers, one for each zone
- **Output**: Logits for each zone → `[batch_size, 6, 5]`
- **Prediction**: Argmax of logits gives CSI class (0-4) for each zone

**Zone Mapping**:
```python
zone_names = ["right_sup", "left_sup", "right_mid", "left_mid", "right_inf", "left_inf"]
CSI_COLUMNS = ['right_sup', 'left_sup', 'right_mid', 'left_mid', 'right_inf', 'left_inf']
```

**Output Interpretation**:
- Each zone gets a predicted CSI score from 0-4
- Class 4 represents "ungradable" cases where assessment is not possible
- Classes 0-3 represent increasing levels of pulmonary congestion severity

## When everything seems to work, test manually with a few images to see if anything seems wrong.

This part must be done manually when I believe it to be worth it. **The codebase includes visualization tools for manual inspection:**

**Available Tools**:
1. **`show_batch()` function** in `src/utils.py`: Visualizes predictions overlaid on chest X-ray images with color-coded zone predictions
2. **Confusion matrices**: Per-zone confusion matrices saved as PNG graphs in the `graphs_dir`
3. **Classification reports**: Detailed per-zone precision, recall, and F1 scores
4. **WandB logging**: Automatic logging of sample predictions and confusion matrix heatmaps

**Manual Testing Process**:
1. Load a trained model using `src/evaluate.py`
2. Run inference on test images
3. Use visualization tools to inspect predictions vs ground truth
4. Look for systematic errors or biases in specific zones
5. Check if predictions make medical sense (e.g., bilateral consistency)

(not for now)