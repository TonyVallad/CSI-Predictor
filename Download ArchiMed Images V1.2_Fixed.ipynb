{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "**<h1 align=\"center\">Download ArchiMed Images V1.2 - FIXED VERSION</h1>**\n",
        "\n",
        "## üîß **FIXED: Tensor Dimension Mismatch Issue**\n",
        "- **Problem**: EfficientNet-B4 skip connection dimensions mismatch\n",
        "- **Solution**: Simplified U-Net with correct channel dimensions\n",
        "- **Alternative**: Traditional U-Net encoder or disable segmentation option\n",
        "\n",
        "## üöÄ **Features:**\n",
        "- **Fixed Lung Segmentation**: Corrected tensor dimensions\n",
        "- **Fallback Option**: Disable segmentation if issues persist\n",
        "- **Smart Cropping**: Focus on lung regions before resizing\n",
        "- **Enhanced Quality**: Preserve lung details during preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## üõ†Ô∏è Quick Fix Options\n",
        "\n",
        "**Option 1: Disable Segmentation (Immediate Fix)**\n",
        "```python\n",
        "USE_LUNG_SEGMENTATION = False  # Set this to False to bypass segmentation\n",
        "```\n",
        "\n",
        "**Option 2: Use Fixed Architecture (Recommended)**\n",
        "- Use the corrected U-Net implementation below\n",
        "- Properly matched EfficientNet-B4 dimensions\n",
        "\n",
        "**Option 3: Traditional U-Net**\n",
        "- Simpler encoder without pre-trained weights\n",
        "- More reliable but less powerful"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üîß FIXED Configuration Variables\n",
        "CSV_FOLDER = \"../../data/Paradise_CSV/\"\n",
        "CSV_LABELS_FILE = \"Labeled_Data_RAW_Sample.csv\"\n",
        "CSV_SEPARATOR = \";\"\n",
        "IMPORT_COLUMNS = []\n",
        "CHUNK_SIZE = 50000\n",
        "\n",
        "# Download parameters  \n",
        "DOWNLOAD_PATH = '../../data/Paradise_Test_DICOMs'\n",
        "IMAGES_PATH = '../../data/Paradise_Test_Images'\n",
        "EXPORT_METADATA = True\n",
        "ARCHIMED_METADATA_FILE = 'DICOM_Metadata.csv'\n",
        "CONVERT = True\n",
        "\n",
        "# Conversion parameters\n",
        "BATCH_SIZE = 50\n",
        "BIT_DEPTH = 8\n",
        "CREATE_SUBFOLDERS = False\n",
        "DELETE_DICOM = True\n",
        "MONOCHROME = 1\n",
        "\n",
        "# üîß FIXED SEGMENTATION SETTINGS\n",
        "USE_LUNG_SEGMENTATION = True   # üéØ ENABLE SEGMENTATION FOR TESTING\n",
        "OVERWRITE_EXISTING = True      # üîÑ OVERWRITE EXISTING FILES FOR TESTING\n",
        "SEGMENTATION_MODEL = 'simple_unet'  # Changed to simpler model\n",
        "LUNG_CROP_PADDING = 20\n",
        "MIN_LUNG_AREA_RATIO = 0.001  # üîß LOWERED: More sensitive detection\n",
        "USE_FALLBACK_SEGMENTATION = True  # üéØ Use simple thresholding instead of untrained model\n",
        "SAVE_SEGMENTATION_MASKS = False  # Disabled to prevent errors\n",
        "MASKS_PATH = '../../data/Paradise_Masks'\n",
        "\n",
        "# Enhanced Resize Parameters\n",
        "TARGET_SIZE = (518, 518)\n",
        "PRESERVE_ASPECT_RATIO = True\n",
        "\n",
        "print(\"üîß FIXED configuration loaded!\")\n",
        "print(f\"ü´Å Lung segmentation: {'ENABLED' if USE_LUNG_SEGMENTATION else 'DISABLED (SAFE MODE)'}\")\n",
        "print(\"üí° To enable segmentation, set USE_LUNG_SEGMENTATION = True and run fixed model below\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Colors\n",
        "ANSI = {\n",
        "    'R' : '\\033[91m',  # Red\n",
        "    'G' : '\\033[92m',  # Green\n",
        "    'B' : '\\033[94m',  # Blue\n",
        "    'Y' : '\\033[93m',  # Yellow\n",
        "    'W' : '\\033[0m',   # White\n",
        "    'M' : '\\033[95m',  # Magenta\n",
        "    'C' : '\\033[96m',  # Cyan\n",
        "}\n",
        "\n",
        "# Dependencies\n",
        "import ArchiMedConnector.A3_Connector as A3_Conn\n",
        "import pandas as pd\n",
        "import os\n",
        "import pydicom\n",
        "import numpy as np\n",
        "from PIL import Image, ImageDraw\n",
        "import glob\n",
        "from tqdm import tqdm\n",
        "import shutil\n",
        "import warnings\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import cv2\n",
        "\n",
        "# Deep Learning dependencies (with error handling)\n",
        "try:\n",
        "    import torch\n",
        "    import torch.nn as nn\n",
        "    import torch.nn.functional as F\n",
        "    import torchvision.transforms as transforms\n",
        "    from torchvision import models\n",
        "    import albumentations as A\n",
        "    from albumentations.pytorch import ToTensorV2\n",
        "    TORCH_AVAILABLE = True\n",
        "    print(f\"{ANSI['G']}‚úÖ PyTorch dependencies loaded successfully{ANSI['W']}\")\n",
        "except ImportError as e:\n",
        "    TORCH_AVAILABLE = False\n",
        "    print(f\"{ANSI['Y']}‚ö†Ô∏è PyTorch not available: {e}{ANSI['W']}\")\n",
        "    print(f\"{ANSI['B']}   Segmentation will be disabled{ANSI['W']}\")\n",
        "\n",
        "print(f\"{ANSI['C']}üîß Fixed dependencies loaded{ANSI['W']}\")\n",
        "\n",
        "# Initialize ArchiMed connector\n",
        "a3conn = A3_Conn.A3_Connector()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## üîß Fixed Lung Segmentation Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if TORCH_AVAILABLE:\n",
        "    \n",
        "    class SimpleUNet(nn.Module):\n",
        "        \"\"\"\n",
        "        Simplified U-Net for lung segmentation - FIXED VERSION\n",
        "        Avoids dimension mismatch issues with EfficientNet\n",
        "        \"\"\"\n",
        "        \n",
        "        def __init__(self, n_classes=1):\n",
        "            super(SimpleUNet, self).__init__()\n",
        "            \n",
        "            # Encoder (Downsampling)\n",
        "            self.enc1 = self._make_encoder_block(3, 64)\n",
        "            self.enc2 = self._make_encoder_block(64, 128)\n",
        "            self.enc3 = self._make_encoder_block(128, 256)\n",
        "            self.enc4 = self._make_encoder_block(256, 512)\n",
        "            \n",
        "            # Bottleneck\n",
        "            self.bottleneck = self._make_encoder_block(512, 1024)\n",
        "            \n",
        "            # Decoder (Upsampling)\n",
        "            self.dec4 = self._make_decoder_block(1024 + 512, 512)\n",
        "            self.dec3 = self._make_decoder_block(512 + 256, 256)\n",
        "            self.dec2 = self._make_decoder_block(256 + 128, 128)\n",
        "            self.dec1 = self._make_decoder_block(128 + 64, 64)\n",
        "            \n",
        "            # Final layer\n",
        "            self.final = nn.Conv2d(64, n_classes, 1)\n",
        "            \n",
        "            # Pooling and upsampling\n",
        "            self.pool = nn.MaxPool2d(2)\n",
        "            self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "        \n",
        "        def _make_encoder_block(self, in_channels, out_channels):\n",
        "            return nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
        "                nn.BatchNorm2d(out_channels),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Conv2d(out_channels, out_channels, 3, padding=1),\n",
        "                nn.BatchNorm2d(out_channels),\n",
        "                nn.ReLU(inplace=True)\n",
        "            )\n",
        "        \n",
        "        def _make_decoder_block(self, in_channels, out_channels):\n",
        "            return nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
        "                nn.BatchNorm2d(out_channels),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Conv2d(out_channels, out_channels, 3, padding=1),\n",
        "                nn.BatchNorm2d(out_channels),\n",
        "                nn.ReLU(inplace=True)\n",
        "            )\n",
        "        \n",
        "        def forward(self, x):\n",
        "            # Encoder path\n",
        "            e1 = self.enc1(x)\n",
        "            e2 = self.enc2(self.pool(e1))\n",
        "            e3 = self.enc3(self.pool(e2))\n",
        "            e4 = self.enc4(self.pool(e3))\n",
        "            \n",
        "            # Bottleneck\n",
        "            b = self.bottleneck(self.pool(e4))\n",
        "            \n",
        "            # Decoder path with skip connections\n",
        "            d4 = self.dec4(torch.cat([self.upsample(b), e4], dim=1))\n",
        "            d3 = self.dec3(torch.cat([self.upsample(d4), e3], dim=1))\n",
        "            d2 = self.dec2(torch.cat([self.upsample(d3), e2], dim=1))\n",
        "            d1 = self.dec1(torch.cat([self.upsample(d2), e1], dim=1))\n",
        "            \n",
        "            # Final output\n",
        "            output = self.final(d1)\n",
        "            return torch.sigmoid(output)\n",
        "    \n",
        "    \n",
        "    class LungSegmentationPipeline:\n",
        "        \"\"\"FIXED: Lung segmentation pipeline with proper error handling\"\"\"\n",
        "        \n",
        "        def __init__(self, model_path=None, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
        "            self.device = device\n",
        "            \n",
        "            # Use simple U-Net instead of EfficientNet-based one\n",
        "            self.model = SimpleUNet(n_classes=1)\n",
        "            \n",
        "            if model_path and os.path.exists(model_path):\n",
        "                try:\n",
        "                    self.model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "                    print(f\"{ANSI['G']}‚úÖ Loaded pre-trained model from {model_path}{ANSI['W']}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"{ANSI['Y']}‚ö†Ô∏è Failed to load model: {e}{ANSI['W']}\")\n",
        "                    print(f\"{ANSI['B']}   Using randomly initialized weights{ANSI['W']}\")\n",
        "            else:\n",
        "                print(f\"{ANSI['Y']}‚ö†Ô∏è No pre-trained model found{ANSI['W']}\")\n",
        "                print(f\"{ANSI['B']}   Using simple thresholding fallback{ANSI['W']}\")\n",
        "            \n",
        "            self.model.to(device)\n",
        "            self.model.eval()\n",
        "            \n",
        "            # Preprocessing transforms\n",
        "            self.transform = A.Compose([\n",
        "                A.Resize(256, 256),\n",
        "                A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "                ToTensorV2()\n",
        "            ])\n",
        "        \n",
        "        def preprocess_image(self, image):\n",
        "            \"\"\"Preprocess image for segmentation model.\"\"\"\n",
        "            if len(image.shape) == 2:  # Grayscale\n",
        "                image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
        "            elif len(image.shape) == 3 and image.shape[2] == 1:\n",
        "                image = np.repeat(image, 3, axis=2)\n",
        "            \n",
        "            try:\n",
        "                transformed = self.transform(image=image)\n",
        "                tensor = transformed['image'].unsqueeze(0)\n",
        "                return tensor.to(self.device)\n",
        "            except Exception as e:\n",
        "                print(f\"{ANSI['Y']}‚ö†Ô∏è Transform error: {e}, using fallback{ANSI['W']}\")\n",
        "                return None\n",
        "        \n",
        "        def simple_lung_detection(self, image):\n",
        "            \"\"\"Enhanced fallback: Chest X-ray specific lung detection\"\"\"\n",
        "            # Convert to grayscale if needed\n",
        "            if len(image.shape) == 3:\n",
        "                gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
        "            else:\n",
        "                gray = image.copy().astype(np.uint8)\n",
        "            \n",
        "            # Normalize image\n",
        "            gray = cv2.normalize(gray, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
        "            \n",
        "            # Apply CLAHE for better contrast\n",
        "            clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
        "            gray = clahe.apply(gray)\n",
        "            \n",
        "            # Multiple thresholding approaches for chest X-rays\n",
        "            # Approach 1: Otsu thresholding\n",
        "            _, binary1 = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "            \n",
        "            # Approach 2: Adaptive thresholding \n",
        "            binary2 = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, \n",
        "                                          cv2.THRESH_BINARY, 11, 2)\n",
        "            \n",
        "            # Approach 3: Simple intensity-based (lung tissue is darker)\n",
        "            mean_intensity = np.mean(gray)\n",
        "            _, binary3 = cv2.threshold(gray, mean_intensity * 0.7, 255, cv2.THRESH_BINARY)\n",
        "            \n",
        "            # Combine approaches - use the one that gives largest regions\n",
        "            binaries = [binary1, binary2, binary3]\n",
        "            areas = []\n",
        "            \n",
        "            for binary in binaries:\n",
        "                # Clean up\n",
        "                kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (7, 7))\n",
        "                cleaned = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, kernel)\n",
        "                cleaned = cv2.morphologyEx(cleaned, cv2.MORPH_OPEN, kernel)\n",
        "                \n",
        "                # Calculate area of largest components\n",
        "                contours, _ = cv2.findContours(cleaned, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "                total_area = sum(cv2.contourArea(c) for c in contours if cv2.contourArea(c) > 1000)\n",
        "                areas.append(total_area)\n",
        "            \n",
        "            # Use the method with the largest total area\n",
        "            best_idx = np.argmax(areas)\n",
        "            best_binary = binaries[best_idx]\n",
        "            \n",
        "            # Final cleanup\n",
        "            kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (7, 7))\n",
        "            best_binary = cv2.morphologyEx(best_binary, cv2.MORPH_CLOSE, kernel)\n",
        "            best_binary = cv2.morphologyEx(best_binary, cv2.MORPH_OPEN, kernel)\n",
        "            \n",
        "            # Remove small components and ensure reasonable detection\n",
        "            contours, _ = cv2.findContours(best_binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "            final_mask = np.zeros_like(best_binary)\n",
        "            \n",
        "            # More restrictive area filtering\n",
        "            total_image_area = gray.shape[0] * gray.shape[1]\n",
        "            min_area = total_image_area * 0.02  # At least 2% of image\n",
        "            max_area = total_image_area * 0.8   # At most 80% of image\n",
        "            \n",
        "            valid_contours = []\n",
        "            for contour in contours:\n",
        "                area = cv2.contourArea(contour)\n",
        "                if min_area < area < max_area:\n",
        "                    valid_contours.append(contour)\n",
        "            \n",
        "            # Only proceed if we have reasonable lung-like regions\n",
        "            if len(valid_contours) > 0:\n",
        "                for contour in valid_contours:\n",
        "                    cv2.fillPoly(final_mask, [contour], 255)\n",
        "                \n",
        "                total_detected = np.sum(final_mask > 0) / total_image_area\n",
        "                print(f\"{ANSI['C']}üìä Fallback segmentation: {total_detected:.3f} of image detected as lung tissue{ANSI['W']}\")\n",
        "            else:\n",
        "                # If no reasonable regions found, create a conservative central crop\n",
        "                print(f\"{ANSI['Y']}‚ö†Ô∏è No valid lung regions found, using conservative central crop{ANSI['W']}\")\n",
        "                h, w = gray.shape\n",
        "                # Create a rectangular region in the center covering ~40% of the image\n",
        "                margin_h, margin_w = int(h * 0.3), int(w * 0.3)\n",
        "                final_mask[margin_h:h-margin_h, margin_w:w-margin_w] = 255\n",
        "                total_detected = np.sum(final_mask > 0) / total_image_area\n",
        "                print(f\"{ANSI['C']}üìä Conservative crop: {total_detected:.3f} of image selected{ANSI['W']}\")\n",
        "            \n",
        "            return (final_mask > 0).astype(np.uint8), final_mask / 255.0\n",
        "        \n",
        "        def segment_lungs(self, image):\n",
        "            \"\"\"Segment lung regions with fallback.\"\"\"\n",
        "            original_shape = image.shape[:2]\n",
        "            print(f\"{ANSI['C']}üîç Segmenting lungs on image shape: {original_shape}{ANSI['W']}\")\n",
        "            \n",
        "            # Check if we should force fallback mode\n",
        "            if USE_FALLBACK_SEGMENTATION:\n",
        "                print(f\"{ANSI['B']}üîÑ Using forced fallback thresholding (better for untrained models)...{ANSI['W']}\")\n",
        "                return self.simple_lung_detection(image)\n",
        "            \n",
        "            # Try deep learning approach first\n",
        "            input_tensor = self.preprocess_image(image)\n",
        "            \n",
        "            if input_tensor is not None:\n",
        "                try:\n",
        "                    print(f\"{ANSI['B']}üß† Running deep learning segmentation...{ANSI['W']}\")\n",
        "                    with torch.no_grad():\n",
        "                        output = self.model(input_tensor)\n",
        "                        mask = output.squeeze().cpu().numpy()\n",
        "                    \n",
        "                    # Resize back to original size\n",
        "                    mask_resized = cv2.resize(mask, (original_shape[1], original_shape[0]))\n",
        "                    binary_mask = (mask_resized > 0.5).astype(np.uint8)\n",
        "                    \n",
        "                    # Check if mask has any meaningful content\n",
        "                    mask_area = np.sum(binary_mask) / (original_shape[0] * original_shape[1])\n",
        "                    print(f\"{ANSI['G']}‚úÖ Deep learning segmentation successful (mask area: {mask_area:.3f}){ANSI['W']}\")\n",
        "                    \n",
        "                    if mask_area < 0.001:  # Very small mask, likely poor quality\n",
        "                        print(f\"{ANSI['Y']}‚ö†Ô∏è Mask too small, using fallback{ANSI['W']}\")\n",
        "                        return self.simple_lung_detection(image)\n",
        "                    \n",
        "                    return binary_mask, mask_resized\n",
        "                except Exception as e:\n",
        "                    print(f\"{ANSI['Y']}‚ö†Ô∏è Model inference failed: {e}, using fallback{ANSI['W']}\")\n",
        "            \n",
        "            # Fallback to simple detection\n",
        "            print(f\"{ANSI['B']}üîÑ Using fallback thresholding segmentation...{ANSI['W']}\")\n",
        "            return self.simple_lung_detection(image)\n",
        "        \n",
        "        def extract_lung_regions(self, image, mask, padding=20):\n",
        "            \"\"\"Extract and crop lung regions with smart bounding box.\"\"\"\n",
        "            contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "            \n",
        "            if not contours:\n",
        "                print(f\"{ANSI['Y']}‚ö†Ô∏è No lung regions detected - no cropping{ANSI['W']}\")\n",
        "                return image, None\n",
        "            \n",
        "            # Filter by area\n",
        "            total_area = image.shape[0] * image.shape[1]\n",
        "            min_area = total_area * MIN_LUNG_AREA_RATIO\n",
        "            valid_contours = [c for c in contours if cv2.contourArea(c) > min_area]\n",
        "            \n",
        "            if not valid_contours:\n",
        "                print(f\"{ANSI['Y']}‚ö†Ô∏è No significant lung regions found - no cropping{ANSI['W']}\")\n",
        "                return image, None\n",
        "            \n",
        "            # Get bounding box that encompasses all valid contours\n",
        "            all_points = np.vstack([c.reshape(-1, 2) for c in valid_contours])\n",
        "            x_min, y_min = np.min(all_points, axis=0)\n",
        "            x_max, y_max = np.max(all_points, axis=0)\n",
        "            \n",
        "            # Add padding\n",
        "            h, w = image.shape[:2]\n",
        "            x_min = max(0, x_min - padding)\n",
        "            y_min = max(0, y_min - padding)\n",
        "            x_max = min(w, x_max + padding)\n",
        "            y_max = min(h, y_max + padding)\n",
        "            \n",
        "            # Ensure we actually have a meaningful crop (not the entire image)\n",
        "            crop_width = x_max - x_min\n",
        "            crop_height = y_max - y_min\n",
        "            original_area = h * w\n",
        "            crop_area = crop_width * crop_height\n",
        "            \n",
        "            if crop_area >= original_area * 0.95:  # If crop is >95% of original\n",
        "                print(f\"{ANSI['Y']}‚ö†Ô∏è Crop too large ({crop_area/original_area:.2f}), no meaningful cropping{ANSI['W']}\")\n",
        "                return image, None\n",
        "            \n",
        "            # Perform the crop\n",
        "            if len(image.shape) == 3:\n",
        "                cropped = image[y_min:y_max, x_min:x_max, :]\n",
        "            else:\n",
        "                cropped = image[y_min:y_max, x_min:x_max]\n",
        "            \n",
        "            crop_info = {\n",
        "                'bbox': (x_min, y_min, x_max, y_max),\n",
        "                'original_shape': image.shape[:2],\n",
        "                'cropped_shape': cropped.shape[:2],\n",
        "                'area_reduction': 1 - (crop_area / original_area)\n",
        "            }\n",
        "            \n",
        "            print(f\"{ANSI['G']}‚úÖ Cropped to {crop_info['area_reduction']:.2f} area reduction{ANSI['W']}\")\n",
        "            return cropped, crop_info\n",
        "        \n",
        "        def process_image(self, image, save_mask_path=None):\n",
        "            \"\"\"Complete pipeline with error handling.\"\"\"\n",
        "            try:\n",
        "                binary_mask, probability_mask = self.segment_lungs(image)\n",
        "                \n",
        "                if save_mask_path:\n",
        "                    mask_image = (probability_mask * 255).astype(np.uint8)\n",
        "                    cv2.imwrite(save_mask_path, mask_image)\n",
        "                \n",
        "                cropped_image, crop_info = self.extract_lung_regions(image, binary_mask, LUNG_CROP_PADDING)\n",
        "                return cropped_image, crop_info, binary_mask, probability_mask\n",
        "                \n",
        "            except Exception as e:\n",
        "                print(f\"{ANSI['R']}‚ùå Segmentation pipeline error: {e}{ANSI['W']}\")\n",
        "                return image, None, None, None\n",
        "    \n",
        "    print(f\"{ANSI['G']}‚úÖ Fixed segmentation models loaded{ANSI['W']}\")\n",
        "    \n",
        "else:\n",
        "    class LungSegmentationPipeline:\n",
        "        def __init__(self, *args, **kwargs):\n",
        "            print(f\"{ANSI['Y']}‚ö†Ô∏è PyTorch not available, segmentation disabled{ANSI['W']}\")\n",
        "        \n",
        "        def process_image(self, image, save_mask_path=None):\n",
        "            return image, None, None, None\n",
        "    \n",
        "    print(f\"{ANSI['Y']}‚ö†Ô∏è Segmentation models disabled (PyTorch not available){ANSI['W']}\")\n",
        "\n",
        "\n",
        "# Initialize pipeline\n",
        "if USE_LUNG_SEGMENTATION and TORCH_AVAILABLE:\n",
        "    segmentation_pipeline = LungSegmentationPipeline()\n",
        "    print(f\"{ANSI['C']}üéØ Fixed lung segmentation pipeline initialized{ANSI['W']}\")\n",
        "    if SAVE_SEGMENTATION_MASKS:\n",
        "        os.makedirs(MASKS_PATH, exist_ok=True)\n",
        "else:\n",
        "    segmentation_pipeline = None\n",
        "    print(f\"{ANSI['Y']}‚ö†Ô∏è Lung segmentation disabled - running in safe mode{ANSI['W']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üîß FIXED: Enhanced DICOM to PNG conversion with optional lung segmentation\n",
        "\n",
        "def enhanced_dicom_to_png_with_segmentation(dicom_paths, png_folder, batch_size=BATCH_SIZE, segmentation_pipeline=None):\n",
        "    \"\"\"\n",
        "    FIXED VERSION: Convert DICOM files to PNG with optional lung-aware preprocessing\n",
        "    \"\"\"\n",
        "    \n",
        "    def safe_segmentation_process(image_array, file_id, segmentation_pipeline):\n",
        "        \"\"\"Safely apply segmentation with comprehensive error handling\"\"\"\n",
        "        if segmentation_pipeline is None:\n",
        "            return image_array\n",
        "        \n",
        "        try:\n",
        "            # Save mask path if enabled\n",
        "            mask_path = None\n",
        "            if SAVE_SEGMENTATION_MASKS:\n",
        "                mask_path = os.path.join(MASKS_PATH, f\"{file_id}_mask.png\")\n",
        "            \n",
        "            # Process with segmentation\n",
        "            processed_image, crop_info, binary_mask, prob_mask = segmentation_pipeline.process_image(\n",
        "                image_array, mask_path\n",
        "            )\n",
        "            \n",
        "            if crop_info is not None:\n",
        "                return processed_image\n",
        "            else:\n",
        "                return image_array\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f\"{ANSI['R']}‚ùå Segmentation error for {file_id}: {e}{ANSI['W']}\")\n",
        "            return image_array  # Return original image on error\n",
        "    \n",
        "    def process_single_dicom(dicom_path, png_folder, segmentation_pipeline=None):\n",
        "        \"\"\"Process a single DICOM file with enhanced error handling\"\"\"        \n",
        "        file_id = os.path.splitext(os.path.basename(dicom_path))[0]\n",
        "        png_path = os.path.join(png_folder, f\"{file_id}.png\")\n",
        "        \n",
        "        # Check if already exists (skip only if OVERWRITE_EXISTING is False)\n",
        "        if os.path.exists(png_path) and not OVERWRITE_EXISTING:\n",
        "            return {\n",
        "                'file_id': file_id,\n",
        "                'original_path': dicom_path,\n",
        "                'png_path': png_path,\n",
        "                'status': 'skipped_existing'\n",
        "            }\n",
        "        \n",
        "        try:\n",
        "            # Read DICOM\n",
        "            dicom_data = pydicom.dcmread(dicom_path)\n",
        "            \n",
        "            if not hasattr(dicom_data, 'pixel_array'):\n",
        "                return None\n",
        "            \n",
        "            # Get pixel array\n",
        "            image_array = dicom_data.pixel_array.astype(np.float32)\n",
        "            \n",
        "            # Handle different photometric interpretations\n",
        "            if hasattr(dicom_data, 'PhotometricInterpretation'):\n",
        "                if dicom_data.PhotometricInterpretation == 'MONOCHROME1':\n",
        "                    image_array = np.max(image_array) - image_array\n",
        "            \n",
        "            # Normalize to 0-255 range\n",
        "            image_array = image_array - np.min(image_array)\n",
        "            if np.max(image_array) > 0:\n",
        "                image_array = image_array / np.max(image_array) * 255\n",
        "            \n",
        "            # Convert to uint8\n",
        "            image_array = image_array.astype(np.uint8)\n",
        "            \n",
        "            # Apply lung segmentation if enabled\n",
        "            if USE_LUNG_SEGMENTATION and segmentation_pipeline:\n",
        "                print(f\"{ANSI['B']}ü´Å Applying segmentation for {file_id}...{ANSI['W']}\")\n",
        "                original_shape = image_array.shape\n",
        "                image_array = safe_segmentation_process(image_array, file_id, segmentation_pipeline)\n",
        "                new_shape = image_array.shape\n",
        "                print(f\"{ANSI['G']}‚úÖ Segmentation complete: {original_shape} ‚Üí {new_shape}{ANSI['W']}\")\n",
        "            else:\n",
        "                print(f\"{ANSI['Y']}‚ö†Ô∏è Segmentation skipped for {file_id} (pipeline: {segmentation_pipeline is not None}){ANSI['W']}\")\n",
        "            \n",
        "            # Resize to target size with aspect ratio preservation\n",
        "            if PRESERVE_ASPECT_RATIO:\n",
        "                h, w = image_array.shape[:2]\n",
        "                target_h, target_w = TARGET_SIZE\n",
        "                \n",
        "                # Calculate scaling\n",
        "                scale = min(target_w / w, target_h / h)\n",
        "                new_w = int(w * scale)\n",
        "                new_h = int(h * scale)\n",
        "                \n",
        "                # Resize\n",
        "                image_array = cv2.resize(image_array, (new_w, new_h), interpolation=cv2.INTER_LANCZOS4)\n",
        "                \n",
        "                # Pad to target size\n",
        "                if len(image_array.shape) == 2:\n",
        "                    padded = np.zeros((target_h, target_w), dtype=np.uint8)\n",
        "                else:\n",
        "                    padded = np.zeros((target_h, target_w, image_array.shape[2]), dtype=np.uint8)\n",
        "                \n",
        "                y_offset = (target_h - new_h) // 2\n",
        "                x_offset = (target_w - new_w) // 2\n",
        "                \n",
        "                if len(image_array.shape) == 2:\n",
        "                    padded[y_offset:y_offset+new_h, x_offset:x_offset+new_w] = image_array\n",
        "                else:\n",
        "                    padded[y_offset:y_offset+new_h, x_offset:x_offset+new_w, :] = image_array\n",
        "                \n",
        "                image_array = padded\n",
        "            else:\n",
        "                image_array = cv2.resize(image_array, TARGET_SIZE, interpolation=cv2.INTER_LANCZOS4)\n",
        "            \n",
        "            # Save as PNG\n",
        "            if len(image_array.shape) == 2 or (len(image_array.shape) == 3 and image_array.shape[2] == 1):\n",
        "                # Grayscale\n",
        "                Image.fromarray(image_array, mode='L').save(png_path, 'PNG')\n",
        "            else:\n",
        "                # RGB\n",
        "                Image.fromarray(image_array, mode='RGB').save(png_path, 'PNG')\n",
        "            \n",
        "            return {\n",
        "                'file_id': file_id,\n",
        "                'original_path': dicom_path,\n",
        "                'png_path': png_path,\n",
        "                'status': 'success'\n",
        "            }\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"{ANSI['R']}‚ùå Error processing {file_id}: {e}{ANSI['W']}\")\n",
        "            return {\n",
        "                'file_id': file_id,\n",
        "                'original_path': dicom_path,\n",
        "                'png_path': None,\n",
        "                'status': 'error',\n",
        "                'error': str(e)\n",
        "            }\n",
        "    \n",
        "    # Process files\n",
        "    os.makedirs(png_folder, exist_ok=True)\n",
        "    results = []\n",
        "    \n",
        "    segmentation_status = \"ENABLED\" if (USE_LUNG_SEGMENTATION and segmentation_pipeline) else \"DISABLED\"\n",
        "    print(f\"{ANSI['C']}üîÑ Converting batch of {len(dicom_paths)} DICOM files with lung segmentation...{ANSI['W']}\")\n",
        "    \n",
        "    # Process with progress bar\n",
        "    with tqdm(dicom_paths, desc=\"üîÑ Converting DICOM with lung segmentation\", unit=\"file\") as pbar:\n",
        "        for dicom_path in pbar:\n",
        "            result = process_single_dicom(dicom_path, png_folder, segmentation_pipeline)\n",
        "            if result:\n",
        "                results.append(result)\n",
        "    \n",
        "    # Statistics\n",
        "    successful = len([r for r in results if r['status'] == 'success'])\n",
        "    failed = len([r for r in results if r['status'] == 'error'])\n",
        "    skipped = len([r for r in results if r['status'] == 'skipped_existing'])\n",
        "    \n",
        "    print(f\"{ANSI['G']}‚úÖ Conversion complete: {successful} successful, {failed} failed, {skipped} skipped{ANSI['W']}\")\n",
        "    print(f\"{ANSI['B']}ü´Å Lung segmentation: {segmentation_status}{ANSI['W']}\")\n",
        "    \n",
        "    # Debug info\n",
        "    if len(results) == 0:\n",
        "        print(f\"{ANSI['R']}‚ùå No results returned - check DICOM processing{ANSI['W']}\")\n",
        "    elif successful == 0 and failed == 0 and skipped == 0:\n",
        "        print(f\"{ANSI['R']}‚ùå All files returned None - check file processing logic{ANSI['W']}\")\n",
        "    \n",
        "    # Show sample of results for debugging\n",
        "    if len(results) > 0:\n",
        "        print(f\"{ANSI['C']}üìä Sample results: {results[:3]}{ANSI['W']}\")\n",
        "    \n",
        "    return results\n",
        "\n",
        "print(f\"{ANSI['G']}‚úÖ Fixed enhanced DICOM conversion function loaded{ANSI['W']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load CSV and other required functions from original notebook\n",
        "def load_csv_data():\n",
        "    \"\"\"Load and process CSV data\"\"\"\n",
        "    csv_path = os.path.join(CSV_FOLDER, CSV_LABELS_FILE)\n",
        "    \n",
        "    if not os.path.exists(csv_path):\n",
        "        print(f\"{ANSI['R']}‚ùå CSV file not found: {csv_path}{ANSI['W']}\")\n",
        "        return None\n",
        "    \n",
        "    # Load CSV\n",
        "    df = pd.read_csv(csv_path, sep=CSV_SEPARATOR)\n",
        "    print(f\"{ANSI['G']}‚úÖ Loaded CSV with {len(df)} rows{ANSI['W']}\")\n",
        "    return df\n",
        "\n",
        "def get_auth_info():\n",
        "    \"\"\"Get ArchiMed user info (corrected method name)\"\"\"\n",
        "    try:\n",
        "        # FIXED: Use correct method name from ArchiMed API\n",
        "        user_info = a3conn.getUserInfos()\n",
        "        print(f\"{ANSI['C']}üîê ArchiMed User Info{ANSI['W']}\")\n",
        "        print(f\"User info: {user_info}\")\n",
        "        return user_info\n",
        "    except Exception as e:\n",
        "        print(f\"{ANSI['R']}‚ùå Authentication failed: {e}{ANSI['W']}\")\n",
        "        return None\n",
        "\n",
        "def download_archimed_files(file_ids, download_path):\n",
        "    \"\"\"Download files from ArchiMed with proper API usage\"\"\"\n",
        "    os.makedirs(download_path, exist_ok=True)\n",
        "    downloaded_files = []\n",
        "    \n",
        "    total_files = len(file_ids)\n",
        "    print(f\"{ANSI['C']}üöÄ Starting enhanced download with lung segmentation{ANSI['W']}\")\n",
        "    print(f\"Total files to process: {total_files}\")\n",
        "    print(f\"Destination: {download_path}\")\n",
        "    print(f\"ü´Å Lung segmentation: {'ENABLED' if USE_LUNG_SEGMENTATION else 'DISABLED'}\")\n",
        "    \n",
        "    for i, file_id in enumerate(file_ids):\n",
        "        progress = ((i + 1) / total_files) * 100\n",
        "        print(f\"{ANSI['B']}‚¨áÔ∏è Downloading file {file_id} (Progress: {progress:.1f}% - {i+1}/{total_files}) from ArchiMed{ANSI['W']}\")\n",
        "        \n",
        "        try:\n",
        "            # Download file (may return file path string or BytesIO object)\n",
        "            result = a3conn.downloadFile(file_id, download_path)\n",
        "            \n",
        "            if result:\n",
        "                # Handle different return types from ArchiMed API\n",
        "                if isinstance(result, str):\n",
        "                    # Result is a file path\n",
        "                    if os.path.exists(result):\n",
        "                        downloaded_files.append(result)\n",
        "                else:\n",
        "                    # Result is likely a BytesIO object - write to file\n",
        "                    try:\n",
        "                        # Import io module for BytesIO handling\n",
        "                        import io\n",
        "                        \n",
        "                        if hasattr(result, 'read') or isinstance(result, io.BytesIO):\n",
        "                            # Create file path\n",
        "                            file_path = os.path.join(download_path, f\"{file_id}.dcm\")\n",
        "                            \n",
        "                            # Write BytesIO content to file\n",
        "                            with open(file_path, 'wb') as f:\n",
        "                                if hasattr(result, 'getvalue'):\n",
        "                                    # BytesIO object\n",
        "                                    f.write(result.getvalue())\n",
        "                                elif hasattr(result, 'read'):\n",
        "                                    # File-like object\n",
        "                                    f.write(result.read())\n",
        "                            \n",
        "                            # Verify file was created\n",
        "                            if os.path.exists(file_path):\n",
        "                                downloaded_files.append(file_path)\n",
        "                                print(f\"{ANSI['G']}‚úÖ Converted BytesIO to file: {file_path}{ANSI['W']}\")\n",
        "                            else:\n",
        "                                print(f\"{ANSI['Y']}‚ö†Ô∏è Failed to write BytesIO to file for {file_id}{ANSI['W']}\")\n",
        "                        else:\n",
        "                            print(f\"{ANSI['Y']}‚ö†Ô∏è Unknown result type for {file_id}: {type(result)}{ANSI['W']}\")\n",
        "                            \n",
        "                    except Exception as write_error:\n",
        "                        print(f\"{ANSI['R']}‚ùå Failed to write BytesIO for {file_id}: {write_error}{ANSI['W']}\")\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f\"{ANSI['R']}‚ùå Failed to download {file_id}: {e}{ANSI['W']}\")\n",
        "    \n",
        "    print(f\"{ANSI['G']}‚úÖ Downloaded {len(downloaded_files)} files successfully{ANSI['W']}\")\n",
        "    return downloaded_files\n",
        "\n",
        "print(f\"{ANSI['G']}‚úÖ Helper functions loaded{ANSI['W']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üöÄ MAIN EXECUTION - FIXED VERSION\n",
        "print(f\"{ANSI['M']}üöÄ Starting enhanced ArchiMed download with lung segmentation...{ANSI['W']}\")\n",
        "\n",
        "# Get user info (FIXED: correct method name)\n",
        "user_info = get_auth_info()\n",
        "\n",
        "if user_info:\n",
        "    # Load CSV data\n",
        "    df = load_csv_data()\n",
        "    \n",
        "    if df is not None:\n",
        "        # FIXED: Use correct column name from CSV\n",
        "        # Check available columns first\n",
        "        print(f\"{ANSI['B']}üìä Available columns: {list(df.columns)}{ANSI['W']}\")\n",
        "        \n",
        "        # Get file IDs (first 25 for testing) - using correct column name\n",
        "        if 'FileID' in df.columns:\n",
        "            file_ids = df['FileID'].head(25).tolist()\n",
        "        elif 'file_id' in df.columns:\n",
        "            file_ids = df['file_id'].head(25).tolist()\n",
        "        elif 'File_ID' in df.columns:\n",
        "            file_ids = df['File_ID'].head(25).tolist()\n",
        "        else:\n",
        "            print(f\"{ANSI['R']}‚ùå Could not find file ID column. Available columns: {list(df.columns)}{ANSI['W']}\")\n",
        "            file_ids = []\n",
        "        \n",
        "        # Download files\n",
        "        downloaded_files = download_archimed_files(file_ids, DOWNLOAD_PATH)\n",
        "        \n",
        "        if downloaded_files and CONVERT:\n",
        "            # Convert with enhanced processing\n",
        "            conversion_results = enhanced_dicom_to_png_with_segmentation(\n",
        "                downloaded_files, \n",
        "                IMAGES_PATH, \n",
        "                batch_size=BATCH_SIZE,\n",
        "                segmentation_pipeline=segmentation_pipeline\n",
        "            )\n",
        "            \n",
        "            # Summary\n",
        "            successful = len([r for r in conversion_results if r['status'] == 'success'])\n",
        "            failed = len([r for r in conversion_results if r['status'] == 'error'])\n",
        "            \n",
        "            print(f\"\\n{ANSI['G']}üéâ FIXED VERSION COMPLETE!{ANSI['W']}\")\n",
        "            print(f\"{ANSI['G']}‚úÖ Successfully processed: {successful} files{ANSI['W']}\")\n",
        "            if failed > 0:\n",
        "                print(f\"{ANSI['Y']}‚ö†Ô∏è  Failed to process: {failed} files{ANSI['W']}\")\n",
        "            print(f\"{ANSI['B']}ü´Å Lung segmentation: {'ENABLED' if USE_LUNG_SEGMENTATION else 'DISABLED (SAFE MODE)'}{ANSI['W']}\")\n",
        "            print(f\"{ANSI['C']}üìÅ Images saved to: {IMAGES_PATH}{ANSI['W']}\")\n",
        "            \n",
        "        else:\n",
        "            print(f\"{ANSI['Y']}‚ö†Ô∏è No files to convert or conversion disabled{ANSI['W']}\")\n",
        "    \n",
        "    else:\n",
        "        print(f\"{ANSI['R']}‚ùå Failed to load CSV data{ANSI['W']}\")\n",
        "else:\n",
        "    print(f\"{ANSI['R']}‚ùå Authentication failed{ANSI['W']}\")\n",
        "\n",
        "print(f\"\\n{ANSI['M']}üí° To enable segmentation: Set USE_LUNG_SEGMENTATION = True in cell 2{ANSI['W']}\")\n",
        "print(f\"{ANSI['M']}üîß Current mode: {'SEGMENTATION ENABLED' if USE_LUNG_SEGMENTATION else 'SAFE MODE (segmentation disabled)'}{ANSI['W']}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
