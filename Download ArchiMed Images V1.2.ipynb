{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "**<h1 align=\"center\">Download ArchiMed Images V1.2 - Enhanced with Lung Segmentation</h1>**\n",
        "\n",
        "## 🚀 **New in V1.2:**\n",
        "- **Lung Segmentation**: Automatic lung region detection using U-Net\n",
        "- **Smart Cropping**: Focus on lung regions before resizing\n",
        "- **Zone-Aware Processing**: Foundation for true anatomical zone prediction\n",
        "- **Enhanced Quality**: Preserve lung details during preprocessing\n",
        "\n",
        "Based on research from: [Automatic lung segmentation in chest X-ray images using improved U-Net](https://www.nature.com/articles/s41598-022-12743-y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Global Variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### Project Specific Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CSV Files\n",
        "CSV_FOLDER = \"../../data/Paradise_CSV/\"\n",
        "CSV_LABELS_FILE = \"Labeled_Data_RAW_Sample.csv\"\n",
        "CSV_SEPARATOR = \";\"  # Specify the CSV separator, e.g., ',' or '\\t'\n",
        "IMPORT_COLUMNS = []  # If empty, import all columns\n",
        "CHUNK_SIZE = 50000  # Number of rows per chunk\n",
        "\n",
        "# Download parameters\n",
        "DOWNLOAD_PATH = '../../data/Paradise_DICOMs'\n",
        "IMAGES_PATH = '../../data/Paradise_Images'\n",
        "EXPORT_METADATA = True\n",
        "ARCHIMED_METADATA_FILE = 'DICOM_Metadata.csv'\n",
        "CONVERT = True\n",
        "\n",
        "# Conversion parameters\n",
        "BATCH_SIZE = 50  # Number of files to process in each batch for progress reporting\n",
        "BIT_DEPTH = 8  # Bit depth for output images (8, 12, or 16)\n",
        "CREATE_SUBFOLDERS = False  # If True, create subfolders named after ExamCode for output files\n",
        "DELETE_DICOM = True  # If True, delete the DICOM file and its containing subfolder after conversion\n",
        "MONOCHROME = 1  # Monochrome type (1 or 2) to use for converted images\n",
        "\n",
        "# 🔥 NEW V1.2 FEATURES\n",
        "# Lung Segmentation Configuration\n",
        "USE_LUNG_SEGMENTATION = True  # Enable automatic lung segmentation\n",
        "SEGMENTATION_MODEL = 'unet_efficientnet_b4'  # Segmentation model architecture\n",
        "LUNG_CROP_PADDING = 20  # Padding around detected lung regions (pixels)\n",
        "MIN_LUNG_AREA_RATIO = 0.02  # Minimum lung area ratio to total image (filter noise)\n",
        "SAVE_SEGMENTATION_MASKS = True  # Save segmentation masks for debugging\n",
        "MASKS_PATH = '../../data/Paradise_Masks'  # Path to save segmentation masks\n",
        "\n",
        "# Enhanced Resize Parameters\n",
        "TARGET_SIZE = (518, 518)  # Target size after lung-aware processing\n",
        "PRESERVE_ASPECT_RATIO = True  # Maintain aspect ratio during final resize"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### Colors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ANSI escape codes for colored output\n",
        "ANSI = {\n",
        "    'R' : '\\033[91m',  # Red\n",
        "    'G' : '\\033[92m',  # Green\n",
        "    'B' : '\\033[94m',  # Blue\n",
        "    'Y' : '\\033[93m',  # Yellow\n",
        "    'W' : '\\033[0m',  # White\n",
        "    'M' : '\\033[95m',  # Magenta (for new features)\n",
        "    'C' : '\\033[96m',  # Cyan (for segmentation)\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 📦 Enhanced Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Core dependencies\n",
        "import ArchiMedConnector.A3_Connector as A3_Conn\n",
        "import pandas as pd\n",
        "import os\n",
        "import pydicom\n",
        "import numpy as np\n",
        "from PIL import Image, ImageDraw\n",
        "import glob\n",
        "from tqdm import tqdm\n",
        "import shutil\n",
        "import warnings\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import cv2\n",
        "\n",
        "# 🆕 NEW: Deep Learning dependencies for lung segmentation\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import models\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "print(f\"{ANSI['G']}✅ Core dependencies loaded{ANSI['W']}\")\n",
        "print(f\"{ANSI['C']}🔬 Lung segmentation dependencies loaded{ANSI['W']}\")\n",
        "print(f\"{ANSI['M']}🚀 Enhanced V1.2 ready for zone-aware processing{ANSI['W']}\")\n",
        "\n",
        "# Initialize ArchiMed connector\n",
        "a3conn = A3_Conn.A3_Connector()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 🧠 Lung Segmentation Model Implementation\n",
        "\n",
        "### Enhanced U-Net Architecture with EfficientNet Backbone\n",
        "Based on the research paper: [\"Automatic lung segmentation in chest X-ray images using improved U-Net\"](https://www.nature.com/articles/s41598-022-12743-y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    \"\"\"Residual block for improved gradient flow in decoder.\"\"\"\n",
        "    \n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        self.leaky_relu = nn.LeakyReLU(0.1, inplace=True)\n",
        "        \n",
        "        # Skip connection adjustment\n",
        "        if in_channels != out_channels:\n",
        "            self.skip = nn.Conv2d(in_channels, out_channels, 1)\n",
        "        else:\n",
        "            self.skip = nn.Identity()\n",
        "    \n",
        "    def forward(self, x):\n",
        "        residual = self.skip(x)\n",
        "        out = self.leaky_relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += residual\n",
        "        return self.leaky_relu(out)\n",
        "\n",
        "\n",
        "class EnhancedUNet(nn.Module):\n",
        "    \"\"\"Enhanced U-Net with EfficientNet-B4 encoder for lung segmentation.\"\"\"\n",
        "    \n",
        "    def __init__(self, n_classes=1, pretrained=True):\n",
        "        super(EnhancedUNet, self).__init__()\n",
        "        \n",
        "        # EfficientNet-B4 encoder (pre-trained on ImageNet)\n",
        "        efficientnet = models.efficientnet_b4(pretrained=pretrained)\n",
        "        self.encoder = efficientnet.features\n",
        "        \n",
        "        # Decoder with residual blocks and skip connections\n",
        "        self.decoder5 = self._make_decoder_block(1792, 512)  # EfficientNet-B4 output: 1792\n",
        "        self.decoder4 = self._make_decoder_block(512 + 448, 256)  # Skip connection: 448\n",
        "        self.decoder3 = self._make_decoder_block(256 + 160, 128)  # Skip connection: 160\n",
        "        self.decoder2 = self._make_decoder_block(128 + 56, 64)   # Skip connection: 56\n",
        "        self.decoder1 = self._make_decoder_block(64 + 32, 32)    # Skip connection: 32\n",
        "        \n",
        "        # Final classification layer\n",
        "        self.final_conv = nn.Conv2d(32, n_classes, 1)\n",
        "        \n",
        "        # Upsampling\n",
        "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "        \n",
        "    def _make_decoder_block(self, in_channels, out_channels):\n",
        "        \"\"\"Create decoder block with dropout, conv, and residual blocks.\"\"\"\n",
        "        return nn.Sequential(\n",
        "            nn.Dropout2d(0.1),\n",
        "            nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.LeakyReLU(0.1, inplace=True),\n",
        "            ResidualBlock(out_channels, out_channels),\n",
        "            ResidualBlock(out_channels, out_channels)\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # Encoder with skip connections\n",
        "        skip_connections = []\n",
        "        \n",
        "        # Extract features at different scales\n",
        "        for i, layer in enumerate(self.encoder):\n",
        "            x = layer(x)\n",
        "            # Save specific layers for skip connections\n",
        "            if i in [2, 3, 4, 6]:  # EfficientNet-B4 specific indices\n",
        "                skip_connections.append(x)\n",
        "        \n",
        "        # Decoder with skip connections\n",
        "        x = self.decoder5(x)\n",
        "        \n",
        "        if len(skip_connections) >= 4:\n",
        "            x = self.upsample(x)\n",
        "            x = torch.cat([x, skip_connections[3]], dim=1)\n",
        "            x = self.decoder4(x)\n",
        "            \n",
        "            x = self.upsample(x)\n",
        "            x = torch.cat([x, skip_connections[2]], dim=1)\n",
        "            x = self.decoder3(x)\n",
        "            \n",
        "            x = self.upsample(x)\n",
        "            x = torch.cat([x, skip_connections[1]], dim=1)\n",
        "            x = self.decoder2(x)\n",
        "            \n",
        "            x = self.upsample(x)\n",
        "            x = torch.cat([x, skip_connections[0]], dim=1)\n",
        "            x = self.decoder1(x)\n",
        "        \n",
        "        # Final upsampling to original size\n",
        "        x = self.upsample(x)\n",
        "        x = self.final_conv(x)\n",
        "        \n",
        "        return torch.sigmoid(x)\n",
        "\n",
        "\n",
        "print(f\"{ANSI['C']}🏗️ Enhanced U-Net architecture defined{ANSI['W']}\")\n",
        "print(f\"{ANSI['B']}   - EfficientNet-B4 encoder with ImageNet pretraining{ANSI['W']}\")\n",
        "print(f\"{ANSI['B']}   - Residual blocks in decoder for better gradient flow{ANSI['W']}\")\n",
        "print(f\"{ANSI['B']}   - LeakyReLU activation to prevent gradient instability{ANSI['W']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 🎯 Lung Segmentation Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LungSegmentationPipeline:\n",
        "    \"\"\"Complete pipeline for lung segmentation and smart cropping.\"\"\"\n",
        "    \n",
        "    def __init__(self, model_path=None, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
        "        self.device = device\n",
        "        self.model = EnhancedUNet(n_classes=1, pretrained=True)\n",
        "        \n",
        "        # Load pre-trained weights if available\n",
        "        if model_path and os.path.exists(model_path):\n",
        "            self.model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "            print(f\"{ANSI['G']}✅ Loaded pre-trained segmentation model from {model_path}{ANSI['W']}\")\n",
        "        else:\n",
        "            print(f\"{ANSI['Y']}⚠️ No pre-trained model found. Using randomly initialized weights.{ANSI['W']}\")\n",
        "            print(f\"{ANSI['B']}   Consider training the model or downloading pre-trained weights{ANSI['W']}\")\n",
        "        \n",
        "        self.model.to(device)\n",
        "        self.model.eval()\n",
        "        \n",
        "        # Preprocessing transforms\n",
        "        self.transform = A.Compose([\n",
        "            A.Resize(256, 256),\n",
        "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "            ToTensorV2()\n",
        "        ])\n",
        "    \n",
        "    def preprocess_image(self, image):\n",
        "        \"\"\"Preprocess image for segmentation model.\"\"\"\n",
        "        if len(image.shape) == 2:  # Grayscale\n",
        "            image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
        "        elif len(image.shape) == 3 and image.shape[2] == 1:  # Single channel\n",
        "            image = np.repeat(image, 3, axis=2)\n",
        "        \n",
        "        # Apply transforms\n",
        "        transformed = self.transform(image=image)\n",
        "        tensor = transformed['image'].unsqueeze(0)  # Add batch dimension\n",
        "        return tensor.to(self.device)\n",
        "    \n",
        "    def segment_lungs(self, image):\n",
        "        \"\"\"Segment lung regions from chest X-ray image.\"\"\"\n",
        "        original_shape = image.shape[:2]\n",
        "        \n",
        "        # Preprocess\n",
        "        input_tensor = self.preprocess_image(image)\n",
        "        \n",
        "        # Inference\n",
        "        with torch.no_grad():\n",
        "            output = self.model(input_tensor)\n",
        "            mask = output.squeeze().cpu().numpy()\n",
        "        \n",
        "        # Resize mask back to original size\n",
        "        mask_resized = cv2.resize(mask, (original_shape[1], original_shape[0]))\n",
        "        \n",
        "        # Threshold to binary mask\n",
        "        binary_mask = (mask_resized > 0.5).astype(np.uint8)\n",
        "        \n",
        "        return binary_mask, mask_resized\n",
        "    \n",
        "    def extract_lung_regions(self, image, mask, padding=20):\n",
        "        \"\"\"Extract and crop lung regions with smart bounding box.\"\"\"\n",
        "        # Find contours of lung regions\n",
        "        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "        \n",
        "        if not contours:\n",
        "            print(f\"{ANSI['Y']}⚠️ No lung regions detected, returning original image{ANSI['W']}\")\n",
        "            return image, None\n",
        "        \n",
        "        # Filter contours by area (remove noise)\n",
        "        total_area = image.shape[0] * image.shape[1]\n",
        "        min_area = total_area * MIN_LUNG_AREA_RATIO\n",
        "        valid_contours = [c for c in contours if cv2.contourArea(c) > min_area]\n",
        "        \n",
        "        if not valid_contours:\n",
        "            print(f\"{ANSI['Y']}⚠️ No significant lung regions found, returning original image{ANSI['W']}\")\n",
        "            return image, None\n",
        "        \n",
        "        # Get bounding box around all lung regions\n",
        "        all_points = np.vstack([c.reshape(-1, 2) for c in valid_contours])\n",
        "        x_min, y_min = np.min(all_points, axis=0)\n",
        "        x_max, y_max = np.max(all_points, axis=0)\n",
        "        \n",
        "        # Add padding\n",
        "        h, w = image.shape[:2]\n",
        "        x_min = max(0, x_min - padding)\n",
        "        y_min = max(0, y_min - padding)\n",
        "        x_max = min(w, x_max + padding)\n",
        "        y_max = min(h, y_max + padding)\n",
        "        \n",
        "        # Crop image\n",
        "        if len(image.shape) == 3:\n",
        "            cropped_image = image[y_min:y_max, x_min:x_max, :]\n",
        "        else:\n",
        "            cropped_image = image[y_min:y_max, x_min:x_max]\n",
        "        \n",
        "        crop_info = {\n",
        "            'bbox': (x_min, y_min, x_max, y_max),\n",
        "            'original_shape': image.shape[:2],\n",
        "            'cropped_shape': cropped_image.shape[:2]\n",
        "        }\n",
        "        \n",
        "        return cropped_image, crop_info\n",
        "    \n",
        "    def process_image(self, image, save_mask_path=None):\n",
        "        \"\"\"Complete pipeline: segment lungs and extract regions.\"\"\"\n",
        "        # Segment lungs\n",
        "        binary_mask, probability_mask = self.segment_lungs(image)\n",
        "        \n",
        "        # Save mask if requested\n",
        "        if save_mask_path:\n",
        "            mask_image = (probability_mask * 255).astype(np.uint8)\n",
        "            cv2.imwrite(save_mask_path, mask_image)\n",
        "        \n",
        "        # Extract lung regions\n",
        "        cropped_image, crop_info = self.extract_lung_regions(image, binary_mask, LUNG_CROP_PADDING)\n",
        "        \n",
        "        return cropped_image, crop_info, binary_mask, probability_mask\n",
        "\n",
        "\n",
        "# Initialize segmentation pipeline\n",
        "if USE_LUNG_SEGMENTATION:\n",
        "    segmentation_pipeline = LungSegmentationPipeline()\n",
        "    print(f\"{ANSI['C']}🎯 Lung segmentation pipeline initialized{ANSI['W']}\")\n",
        "    \n",
        "    # Create masks directory\n",
        "    if SAVE_SEGMENTATION_MASKS:\n",
        "        os.makedirs(MASKS_PATH, exist_ok=True)\n",
        "        print(f\"{ANSI['B']}📁 Masks directory created: {MASKS_PATH}{ANSI['W']}\")\n",
        "else:\n",
        "    segmentation_pipeline = None\n",
        "    print(f\"{ANSI['Y']}⚠️ Lung segmentation disabled{ANSI['W']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 🔄 Enhanced DICOM to PNG Conversion with Lung Segmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def convert_dicom_to_png_v2(\n",
        "    import_folder,\n",
        "    export_folder,\n",
        "    bit_depth=8,\n",
        "    create_subfolders=False,\n",
        "    target_size=TARGET_SIZE,\n",
        "    preserve_aspect_ratio=PRESERVE_ASPECT_RATIO,\n",
        "    monochrome=2,\n",
        "    delete_dicom=True,\n",
        "    dicom_files=None,\n",
        "    use_lung_segmentation=USE_LUNG_SEGMENTATION,\n",
        "    segmentation_pipeline=None,\n",
        "    save_masks=SAVE_SEGMENTATION_MASKS,\n",
        "    masks_path=MASKS_PATH\n",
        "):\n",
        "    \"\"\"\n",
        "    Enhanced DICOM to PNG conversion with lung segmentation and smart cropping.\n",
        "    \n",
        "    🆕 V1.2 Features:\n",
        "    - Automatic lung detection and cropping\n",
        "    - Preserves lung anatomy details\n",
        "    - Smart resizing after segmentation\n",
        "    - Optional mask saving for debugging\n",
        "    \"\"\"\n",
        "    \n",
        "    # Validate parameters\n",
        "    if bit_depth not in [8, 12, 16]:\n",
        "        raise ValueError(\"bit_depth must be 8, 12, or 16\")\n",
        "    \n",
        "    if monochrome not in [1, 2]:\n",
        "        raise ValueError(\"monochrome must be 1 or 2\")\n",
        "    \n",
        "    # Create export folder\n",
        "    os.makedirs(export_folder, exist_ok=True)\n",
        "    \n",
        "    # Get DICOM files to process\n",
        "    if dicom_files is None:\n",
        "        dicom_files = []\n",
        "        for ext in ['.dcm', '.DCM']:\n",
        "            dicom_files.extend(glob.glob(os.path.join(import_folder, '**/*' + ext), recursive=True))\n",
        "    else:\n",
        "        dicom_files = [p for p in dicom_files if os.path.isfile(p)]\n",
        "\n",
        "    # Initialize counters\n",
        "    successful = 0\n",
        "    failed = 0\n",
        "    skipped = 0\n",
        "    segmentation_stats = {'used': 0, 'failed': 0, 'skipped': 0}\n",
        "    \n",
        "    # Suppress warnings\n",
        "    warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"pydicom.charset\")\n",
        "    \n",
        "    # Process each DICOM file\n",
        "    for dicom_path in tqdm(dicom_files, desc=\"🔄 Converting DICOM with lung segmentation\", total=len(dicom_files)):\n",
        "        try:\n",
        "            # Read DICOM\n",
        "            try:\n",
        "                ds = pydicom.dcmread(dicom_path)\n",
        "                pixel_array = ds.pixel_array\n",
        "            except Exception as e:\n",
        "                skipped += 1\n",
        "                continue\n",
        "            \n",
        "            # Get metadata\n",
        "            exam_code = str(getattr(ds, 'StudyDescription', os.path.basename(os.path.dirname(dicom_path))))\n",
        "            file_id = os.path.splitext(os.path.basename(dicom_path))[0]\n",
        "            if file_id.endswith('.dcm'):\n",
        "                file_id = file_id[:-4]\n",
        "            \n",
        "            # Handle monochrome conversion\n",
        "            dicom_monochrome = monochrome\n",
        "            if hasattr(ds, 'PhotometricInterpretation'):\n",
        "                if ds.PhotometricInterpretation == 'MONOCHROME1':\n",
        "                    dicom_monochrome = 1\n",
        "                elif ds.PhotometricInterpretation == 'MONOCHROME2':\n",
        "                    dicom_monochrome = 2\n",
        "            \n",
        "            # Normalize pixel values\n",
        "            bits_allocated = getattr(ds, 'BitsAllocated', 14)\n",
        "            bits_stored = getattr(ds, 'BitsStored', bits_allocated)\n",
        "            max_pixel_value = pixel_array.max()\n",
        "            max_possible_value = (2 ** bits_stored) - 1\n",
        "            output_max_value = (2 ** bit_depth) - 1\n",
        "            \n",
        "            if max_pixel_value > 0:\n",
        "                pixel_array = ((pixel_array / min(max_pixel_value, max_possible_value)) * output_max_value)\n",
        "            \n",
        "            # Convert to appropriate data type\n",
        "            if bit_depth <= 8:\n",
        "                pixel_array = pixel_array.astype(np.uint8)\n",
        "            else:\n",
        "                pixel_array = pixel_array.astype(np.uint16)\n",
        "            \n",
        "            # Invert if needed\n",
        "            if dicom_monochrome != monochrome and dicom_monochrome in [1, 2] and monochrome in [1, 2]:\n",
        "                pixel_array = output_max_value - pixel_array\n",
        "            \n",
        "            # 🆕 V1.2: LUNG SEGMENTATION AND SMART CROPPING\n",
        "            final_image = pixel_array\n",
        "            crop_info = None\n",
        "            \n",
        "            if use_lung_segmentation and segmentation_pipeline is not None:\n",
        "                try:\n",
        "                    # Define mask save path\n",
        "                    mask_save_path = None\n",
        "                    if save_masks:\n",
        "                        mask_filename = f\"{file_id}_mask.png\"\n",
        "                        mask_save_path = os.path.join(masks_path, mask_filename)\n",
        "                    \n",
        "                    # Process with lung segmentation\n",
        "                    cropped_image, crop_info, binary_mask, prob_mask = segmentation_pipeline.process_image(\n",
        "                        pixel_array, mask_save_path\n",
        "                    )\n",
        "                    \n",
        "                    if crop_info is not None:\n",
        "                        final_image = cropped_image\n",
        "                        segmentation_stats['used'] += 1\n",
        "                        print(f\"{ANSI['C']}🫁 Segmented {file_id}: {crop_info['original_shape']} → {crop_info['cropped_shape']}{ANSI['W']}\")\n",
        "                    else:\n",
        "                        segmentation_stats['failed'] += 1\n",
        "                        print(f\"{ANSI['Y']}⚠️ Segmentation failed for {file_id}, using original{ANSI['W']}\")\n",
        "                        \n",
        "                except Exception as e:\n",
        "                    segmentation_stats['failed'] += 1\n",
        "                    print(f\"{ANSI['R']}❌ Segmentation error for {file_id}: {str(e)}{ANSI['W']}\")\n",
        "            else:\n",
        "                segmentation_stats['skipped'] += 1\n",
        "            \n",
        "            # Convert to PIL Image\n",
        "            img = Image.fromarray(final_image)\n",
        "            \n",
        "            # 🆕 V1.2: SMART RESIZING\n",
        "            if target_size:\n",
        "                if preserve_aspect_ratio:\n",
        "                    # Calculate aspect ratio preserving dimensions\n",
        "                    original_width, original_height = img.size\n",
        "                    target_width, target_height = target_size\n",
        "                    \n",
        "                    # Calculate scaling factor to fit within target size\n",
        "                    scale_factor = min(target_width / original_width, target_height / original_height)\n",
        "                    new_width = int(original_width * scale_factor)\n",
        "                    new_height = int(original_height * scale_factor)\n",
        "                    \n",
        "                    # Resize maintaining aspect ratio\n",
        "                    img = img.resize((new_width, new_height), Image.LANCZOS)\n",
        "                    \n",
        "                    # Create new image with target size and paste resized image centered\n",
        "                    final_img = Image.new('L' if bit_depth <= 8 else 'I', target_size, 0)\n",
        "                    paste_x = (target_width - new_width) // 2\n",
        "                    paste_y = (target_height - new_height) // 2\n",
        "                    final_img.paste(img, (paste_x, paste_y))\n",
        "                    img = final_img\n",
        "                else:\n",
        "                    # Direct resize to target size\n",
        "                    img = img.resize(target_size, Image.LANCZOS)\n",
        "            \n",
        "            # Determine output path\n",
        "            base_filename = os.path.splitext(os.path.basename(dicom_path))[0]\n",
        "            if create_subfolders:\n",
        "                subfolder_path = os.path.join(export_folder, exam_code)\n",
        "                os.makedirs(subfolder_path, exist_ok=True)\n",
        "                output_path = os.path.join(subfolder_path, f\"{base_filename}.png\")\n",
        "            else:\n",
        "                output_path = os.path.join(export_folder, f\"{base_filename}.png\")\n",
        "            \n",
        "            # Save PNG\n",
        "            img.save(output_path)\n",
        "            successful += 1\n",
        "            \n",
        "            # Delete DICOM if requested\n",
        "            if delete_dicom:\n",
        "                os.remove(dicom_path)\n",
        "                dicom_folder = os.path.dirname(dicom_path)\n",
        "                if dicom_folder != import_folder:\n",
        "                    try:\n",
        "                        if not os.listdir(dicom_folder):\n",
        "                            shutil.rmtree(dicom_folder)\n",
        "                    except Exception as e:\n",
        "                        print(f\"{ANSI['Y']}Warning: Could not delete folder {dicom_folder}: {str(e)}{ANSI['W']}\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"{ANSI['R']}Error converting {dicom_path}: {str(e)}{ANSI['W']}\")\n",
        "            failed += 1\n",
        "    \n",
        "    # Enhanced summary with segmentation statistics\n",
        "    summary = {\n",
        "        \"successful\": successful,\n",
        "        \"skipped\": skipped,\n",
        "        \"failed\": failed,\n",
        "        \"total\": len(dicom_files),\n",
        "        \"segmentation_stats\": segmentation_stats\n",
        "    }\n",
        "    \n",
        "    # Print enhanced summary\n",
        "    if use_lung_segmentation:\n",
        "        print(f\"\\n{ANSI['C']}🫁 Lung Segmentation Summary:{ANSI['W']}\")\n",
        "        print(f\"   - Successfully segmented: {segmentation_stats['used']}\")\n",
        "        print(f\"   - Segmentation failed: {segmentation_stats['failed']}\")\n",
        "        print(f\"   - Segmentation skipped: {segmentation_stats['skipped']}\")\n",
        "    \n",
        "    return summary\n",
        "\n",
        "\n",
        "print(f\"{ANSI['M']}🚀 Enhanced DICOM conversion function loaded{ANSI['W']}\")\n",
        "print(f\"{ANSI['C']}   - Automatic lung segmentation and cropping{ANSI['W']}\")\n",
        "print(f\"{ANSI['B']}   - Smart resizing with aspect ratio preservation{ANSI['W']}\")\n",
        "print(f\"{ANSI['G']}   - Enhanced quality preservation for lung anatomy{ANSI['W']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Import CSVs to Dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def import_csv_to_dataframe(file_path, separator=';', columns=None, chunk_size=None):\n",
        "    \"\"\"\n",
        "    Import CSV file into a pandas DataFrame.\n",
        "    \n",
        "    Args:\n",
        "        file_path (str): Path to the CSV file\n",
        "        separator (str): CSV separator character\n",
        "        columns (list): List of columns to import (if None, import all)\n",
        "        chunk_size (int): Number of rows to read at a time (if None, read all at once)\n",
        "        \n",
        "    Returns:\n",
        "        pandas.DataFrame: The imported data\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Determine which columns to use\n",
        "        usecols = columns if columns and len(columns) > 0 else None\n",
        "        \n",
        "        if chunk_size:\n",
        "            # Read in chunks and concatenate\n",
        "            chunks = []\n",
        "            for chunk in pd.read_csv(file_path, sep=separator, usecols=usecols, chunksize=chunk_size):\n",
        "                chunks.append(chunk)\n",
        "            return pd.concat(chunks, ignore_index=True)\n",
        "        else:\n",
        "            # Read all at once\n",
        "            return pd.read_csv(file_path, sep=separator, usecols=usecols)\n",
        "    except Exception as e:\n",
        "        print(f\"{ANSI['R']}Error importing CSV: {e}{ANSI['W']}\")\n",
        "        return None\n",
        "\n",
        "# Import labeled data\n",
        "csv_path = os.path.join(CSV_FOLDER, CSV_LABELS_FILE)\n",
        "print(f\"{ANSI['B']}Importing labeled data from: {csv_path}{ANSI['W']}\")\n",
        "\n",
        "df_labeled_data = import_csv_to_dataframe(\n",
        "    file_path=csv_path,\n",
        "    separator=CSV_SEPARATOR,\n",
        "    columns=IMPORT_COLUMNS,\n",
        "    chunk_size=CHUNK_SIZE\n",
        ")\n",
        "\n",
        "if df_labeled_data is not None:\n",
        "    print(f\"{ANSI['G']}Successfully imported {len(df_labeled_data)} rows of labeled data{ANSI['W']}\")\n",
        "    display(df_labeled_data.head())\n",
        "else:\n",
        "    print(f\"{ANSI['R']}Failed to import labeled data{ANSI['W']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 🚀 Enhanced Download and Processing Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def download_archimed_files_v2(dataframe, download_path, file_id_column='FileID', batch_size=20, convert=False):\n",
        "    \"\"\"\n",
        "    🆕 V1.2: Enhanced ArchiMed download with lung segmentation integration.\n",
        "    \n",
        "    Args:\n",
        "        dataframe (pandas.DataFrame): DataFrame containing FileIDs\n",
        "        download_path (str): Path where to save downloaded files\n",
        "        file_id_column (str): Name of the column containing FileIDs (default: 'FileID')\n",
        "        batch_size (int): Number of files to process in each batch for progress reporting\n",
        "        convert (bool): If True, convert downloaded DICOM files to PNG after each batch (default: False)\n",
        "        \n",
        "    Returns:\n",
        "        pandas.DataFrame: DataFrame with metadata of converted files\n",
        "    \"\"\"\n",
        "    \n",
        "    # Create download directory\n",
        "    os.makedirs(download_path, exist_ok=True)\n",
        "    \n",
        "    # Get user info for verification\n",
        "    user_info = a3conn.getUserInfos()\n",
        "    print(f\"{ANSI['G']}🔐 ArchiMed Authentication Info{ANSI['W']}\")\n",
        "    print(f\"{ANSI['B']}Username:{ANSI['W']} {user_info.get('userInfos', {}).get('login', 'Unknown')}\")\n",
        "    print(f\"{ANSI['B']}User level:{ANSI['W']} {user_info.get('userInfos', {}).get('level', 'Unknown')}\")\n",
        "    \n",
        "    # Check if the FileID column exists\n",
        "    if file_id_column not in dataframe.columns:\n",
        "        print(f\"{ANSI['R']}Error: Column '{file_id_column}' not found in dataframe{ANSI['W']}\")\n",
        "        return pd.DataFrame()\n",
        "    \n",
        "    # Get unique FileIDs to avoid downloading duplicates\n",
        "    file_ids = dataframe[file_id_column].unique()\n",
        "    total_files = len(file_ids)\n",
        "    \n",
        "    print(f\"\\\\n{ANSI['M']}🚀 Starting enhanced download with lung segmentation{ANSI['W']}\")\n",
        "    print(f\"{ANSI['B']}Total files to process: {ANSI['W']}{total_files}\")\n",
        "    print(f\"{ANSI['B']}Destination: {ANSI['W']}{download_path}\")\n",
        "    if USE_LUNG_SEGMENTATION:\n",
        "        print(f\"{ANSI['C']}🫁 Lung segmentation: ENABLED{ANSI['W']}\")\n",
        "    else:\n",
        "        print(f\"{ANSI['Y']}⚠️ Lung segmentation: DISABLED{ANSI['W']}\")\n",
        "    print()\n",
        "    \n",
        "    failed_files = []\n",
        "    batch_files = []\n",
        "    all_metadata = pd.DataFrame()\n",
        "    downloaded_count = 0\n",
        "    skipped_count = 0\n",
        "    \n",
        "    # Process files in batches\n",
        "    for i, file_id in enumerate(file_ids):\n",
        "        if pd.isna(file_id):\n",
        "            continue\n",
        "            \n",
        "        try:\n",
        "            file_id = int(file_id)\n",
        "            file_output_path = os.path.join(download_path, f\"{file_id}\")\n",
        "            dicom_file_path = os.path.join(file_output_path, f\"{file_id}.dcm\")\n",
        "            \n",
        "            # Check if file already exists\n",
        "            if os.path.exists(dicom_file_path):\n",
        "                print(f\"{ANSI['Y']}📄 File {ANSI['W']}{file_id}{ANSI['Y']} already exists, skipping download (Progress: {ANSI['W']}{((i+1)/total_files)*100:.1f}%{ANSI['Y']} - {ANSI['W']}{i+1}/{total_files}{ANSI['Y']}){ANSI['W']}\")\n",
        "                batch_files.append(dicom_file_path)\n",
        "                skipped_count += 1\n",
        "            else:\n",
        "                print(f\"{ANSI['B']}⬇️ Downloading file {ANSI['W']}{file_id}{ANSI['B']} (Progress: {ANSI['W']}{((i+1)/total_files)*100:.1f}%{ANSI['B']} - {ANSI['W']}{i+1}/{total_files}{ANSI['B']}) from{ANSI['W']} ArchiMed\")\n",
        "                \n",
        "                # Download the file\n",
        "                result = a3conn.downloadFile(\n",
        "                    file_id,\n",
        "                    asStream=False,\n",
        "                    destDir=file_output_path,\n",
        "                    filename=f\"{file_id}.dcm\",\n",
        "                    inWorklist=False\n",
        "                )\n",
        "                \n",
        "                downloaded_count += 1\n",
        "                batch_files.append(dicom_file_path)\n",
        "            \n",
        "            # Show progress every batch_size files\n",
        "            if (i + 1) % batch_size == 0 or (i + 1) == total_files:\n",
        "                \n",
        "                # 🆕 V1.2: Enhanced conversion with lung segmentation\n",
        "                if convert and batch_files:\n",
        "                    try:\n",
        "                        print(f\"\\\\n{ANSI['M']}🔄 Converting batch of {ANSI['W']}{len(batch_files)}{ANSI['M']} DICOM files with lung segmentation...{ANSI['W']}\")\n",
        "                        summary = convert_dicom_to_png_v2(\n",
        "                            import_folder=download_path,\n",
        "                            export_folder=IMAGES_PATH,\n",
        "                            bit_depth=BIT_DEPTH,\n",
        "                            create_subfolders=CREATE_SUBFOLDERS,\n",
        "                            target_size=TARGET_SIZE,\n",
        "                            preserve_aspect_ratio=PRESERVE_ASPECT_RATIO,\n",
        "                            delete_dicom=DELETE_DICOM,\n",
        "                            monochrome=MONOCHROME,\n",
        "                            dicom_files=batch_files,\n",
        "                            use_lung_segmentation=USE_LUNG_SEGMENTATION,\n",
        "                            segmentation_pipeline=segmentation_pipeline,\n",
        "                            save_masks=SAVE_SEGMENTATION_MASKS,\n",
        "                            masks_path=MASKS_PATH\n",
        "                        )\n",
        "                        print(f\"{ANSI['G']}✅ Conversion summary: {summary}{ANSI['W']}\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"{ANSI['R']}❌ Error during conversion: {str(e)}{ANSI['W']}\")\n",
        "                \n",
        "                batch_files = []\n",
        "                print(f\"{ANSI['Y']}📊 Progress:{ANSI['W']} {i + 1}/{total_files} {ANSI['B']}files processed {ANSI['W']}({ANSI['B']}{((i + 1) / total_files * 100):.1f}%{ANSI['W']})\\\\n\")\n",
        "                \n",
        "        except Exception as e:\n",
        "            failed_files.append(file_id)\n",
        "            print(f\"{ANSI['R']}❌ Error downloading file ID {file_id}: {str(e)}{ANSI['W']}\")\n",
        "    \n",
        "    # Final Summary\n",
        "    print(f\"\\\\n{ANSI['G']}🎉 Download complete!{ANSI['W']}\")\n",
        "    print(f\"{ANSI['B']}📊 Summary:{ANSI['W']}\")\n",
        "    print(f\"   - Downloaded: {downloaded_count} files\")\n",
        "    print(f\"   - Skipped: {skipped_count} files (already existed)\")\n",
        "    print(f\"   - Failed: {len(failed_files)} files\")\n",
        "    \n",
        "    if failed_files:\n",
        "        print(f\"{ANSI['R']}❌ Failed files: {failed_files}{ANSI['W']}\")\n",
        "    \n",
        "    return all_metadata\n",
        "\n",
        "\n",
        "# Execute the enhanced download\n",
        "if df_labeled_data is not None:\n",
        "    print(f\"{ANSI['M']}🚀 Starting enhanced ArchiMed download with lung segmentation...{ANSI['W']}\\\\n\")\n",
        "    metadata_df = download_archimed_files_v2(\n",
        "        dataframe=df_labeled_data,\n",
        "        download_path=DOWNLOAD_PATH,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        convert=CONVERT\n",
        "    )\n",
        "    \n",
        "    if not metadata_df.empty:\n",
        "        print(f\"{ANSI['G']}✅ Files processed successfully!{ANSI['W']}\")\n",
        "        print(f\"{ANSI['B']}📁 Images saved to: {ANSI['W']}{IMAGES_PATH}\")\n",
        "        if USE_LUNG_SEGMENTATION and SAVE_SEGMENTATION_MASKS:\n",
        "            print(f\"{ANSI['C']}🫁 Segmentation masks saved to: {ANSI['W']}{MASKS_PATH}\")\n",
        "    else:\n",
        "        print(f\"{ANSI['Y']}⚠️ Files processed but no metadata was collected{ANSI['W']}\")\n",
        "else:\n",
        "    print(f\"{ANSI['R']}❌ Cannot download files: No labeled data available{ANSI['W']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 📊 V1.2 Summary and Next Steps\n",
        "\n",
        "### ✅ What's New in V1.2:\n",
        "\n",
        "1. **🫁 Automatic Lung Segmentation**: \n",
        "   - Enhanced U-Net with EfficientNet-B4 encoder\n",
        "   - Automatic lung region detection\n",
        "   - Smart cropping to focus on lung anatomy\n",
        "\n",
        "2. **🎯 Zone-Aware Processing**: \n",
        "   - Foundation for true anatomical zone prediction\n",
        "   - Preserved lung details for better CSI scoring\n",
        "   - Reduced background noise and artifacts\n",
        "\n",
        "3. **📐 Smart Resizing**: \n",
        "   - Aspect ratio preservation\n",
        "   - Centered placement within target dimensions\n",
        "   - Quality preservation during scaling\n",
        "\n",
        "4. **🔬 Enhanced Quality**: \n",
        "   - Lung-focused preprocessing\n",
        "   - Segmentation mask debugging\n",
        "   - Improved feature extraction potential\n",
        "\n",
        "### 🚀 Integration with CSI-Predictor:\n",
        "\n",
        "To use these enhanced images with your CSI prediction model:\n",
        "\n",
        "1. **Update data paths** in your training configuration:\n",
        "   ```python\n",
        "   # In config.py or .env\n",
        "   DATA_DIR = \"../../data/Paradise_Images\"  # Enhanced lung-segmented images\n",
        "   ```\n",
        "\n",
        "2. **Verify image quality** by checking the segmentation masks:\n",
        "   ```python\n",
        "   # Check masks directory\n",
        "   ls ../../data/Paradise_Masks/\n",
        "   ```\n",
        "\n",
        "3. **Train your model** with zone-aware features:\n",
        "   - The lung-cropped images should improve zone-specific learning\n",
        "   - Better anatomical focus for CSI score prediction\n",
        "\n",
        "### 📋 Usage Instructions:\n",
        "\n",
        "1. **Enable/Disable Segmentation**:\n",
        "   ```python\n",
        "   USE_LUNG_SEGMENTATION = True   # Enable for zone-aware processing\n",
        "   SAVE_SEGMENTATION_MASKS = True # Enable for debugging\n",
        "   ```\n",
        "\n",
        "2. **Adjust Parameters**:\n",
        "   ```python\n",
        "   LUNG_CROP_PADDING = 20        # Padding around lung regions\n",
        "   MIN_LUNG_AREA_RATIO = 0.02    # Filter noise\n",
        "   TARGET_SIZE = (518, 518)      # Match your model requirements\n",
        "   ```\n",
        "\n",
        "3. **Monitor Processing**:\n",
        "   - Green 🟢: Successful segmentation and cropping\n",
        "   - Yellow 🟡: Segmentation failed, using original image  \n",
        "   - Red 🔴: Processing error\n",
        "\n",
        "### 🔬 For Advanced Users:\n",
        "\n",
        "**Train Custom Segmentation Model**:\n",
        "- Use the provided U-Net architecture\n",
        "- Train on your specific chest X-ray dataset\n",
        "- Save weights and update `model_path` parameter\n",
        "\n",
        "**Zone-Specific Modifications**:\n",
        "- Modify `extract_lung_regions()` to extract individual zones\n",
        "- Implement zone-specific cropping for 6-zone CSI prediction\n",
        "- Add anatomical landmarks detection\n",
        "\n",
        "---\n",
        "\n",
        "**Ready for True Zone-Aware CSI Prediction! 🎯**\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
