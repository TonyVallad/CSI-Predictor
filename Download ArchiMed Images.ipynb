{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h1 align=\"center\">CSV Preprocessing</h1>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Specific Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV Files\n",
    "CSV_FOLDER = \"../../data/Paradise_CSV/\"\n",
    "# CSV_FOLDER = \"/home/pyuser/data/\"\n",
    "CSV_LABELS_FILE = \"Labeled_Data_RAW_Sample.csv\"\n",
    "CSV_SEPARATOR = \";\"  # Specify the CSV separator, e.g., ',' or '\\t'\n",
    "IMPORT_COLUMNS = []  # If empty, import all columns\n",
    "CHUNK_SIZE = 50000  # Number of rows per chunk\n",
    "\n",
    "# Download parameters\n",
    "DOWNLOAD_PATH = '../../data/Paradise_DICOMs'\n",
    "IMAGES_PATH = '../../data/Paradise_Images'\n",
    "CONVERT = True\n",
    "\n",
    "# Conversion parameters\n",
    "BATCH_SIZE = 10  # Number of files to process in each batch for progress reporting\n",
    "BIT_DEPTH = 8  # Bit depth for output images (8, 12, or 16)\n",
    "CREATE_SUBFOLDERS = False  # If True, create subfolders named after ExamCode for output files\n",
    "DELETE_DICOM = False  # If True, delete the DICOM file and its containing subfolder after conversion\n",
    "MONOCHROME = 1  # Monochrome type (1 or 2) to use for converted images\n",
    "# RESIZE_X = 518\n",
    "RESIZE_Y = 750"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSI escape codes for colored output\n",
    "ANSI = {\n",
    "    'R' : '\\033[91m',  # Red\n",
    "    'G' : '\\033[92m',  # Green\n",
    "    'B' : '\\033[94m',  # Blue\n",
    "    'Y' : '\\033[93m',  # Yellow\n",
    "    'W' : '\\033[0m',  # White\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ArchiMedConnector.A3_Connector import A3_Connector\n",
    "import ArchiMedConnector.A3_Connector as A3_Conn\n",
    "\n",
    "# Initialize ArchiMed connector\n",
    "# a3conn= A3_Connector()\n",
    "a3conn = A3_Conn.A3_Connector()\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import pydicom\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import CSVs to Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSV Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_csv_to_dataframe(file_path, separator=';', columns=None, chunk_size=None):\n",
    "    \"\"\"\n",
    "    Import CSV file into a pandas DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the CSV file\n",
    "        separator (str): CSV separator character\n",
    "        columns (list): List of columns to import (if None, import all)\n",
    "        chunk_size (int): Number of rows to read at a time (if None, read all at once)\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: The imported data\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Determine which columns to use\n",
    "        usecols = columns if columns and len(columns) > 0 else None\n",
    "        \n",
    "        if chunk_size:\n",
    "            # Read in chunks and concatenate\n",
    "            chunks = []\n",
    "            for chunk in pd.read_csv(file_path, sep=separator, usecols=usecols, chunksize=chunk_size):\n",
    "                chunks.append(chunk)\n",
    "            return pd.concat(chunks, ignore_index=True)\n",
    "        else:\n",
    "            # Read all at once\n",
    "            return pd.read_csv(file_path, sep=separator, usecols=usecols)\n",
    "    except Exception as e:\n",
    "        print(f\"{ANSI['R']}Error importing CSV: {e}{ANSI['W']}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import labeled data\n",
    "csv_path = os.path.join(CSV_FOLDER, CSV_LABELS_FILE)\n",
    "print(f\"{ANSI['B']}Importing labeled data from: {csv_path}{ANSI['W']}\")\n",
    "\n",
    "df_labeled_data = import_csv_to_dataframe(\n",
    "    file_path=csv_path,\n",
    "    separator=CSV_SEPARATOR,\n",
    "    columns=IMPORT_COLUMNS,\n",
    "    chunk_size=CHUNK_SIZE\n",
    ")\n",
    "\n",
    "if df_labeled_data is not None:\n",
    "    print(f\"{ANSI['G']}Successfully imported {len(df_labeled_data)} rows of labeled data{ANSI['W']}\")\n",
    "    display(df_labeled_data.head())\n",
    "else:\n",
    "    print(f\"{ANSI['R']}Failed to import labeled data{ANSI['W']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the files from ArchiMed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_metadata(file_ids):\n",
    "    \"\"\"\n",
    "    Collect metadata from DICOM file headers for one or more FileIDs.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    file_ids : str or list\n",
    "        A single FileID or a list of FileIDs to process\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        DataFrame containing the metadata from the DICOM headers\n",
    "    \"\"\"\n",
    "    # Convert single FileID to list for consistent processing\n",
    "    if isinstance(file_ids, str):\n",
    "        file_ids = [file_ids]\n",
    "    \n",
    "    metadata_list = []\n",
    "    \n",
    "    for file_id in file_ids:\n",
    "        # Construct path to the DICOM file\n",
    "        subfolder_path = os.path.join(DOWNLOAD_PATH, file_id)\n",
    "        dicom_file_path = os.path.join(subfolder_path, f\"{file_id}.dcm\")\n",
    "        \n",
    "        try:\n",
    "            if os.path.exists(dicom_file_path):\n",
    "                # Read the DICOM file\n",
    "                dicom_data = pydicom.dcmread(dicom_file_path)\n",
    "                \n",
    "                # Extract metadata as a dictionary\n",
    "                metadata = {}\n",
    "                metadata['FileID'] = file_id\n",
    "                \n",
    "                # Extract common DICOM attributes\n",
    "                for attr in dir(dicom_data):\n",
    "                    # Skip private attributes, methods, and sequences\n",
    "                    if not attr.startswith('_') and not callable(getattr(dicom_data, attr)) and attr != 'PixelData':\n",
    "                        try:\n",
    "                            value = getattr(dicom_data, attr)\n",
    "                            # Skip sequence attributes which are complex objects\n",
    "                            if not isinstance(value, pydicom.sequence.Sequence):\n",
    "                                metadata[attr] = str(value)\n",
    "                        except Exception as e:\n",
    "                            metadata[attr] = f\"Error: {str(e)}\"\n",
    "                \n",
    "                metadata_list.append(metadata)\n",
    "            else:\n",
    "                print(f\"{ANSI['R']}DICOM file not found: {dicom_file_path}{ANSI['W']}\")\n",
    "        except Exception as e:\n",
    "            print(f\"{ANSI['R']}Error processing {file_id}: {str(e)}{ANSI['W']}\")\n",
    "    \n",
    "    # Create DataFrame from the collected metadata\n",
    "    if metadata_list:\n",
    "        return pd.DataFrame(metadata_list)\n",
    "    else:\n",
    "        print(f\"{ANSI['Y']}No metadata collected.{ANSI['W']}\")\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_dicom_to_png(\n",
    "    import_folder,\n",
    "    export_folder,\n",
    "    bit_depth=8,\n",
    "    create_subfolders=False,\n",
    "    resize_x=None,\n",
    "    resize_y=None,\n",
    "    monochrome=2,\n",
    "    delete_dicom=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Convert all DICOM files in import_folder (including subfolders) to PNG format.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    import_folder : str\n",
    "        Path to folder containing DICOM files to convert\n",
    "    export_folder : str\n",
    "        Path to folder where PNG files will be saved\n",
    "    bit_depth : int\n",
    "        Bit depth for output images (8, 12, or 16)\n",
    "    create_subfolders : bool\n",
    "        If True, create subfolders named after ExamCode for output files\n",
    "    resize_x : int or None\n",
    "        Width to resize images to (if None, no resizing unless resize_y is specified)\n",
    "    resize_y : int or None\n",
    "        Height to resize images to (if None, no resizing unless resize_x is specified)\n",
    "    monochrome : int\n",
    "        Default monochrome type (1 or 2) to use if not specified in DICOM header\n",
    "    delete_dicom : bool\n",
    "        If True, delete the DICOM file and its containing subfolder after conversion\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Summary of conversion process with counts\n",
    "    \"\"\"\n",
    "    # Validate bit depth\n",
    "    if bit_depth not in [8, 12, 16]:\n",
    "        raise ValueError(\"bit_depth must be 8, 12, or 16\")\n",
    "    \n",
    "    # Validate monochrome\n",
    "    if monochrome not in [1, 2]:\n",
    "        raise ValueError(\"monochrome must be 1 or 2\")\n",
    "    \n",
    "    # Create export folder if it doesn't exist\n",
    "    os.makedirs(export_folder, exist_ok=True)\n",
    "    \n",
    "    # Find all DICOM files recursively\n",
    "    dicom_files = []\n",
    "    for ext in ['.dcm', '.DCM']:  # Common DICOM extensions\n",
    "        dicom_files.extend(glob.glob(os.path.join(import_folder, '**/*' + ext), recursive=True))\n",
    "    \n",
    "    # Initialize counters\n",
    "    successful = 0\n",
    "    failed = 0\n",
    "    skipped = 0\n",
    "    \n",
    "    # Suppress specific pydicom warnings about character sets\n",
    "    warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"pydicom.charset\")\n",
    "    \n",
    "    # Process each DICOM file\n",
    "    for dicom_path in tqdm(dicom_files, desc=\"Converting DICOM files to PNG\", total=len(dicom_files)):\n",
    "        try:\n",
    "            # Try to read as DICOM\n",
    "            try:\n",
    "                ds = pydicom.dcmread(dicom_path)\n",
    "                pixel_array = ds.pixel_array\n",
    "            except Exception as e:\n",
    "                skipped += 1\n",
    "                continue  # Skip if not a valid DICOM file\n",
    "            \n",
    "            # Get metadata for subfolder creation if needed\n",
    "            exam_code = str(getattr(ds, 'StudyDescription', os.path.basename(os.path.dirname(dicom_path))))\n",
    "            \n",
    "            # Get file ID from the filename\n",
    "            file_id = os.path.splitext(os.path.basename(dicom_path))[0]\n",
    "            if file_id.endswith('.dcm'):\n",
    "                file_id = file_id[:-4]  # Remove .dcm if present\n",
    "            \n",
    "            # Check the PhotometricInterpretation from DICOM header\n",
    "            dicom_monochrome = monochrome  # Default value\n",
    "            \n",
    "            if hasattr(ds, 'PhotometricInterpretation'):\n",
    "                if ds.PhotometricInterpretation == 'MONOCHROME1':\n",
    "                    dicom_monochrome = 1\n",
    "                elif ds.PhotometricInterpretation == 'MONOCHROME2':\n",
    "                    dicom_monochrome = 2\n",
    "            \n",
    "            # Get bit depth information from DICOM header\n",
    "            bits_allocated = getattr(ds, 'BitsAllocated', 14)  # Default to 14 if not present\n",
    "            bits_stored = getattr(ds, 'BitsStored', bits_allocated)  # Default to bits_allocated if not present\n",
    "            high_bit = getattr(ds, 'HighBit', bits_stored - 1)  # Default to bits_stored-1 if not present\n",
    "            max_pixel_value = pixel_array.max()\n",
    "            \n",
    "            # Calculate the maximum possible value based on bits_stored\n",
    "            max_possible_value = (2 ** bits_stored) - 1\n",
    "            \n",
    "            # Normalize pixel values based on bit depth\n",
    "            output_max_value = (2 ** bit_depth) - 1  # Maximum value for the output bit depth\n",
    "            \n",
    "            # Scale to the appropriate range based on the output bit depth\n",
    "            if max_pixel_value > 0:\n",
    "                # Use the actual bit depth for scaling\n",
    "                pixel_array = ((pixel_array / min(max_pixel_value, max_possible_value)) * output_max_value)\n",
    "            \n",
    "            # Convert to appropriate data type based on bit depth\n",
    "            if bit_depth <= 8:\n",
    "                pixel_array = pixel_array.astype(np.uint8)\n",
    "            else:\n",
    "                pixel_array = pixel_array.astype(np.uint16)\n",
    "            \n",
    "            # Invert pixel values if needed to match the desired monochrome type\n",
    "            # If DICOM is MONOCHROME1 and we want MONOCHROME2, or vice versa, we need to invert\n",
    "            if dicom_monochrome != monochrome and dicom_monochrome in [1, 2] and monochrome in [1, 2]:\n",
    "                pixel_array = output_max_value - pixel_array\n",
    "            \n",
    "            # Convert to PIL Image\n",
    "            img = Image.fromarray(pixel_array)\n",
    "            \n",
    "            # Resize if specified, maintaining aspect ratio if only one dimension is provided\n",
    "            if resize_x is not None or resize_y is not None:\n",
    "                original_width, original_height = img.size\n",
    "                \n",
    "                if resize_x is not None and resize_y is not None:\n",
    "                    # Both dimensions specified, resize to exact dimensions\n",
    "                    new_size = (resize_x, resize_y)\n",
    "                elif resize_x is not None:\n",
    "                    # Only width specified, calculate height to maintain aspect ratio\n",
    "                    aspect_ratio = original_height / original_width\n",
    "                    new_size = (resize_x, int(resize_x * aspect_ratio))\n",
    "                else:\n",
    "                    # Only height specified, calculate width to maintain aspect ratio\n",
    "                    aspect_ratio = original_width / original_height\n",
    "                    new_size = (int(resize_y * aspect_ratio), resize_y)\n",
    "                \n",
    "                img = img.resize(new_size, Image.LANCZOS)\n",
    "            \n",
    "            # Determine output path\n",
    "            base_filename = os.path.splitext(os.path.basename(dicom_path))[0]\n",
    "            if create_subfolders:\n",
    "                subfolder_path = os.path.join(export_folder, exam_code)\n",
    "                os.makedirs(subfolder_path, exist_ok=True)\n",
    "                output_path = os.path.join(subfolder_path, f\"{base_filename}.png\")\n",
    "            else:\n",
    "                output_path = os.path.join(export_folder, f\"{base_filename}.png\")\n",
    "            \n",
    "            # Save as PNG\n",
    "            img.save(output_path)\n",
    "            successful += 1\n",
    "            \n",
    "            # Delete DICOM file and its containing folder if requested\n",
    "            if delete_dicom:\n",
    "                # Delete the DICOM file\n",
    "                os.remove(dicom_path)\n",
    "                \n",
    "                # Delete the containing subfolder if it's empty\n",
    "                dicom_folder = os.path.dirname(dicom_path)\n",
    "                if dicom_folder != import_folder:  # Don't delete the main import folder\n",
    "                    try:\n",
    "                        # Check if folder is empty\n",
    "                        if not os.listdir(dicom_folder):\n",
    "                            shutil.rmtree(dicom_folder)\n",
    "                    except Exception as e:\n",
    "                        print(f\"{ANSI['Y']}Warning: Could not delete folder {dicom_folder}: {str(e)}{ANSI['W']}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"{ANSI['R']}Error converting {dicom_path}: {str(e)}{ANSI['W']}\")\n",
    "            failed += 1\n",
    "    \n",
    "    # Summary\n",
    "    summary = {\n",
    "        \"successful\": successful,\n",
    "        \"skipped\": skipped,\n",
    "        \"failed\": failed,\n",
    "        \"total\": len(dicom_files)\n",
    "    }\n",
    "    \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_archimed_files(dataframe, download_path, file_id_column='FileID', batch_size=20, convert=False):\n",
    "    \"\"\"\n",
    "    Downloads files from ArchiMed based on FileIDs in the dataframe.\n",
    "    \n",
    "    Args:\n",
    "        dataframe (pandas.DataFrame): DataFrame containing FileIDs\n",
    "        download_path (str): Path where to save downloaded files\n",
    "        file_id_column (str): Name of the column containing FileIDs (default: 'FileID')\n",
    "        batch_size (int): Number of files to process in each batch for progress reporting\n",
    "        convert (bool): If True, convert downloaded DICOM files to PNG after each batch (default: False)\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: DataFrame with metadata of converted files\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create download directory if it doesn't exist\n",
    "    os.makedirs(download_path, exist_ok=True)\n",
    "    \n",
    "    # Get user info for verification\n",
    "    user_info = a3conn.getUserInfos()\n",
    "    print(f\"{ANSI['G']}ArchiMed Connector Authentication Information\")\n",
    "    print(f\"{ANSI['B']}Username:{ANSI['W']} {user_info.get('userInfos', {}).get('login', 'Unknown')}\")\n",
    "    print(f\"{ANSI['B']}User level:{ANSI['W']} {user_info.get('userInfos', {}).get('level', 'Unknown')}\")\n",
    "    print(f\"{ANSI['B']}Native Groups:{ANSI['W']} {', '.join(user_info.get('nativeGroups', ['None']))}\")\n",
    "    print(f\"{ANSI['B']}Authorized studies:{ANSI['W']} {', '.join(user_info.get('authorizedStudies', ['None']))}\")\n",
    "    print(f\"{ANSI['B']}Authorized temporary storages:{ANSI['W']} {', '.join(user_info.get('authorizedTmpStorages', ['None']))}\")\n",
    "    \n",
    "    # Check if the FileID column exists\n",
    "    if file_id_column not in dataframe.columns:\n",
    "        print(f\"{ANSI['R']}Error: Column '{file_id_column}' not found in dataframe{ANSI['W']}\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Get unique FileIDs to avoid downloading duplicates\n",
    "    file_ids = dataframe[file_id_column].unique()\n",
    "    total_files = len(file_ids)\n",
    "    \n",
    "    print(f\"\\n{ANSI['B']}Starting download of {ANSI['W']}{total_files}{ANSI['B']} files to{ANSI['W']} {download_path}\\n\")\n",
    "    \n",
    "    failed_files = []\n",
    "    batch_files = []\n",
    "    all_metadata = pd.DataFrame()  # Store all metadata records\n",
    "    downloaded_count = 0\n",
    "    skipped_count = 0\n",
    "    \n",
    "    # Process files in batches to show progress\n",
    "    for i, file_id in enumerate(file_ids):\n",
    "        if pd.isna(file_id):\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            # Convert to integer if needed\n",
    "            file_id = int(file_id)\n",
    "            \n",
    "            # Define output path for this file\n",
    "            file_output_path = os.path.join(download_path, f\"{file_id}\")\n",
    "            dicom_file_path = os.path.join(file_output_path, f\"{file_id}.dcm\")\n",
    "            \n",
    "            # Check if the file already exists\n",
    "            if os.path.exists(dicom_file_path):\n",
    "                print(f\"{ANSI['Y']}File {ANSI['W']}{file_id}{ANSI['Y']} already exists, skipping download (Progress: {ANSI['W']}{((i+1)/total_files)*100:.1f}%{ANSI['Y']} - {ANSI['W']}{i+1}/{total_files}{ANSI['Y']}){ANSI['W']}\")\n",
    "                batch_files.append(dicom_file_path)\n",
    "                skipped_count += 1\n",
    "            else:\n",
    "                print(f\"{ANSI['B']}Downloading file {ANSI['W']}{file_id}{ANSI['B']} (Progress: {ANSI['W']}{((i+1)/total_files)*100:.1f}%{ANSI['B']} - {ANSI['W']}{i+1}/{total_files}{ANSI['B']}) from{ANSI['W']} ArchiMed\")\n",
    "                \n",
    "                # Download the file\n",
    "                result = a3conn.downloadFile(\n",
    "                    file_id,\n",
    "                    asStream=False,\n",
    "                    destDir=file_output_path,\n",
    "                    filename=f\"{file_id}.dcm\",\n",
    "                    inWorklist=False\n",
    "                )\n",
    "                \n",
    "                downloaded_count += 1\n",
    "                batch_files.append(result)\n",
    "            \n",
    "            # Collect metadata for this file\n",
    "            file_metadata = collect_metadata(file_id)\n",
    "            \n",
    "            # Add metadata to the collection if not already present\n",
    "            if not file_metadata.empty and (all_metadata.empty or not (all_metadata['FileID'] == file_id).any()):\n",
    "                all_metadata = pd.concat([all_metadata, file_metadata], ignore_index=True)\n",
    "            \n",
    "            # Show progress every batch_size files\n",
    "            if (i + 1) % batch_size == 0 or (i + 1) == total_files:\n",
    "                \n",
    "                # Convert batch if requested\n",
    "                if convert and batch_files:\n",
    "                    try:\n",
    "                        # print(f\"\\n{ANSI['B']}Converting batch of {ANSI['W']}{len(batch_files)}{ANSI['B']} DICOM files to PNG...{ANSI['W']}\")\n",
    "                        summary = convert_dicom_to_png(\n",
    "                            import_folder=download_path,\n",
    "                            export_folder=IMAGES_PATH,\n",
    "                            bit_depth=BIT_DEPTH,\n",
    "                            create_subfolders=CREATE_SUBFOLDERS,\n",
    "                            delete_dicom=DELETE_DICOM,\n",
    "                            monochrome=MONOCHROME,\n",
    "                            resize_y=RESIZE_Y,\n",
    "                        )\n",
    "                    except Exception as e:\n",
    "                        print(f\"{ANSI['R']}Error during conversion: {str(e)}{ANSI['W']}\")\n",
    "                \n",
    "                batch_files = []\n",
    "                print(f\"{ANSI['Y']}Progress:{ANSI['W']} {i + 1}/{total_files} {ANSI['B']}files processed {ANSI['W']}({ANSI['B']}{((i + 1) / total_files * 100):.1f}%{ANSI['W']})\\n\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            failed_files.append(file_id)\n",
    "            print(f\"{ANSI['R']}Error downloading file ID {file_id}: {str(e)}{ANSI['W']}\")\n",
    "    \n",
    "    # Summary\n",
    "    print(f\"\\n{ANSI['G']}Download complete: {downloaded_count} files downloaded successfully{ANSI['W']}\")\n",
    "    if skipped_count > 0:\n",
    "        print(f\"{ANSI['Y']}Skipped {skipped_count} files (already downloaded){ANSI['W']}\")\n",
    "    if failed_files:\n",
    "        print(f\"{ANSI['R']}Failed to download {len(failed_files)} files{ANSI['W']}\")\n",
    "    \n",
    "    return all_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download files from the labeled data\n",
    "if df_labeled_data is not None:\n",
    "    print(f\"{ANSI['Y']}Starting download of ArchiMed files...{ANSI['W']}\\n\")\n",
    "    metadata_df = download_archimed_files(\n",
    "        dataframe=df_labeled_data,\n",
    "        download_path=DOWNLOAD_PATH,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        convert=CONVERT\n",
    "    )\n",
    "    if not metadata_df.empty:\n",
    "        print(f\"{ANSI['G']}Downloaded files successfully to {ANSI['W']}{DOWNLOAD_PATH}\")\n",
    "        print(f\"{ANSI['B']}Metadata collected for {ANSI['W']}{len(metadata_df)}{ANSI['B']} images{ANSI['W']}\")\n",
    "    else:\n",
    "        print(f\"{ANSI['Y']}Files downloaded but no metadata was collected{ANSI['W']}\")\n",
    "else:\n",
    "    print(f\"{ANSI['R']}Cannot download files: No labeled data available{ANSI['W']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_correlation_matrix(dataframe, gradient_name='coolwarm'):\n",
    "    \"\"\"\n",
    "    Display a correlation matrix for the given dataframe.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    dataframe : pandas.DataFrame\n",
    "        The dataframe containing the data to analyze\n",
    "    gradient_name : str, default='coolwarm'\n",
    "        The name of the color gradient to use for the heatmap\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    None\n",
    "        Displays the correlation matrix as a heatmap\n",
    "    \"\"\"\n",
    "    \n",
    "    # Select only numeric columns\n",
    "    numeric_df = dataframe.select_dtypes(include=['number'])\n",
    "    \n",
    "    # Calculate the correlation matrix\n",
    "    corr_matrix = numeric_df.corr()\n",
    "    \n",
    "    # Create a figure with appropriate size\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # Create a heatmap\n",
    "    sns.heatmap(\n",
    "        corr_matrix, \n",
    "        annot=True,              # Show correlation values\n",
    "        cmap=gradient_name,      # Color map\n",
    "        linewidths=0.5,          # Width of the grid lines\n",
    "        vmin=-1, vmax=1,         # Value range\n",
    "        fmt='.2f',               # Format of the annotations\n",
    "        square=True              # Make cells square-shaped\n",
    "    )\n",
    "    \n",
    "    # Add title and adjust layout\n",
    "    plt.title('Numeric Parameters Correlation Matrix', fontsize=14, pad=20)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Display the plot\n",
    "    plt.show()\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_correlation_matrix(df_labeled_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
