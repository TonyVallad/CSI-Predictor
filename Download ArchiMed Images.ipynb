{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h1 align=\"center\">CSV Preprocessing</h1>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Specific Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV Files\n",
    "CSV_FOLDER = \"../../data/\"\n",
    "# CSV_FOLDER = \"/home/pyuser/data/\"\n",
    "CSV_LABELS_FILE = \"Labeled_Data_RAW_Sample.csv\"\n",
    "CSV_SEPARATOR = \";\"  # Specify the CSV separator, e.g., ',' or '\\t'\n",
    "IMPORT_COLUMNS = []  # If empty, import all columns\n",
    "CHUNK_SIZE = 50000  # Number of rows per chunk\n",
    "\n",
    "DOWNLOAD_PATH = '../../data/Paradise_Images'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSI escape codes for colored output\n",
    "ANSI = {\n",
    "    'R' : '\\033[91m',  # Red\n",
    "    'G' : '\\033[92m',  # Green\n",
    "    'B' : '\\033[94m',  # Blue\n",
    "    'Y' : '\\033[93m',  # Yellow\n",
    "    'W' : '\\033[0m',  # White\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import CSVs to Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSV Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def import_csv_to_dataframe(file_path, separator=',', columns=None, chunk_size=None):\n",
    "    \"\"\"\n",
    "    Import CSV file into a pandas DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the CSV file\n",
    "        separator (str): CSV separator character\n",
    "        columns (list): List of columns to import (if None, import all)\n",
    "        chunk_size (int): Number of rows to read at a time (if None, read all at once)\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: The imported data\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Determine which columns to use\n",
    "        usecols = columns if columns and len(columns) > 0 else None\n",
    "        \n",
    "        if chunk_size:\n",
    "            # Read in chunks and concatenate\n",
    "            chunks = []\n",
    "            for chunk in pd.read_csv(file_path, sep=separator, usecols=usecols, chunksize=chunk_size):\n",
    "                chunks.append(chunk)\n",
    "            return pd.concat(chunks, ignore_index=True)\n",
    "        else:\n",
    "            # Read all at once\n",
    "            return pd.read_csv(file_path, sep=separator, usecols=usecols)\n",
    "    except Exception as e:\n",
    "        print(f\"{ANSI['R']}Error importing CSV: {e}{ANSI['W']}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import labeled data\n",
    "csv_path = os.path.join(CSV_FOLDER, CSV_LABELS_FILE)\n",
    "print(f\"{ANSI['B']}Importing labeled data from: {csv_path}{ANSI['W']}\")\n",
    "\n",
    "df_labeled_data = import_csv_to_dataframe(\n",
    "    file_path=csv_path,\n",
    "    separator=CSV_SEPARATOR,\n",
    "    columns=IMPORT_COLUMNS,\n",
    "    chunk_size=CHUNK_SIZE\n",
    ")\n",
    "\n",
    "if df_labeled_data is not None:\n",
    "    print(f\"{ANSI['G']}Successfully imported {len(df_labeled_data)} rows of labeled data{ANSI['W']}\")\n",
    "    display(df_labeled_data.head())\n",
    "else:\n",
    "    print(f\"{ANSI['R']}Failed to import labeled data{ANSI['W']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the files from ArchiMed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_archimed_files(dataframe, download_path, file_id_column='FileID', batch_size=100):\n",
    "    \"\"\"\n",
    "    Downloads files from ArchiMed based on FileIDs in the dataframe.\n",
    "    \n",
    "    Args:\n",
    "        dataframe (pandas.DataFrame): DataFrame containing FileIDs\n",
    "        download_path (str): Path where to save downloaded files\n",
    "        file_id_column (str): Name of the column containing FileIDs (default: 'FileID')\n",
    "        batch_size (int): Number of files to process in each batch for progress reporting\n",
    "        \n",
    "    Returns:\n",
    "        list: List of successfully downloaded file paths\n",
    "    \"\"\"\n",
    "    from ArchiMedConnector.A3_Connector import A3_Connector\n",
    "    import os\n",
    "    \n",
    "    # Create download directory if it doesn't exist\n",
    "    os.makedirs(download_path, exist_ok=True)\n",
    "    \n",
    "    # Initialize ArchiMed connector\n",
    "    a3conn = A3_Connector()\n",
    "    \n",
    "    # Get user info for verification\n",
    "    user_info = a3conn.getUserInfos()\n",
    "    print(f\"{ANSI['B']}Connected to ArchiMed as:{ANSI['W']} {user_info.get('login', 'Unknown')}\")\n",
    "    \n",
    "    # Check if the FileID column exists\n",
    "    if file_id_column not in dataframe.columns:\n",
    "        print(f\"{ANSI['R']}Error: Column '{file_id_column}' not found in dataframe{ANSI['W']}\")\n",
    "        return []\n",
    "    \n",
    "    # Get unique FileIDs to avoid downloading duplicates\n",
    "    file_ids = dataframe[file_id_column].unique()\n",
    "    total_files = len(file_ids)\n",
    "    \n",
    "    print(f\"{ANSI['B']}Starting download of {total_files} files to{ANSI['W']} {download_path}\")\n",
    "    \n",
    "    downloaded_files = []\n",
    "    failed_files = []\n",
    "    \n",
    "    # Process files in batches to show progress\n",
    "    for i, file_id in enumerate(file_ids):\n",
    "        if pd.isna(file_id):\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            # Convert to integer if needed\n",
    "            file_id = int(file_id)\n",
    "            \n",
    "            # Define output path for this file\n",
    "            file_output_path = os.path.join(download_path, f\"{file_id}\")\n",
    "            \n",
    "            # Download the file\n",
    "            result = a3conn.downloadFile(\n",
    "                file_id,\n",
    "                asStream=False,\n",
    "                filePath=file_output_path,\n",
    "                inWorklist=False\n",
    "            )\n",
    "            \n",
    "            downloaded_files.append(result)\n",
    "            \n",
    "            # Show progress every batch_size files\n",
    "            if (i + 1) % batch_size == 0 or (i + 1) == total_files:\n",
    "                print(f\"{ANSI['G']}Progress: {i + 1}/{total_files} files processed ({((i + 1) / total_files * 100):.1f}%){ANSI['W']}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            failed_files.append(file_id)\n",
    "            print(f\"{ANSI['R']}Error downloading file ID {file_id}: {str(e)}{ANSI['W']}\")\n",
    "    \n",
    "    # Summary\n",
    "    print(f\"{ANSI['G']}Download complete: {len(downloaded_files)} files downloaded successfully{ANSI['W']}\")\n",
    "    if failed_files:\n",
    "        print(f\"{ANSI['R']}Failed to download {len(failed_files)} files{ANSI['W']}\")\n",
    "    \n",
    "    return downloaded_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define download path\n",
    "download_path = os.path.join(os.getcwd(), \"downloaded_images\")\n",
    "\n",
    "# Download files from the labeled data\n",
    "if df_labeled_data is not None:\n",
    "    print(f\"{ANSI['B']}Starting download of ArchiMed files...{ANSI['W']}\")\n",
    "    downloaded_files = download_archimed_files(\n",
    "        dataframe=df_labeled_data,\n",
    "        download_path=DOWNLOAD_PATH\n",
    "    )\n",
    "    print(f\"{ANSI['G']}Downloaded {len(downloaded_files)} files to {download_path}{ANSI['W']}\")\n",
    "else:\n",
    "    print(f\"{ANSI['R']}Cannot download files: No labeled data available{ANSI['W']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download_archimed_files(df_labeled_data, DOWNLOAD_PATH, file_id_column='FileID', batch_size=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
