{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h1 align=\"center\">CSV Preprocessing</h1>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Specific Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV Files\n",
    "CSV_FOLDER = \"../../data/\"\n",
    "# CSV_FOLDER = \"/home/pyuser/data/\"\n",
    "CSV_LABELS_FILE = \"Labeled_Data_RAW_Sample.csv\"\n",
    "CSV_SEPARATOR = \";\"  # Specify the CSV separator, e.g., ',' or '\\t'\n",
    "IMPORT_COLUMNS = []  # If empty, import all columns\n",
    "CHUNK_SIZE = 50000  # Number of rows per chunk\n",
    "\n",
    "DOWNLOAD_PATH = '../../data/Paradise_Images'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSI escape codes for colored output\n",
    "ANSI = {\n",
    "    'R' : '\\033[91m',  # Red\n",
    "    'G' : '\\033[92m',  # Green\n",
    "    'B' : '\\033[94m',  # Blue\n",
    "    'Y' : '\\033[93m',  # Yellow\n",
    "    'W' : '\\033[0m',  # White\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import CSVs to Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSV Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def import_csv_to_dataframe(file_path, separator=';', columns=None, chunk_size=None):\n",
    "    \"\"\"\n",
    "    Import CSV file into a pandas DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the CSV file\n",
    "        separator (str): CSV separator character\n",
    "        columns (list): List of columns to import (if None, import all)\n",
    "        chunk_size (int): Number of rows to read at a time (if None, read all at once)\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: The imported data\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Determine which columns to use\n",
    "        usecols = columns if columns and len(columns) > 0 else None\n",
    "        \n",
    "        if chunk_size:\n",
    "            # Read in chunks and concatenate\n",
    "            chunks = []\n",
    "            for chunk in pd.read_csv(file_path, sep=separator, usecols=usecols, chunksize=chunk_size):\n",
    "                chunks.append(chunk)\n",
    "            return pd.concat(chunks, ignore_index=True)\n",
    "        else:\n",
    "            # Read all at once\n",
    "            return pd.read_csv(file_path, sep=separator, usecols=usecols)\n",
    "    except Exception as e:\n",
    "        print(f\"{ANSI['R']}Error importing CSV: {e}{ANSI['W']}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import labeled data\n",
    "csv_path = os.path.join(CSV_FOLDER, CSV_LABELS_FILE)\n",
    "print(f\"{ANSI['B']}Importing labeled data from: {csv_path}{ANSI['W']}\")\n",
    "\n",
    "df_labeled_data = import_csv_to_dataframe(\n",
    "    file_path=csv_path,\n",
    "    separator=CSV_SEPARATOR,\n",
    "    columns=IMPORT_COLUMNS,\n",
    "    chunk_size=CHUNK_SIZE\n",
    ")\n",
    "\n",
    "if df_labeled_data is not None:\n",
    "    print(f\"{ANSI['G']}Successfully imported {len(df_labeled_data)} rows of labeled data{ANSI['W']}\")\n",
    "    display(df_labeled_data.head())\n",
    "else:\n",
    "    print(f\"{ANSI['R']}Failed to import labeled data{ANSI['W']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the files from ArchiMed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_archimed_files(dataframe, download_path, file_id_column='FileID', batch_size=100):\n",
    "    \"\"\"\n",
    "    Downloads files from ArchiMed based on FileIDs in the dataframe.\n",
    "    \n",
    "    Args:\n",
    "        dataframe (pandas.DataFrame): DataFrame containing FileIDs\n",
    "        download_path (str): Path where to save downloaded files\n",
    "        file_id_column (str): Name of the column containing FileIDs (default: 'FileID')\n",
    "        batch_size (int): Number of files to process in each batch for progress reporting\n",
    "        \n",
    "    Returns:\n",
    "        list: List of successfully downloaded file paths\n",
    "    \"\"\"\n",
    "    from ArchiMedConnector.A3_Connector import A3_Connector\n",
    "    import os\n",
    "    \n",
    "    # Create download directory if it doesn't exist\n",
    "    os.makedirs(download_path, exist_ok=True)\n",
    "    \n",
    "    # Initialize ArchiMed connector\n",
    "    a3conn = A3_Connector()\n",
    "    \n",
    "    # Get user info for verification\n",
    "    user_info = a3conn.getUserInfos()\n",
    "    print(f\"{ANSI['B']}Connected to ArchiMed as:{ANSI['W']} {user_info.get('login', 'Unknown')}\")\n",
    "    \n",
    "    # Check if the FileID column exists\n",
    "    if file_id_column not in dataframe.columns:\n",
    "        print(f\"{ANSI['R']}Error: Column '{file_id_column}' not found in dataframe{ANSI['W']}\")\n",
    "        return []\n",
    "    \n",
    "    # Get unique FileIDs to avoid downloading duplicates\n",
    "    file_ids = dataframe[file_id_column].unique()\n",
    "    total_files = len(file_ids)\n",
    "    \n",
    "    print(f\"{ANSI['B']}Starting download of {total_files} files to{ANSI['W']} {download_path}\")\n",
    "    \n",
    "    downloaded_files = []\n",
    "    failed_files = []\n",
    "    \n",
    "    # Process files in batches to show progress\n",
    "    for i, file_id in enumerate(file_ids):\n",
    "        if pd.isna(file_id):\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            # Convert to integer if needed\n",
    "            file_id = int(file_id)\n",
    "            \n",
    "            # Define output path for this file\n",
    "            file_output_path = os.path.join(download_path, f\"{file_id}\")\n",
    "            \n",
    "            # Download the file\n",
    "            result = a3conn.downloadFile(\n",
    "                file_id,\n",
    "                asStream=False,\n",
    "                filePath=file_output_path,\n",
    "                inWorklist=False\n",
    "            )\n",
    "            \n",
    "            downloaded_files.append(result)\n",
    "            \n",
    "            # Show progress every batch_size files\n",
    "            if (i + 1) % batch_size == 0 or (i + 1) == total_files:\n",
    "                print(f\"{ANSI['G']}Progress: {i + 1}/{total_files} files processed ({((i + 1) / total_files * 100):.1f}%){ANSI['W']}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            failed_files.append(file_id)\n",
    "            print(f\"{ANSI['R']}Error downloading file ID {file_id}: {str(e)}{ANSI['W']}\")\n",
    "    \n",
    "    # Summary\n",
    "    print(f\"{ANSI['G']}Download complete: {len(downloaded_files)} files downloaded successfully{ANSI['W']}\")\n",
    "    if failed_files:\n",
    "        print(f\"{ANSI['R']}Failed to download {len(failed_files)} files{ANSI['W']}\")\n",
    "    \n",
    "    return downloaded_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download files from the labeled data\n",
    "if df_labeled_data is not None:\n",
    "    print(f\"{ANSI['B']}Starting download of ArchiMed files...{ANSI['W']}\")\n",
    "    downloaded_files = download_archimed_files(\n",
    "        dataframe=df_labeled_data,\n",
    "        download_path=DOWNLOAD_PATH\n",
    "    )\n",
    "    print(f\"{ANSI['G']}Downloaded {len(downloaded_files)} files to {download_path}{ANSI['W']}\")\n",
    "else:\n",
    "    print(f\"{ANSI['R']}Cannot download files: No labeled data available{ANSI['W']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert DICOM files to PNG files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pydicom\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "def convert_dicom_to_png(\n",
    "    import_folder,\n",
    "    export_folder,\n",
    "    bit_depth=8,\n",
    "    create_subfolders=False,\n",
    "    resize_x=None,\n",
    "    resize_y=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Convert all DICOM files in import_folder (including subfolders) to PNG format.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    import_folder : str\n",
    "        Path to folder containing DICOM files to convert\n",
    "    export_folder : str\n",
    "        Path to folder where PNG files will be saved\n",
    "    bit_depth : int\n",
    "        Bit depth for output images (8, 12, or 16)\n",
    "    create_subfolders : bool\n",
    "        If True, create subfolders named after ExamCode for output files\n",
    "    resize_x : int or None\n",
    "        Width to resize images to (if None, no resizing)\n",
    "    resize_y : int or None\n",
    "        Height to resize images to (if None, no resizing)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Summary of conversion process with counts of successful and failed conversions\n",
    "    \"\"\"\n",
    "    # Validate bit depth\n",
    "    if bit_depth not in [8, 12, 16]:\n",
    "        raise ValueError(\"bit_depth must be 8, 12, or 16\")\n",
    "    \n",
    "    # Create export folder if it doesn't exist\n",
    "    os.makedirs(export_folder, exist_ok=True)\n",
    "    \n",
    "    # Find all DICOM files recursively\n",
    "    dicom_files = []\n",
    "    for ext in ['.dcm', '.DCM', '']:  # Common DICOM extensions (including no extension)\n",
    "        dicom_files.extend(glob.glob(os.path.join(import_folder, '**/*' + ext), recursive=True))\n",
    "    \n",
    "    # Initialize counters\n",
    "    successful = 0\n",
    "    failed = 0\n",
    "    skipped = 0\n",
    "    \n",
    "    print(f\"{ANSI['B']}Converting {len(dicom_files)} DICOM files to PNG...{ANSI['W']}\")\n",
    "    \n",
    "    # Process each DICOM file\n",
    "    for dicom_path in tqdm(dicom_files):\n",
    "        try:\n",
    "            # Try to read as DICOM\n",
    "            try:\n",
    "                ds = pydicom.dcmread(dicom_path)\n",
    "                pixel_array = ds.pixel_array\n",
    "            except:\n",
    "                skipped += 1\n",
    "                continue  # Skip if not a valid DICOM file\n",
    "            \n",
    "            # Get metadata for subfolder creation if needed\n",
    "            exam_code = str(getattr(ds, 'StudyDescription', os.path.basename(os.path.dirname(dicom_path))))\n",
    "            \n",
    "            # Normalize pixel values based on bit depth\n",
    "            if bit_depth == 8:\n",
    "                # Scale to 0-255\n",
    "                if pixel_array.max() > 0:\n",
    "                    pixel_array = ((pixel_array / pixel_array.max()) * 255).astype(np.uint8)\n",
    "                else:\n",
    "                    pixel_array = pixel_array.astype(np.uint8)\n",
    "            elif bit_depth == 12:\n",
    "                # Scale to 0-4095\n",
    "                if pixel_array.max() > 0:\n",
    "                    pixel_array = ((pixel_array / pixel_array.max()) * 4095).astype(np.uint16)\n",
    "                else:\n",
    "                    pixel_array = pixel_array.astype(np.uint16)\n",
    "            elif bit_depth == 16:\n",
    "                # Scale to 0-65535\n",
    "                if pixel_array.max() > 0:\n",
    "                    pixel_array = ((pixel_array / pixel_array.max()) * 65535).astype(np.uint16)\n",
    "                else:\n",
    "                    pixel_array = pixel_array.astype(np.uint16)\n",
    "            \n",
    "            # Convert to PIL Image\n",
    "            img = Image.fromarray(pixel_array)\n",
    "            \n",
    "            # Resize if specified\n",
    "            if resize_x is not None and resize_y is not None:\n",
    "                img = img.resize((resize_x, resize_y), Image.LANCZOS)\n",
    "            \n",
    "            # Determine output path\n",
    "            base_filename = os.path.splitext(os.path.basename(dicom_path))[0]\n",
    "            if create_subfolders:\n",
    "                subfolder_path = os.path.join(export_folder, exam_code)\n",
    "                os.makedirs(subfolder_path, exist_ok=True)\n",
    "                output_path = os.path.join(subfolder_path, f\"{base_filename}.png\")\n",
    "            else:\n",
    "                output_path = os.path.join(export_folder, f\"{base_filename}.png\")\n",
    "            \n",
    "            # Save as PNG\n",
    "            img.save(output_path)\n",
    "            successful += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"{ANSI['R']}Error converting {dicom_path}: {str(e)}{ANSI['W']}\")\n",
    "            failed += 1\n",
    "    \n",
    "    # Summary\n",
    "    print(f\"{ANSI['G']}Conversion complete:{ANSI['W']}\")\n",
    "    print(f\"  - {successful} files successfully converted\")\n",
    "    print(f\"  - {skipped} files skipped (not valid DICOM)\")\n",
    "    print(f\"  - {failed} files failed to convert\")\n",
    "    \n",
    "    return {\n",
    "        \"successful\": successful,\n",
    "        \"skipped\": skipped,\n",
    "        \"failed\": failed,\n",
    "        \"total\": len(dicom_files)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "convert_dicom_to_png(\n",
    "    import_folder=DOWNLOAD_PATH,\n",
    "    export_folder=DOWNLOAD_PATH,\n",
    "    bit_depth=8,\n",
    "    create_subfolders=False\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
