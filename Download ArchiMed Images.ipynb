{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h1 align=\"center\">CSV Preprocessing</h1>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Specific Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV Files\n",
    "CSV_FOLDER = \"../../data/Paradise_CSV/\"\n",
    "# CSV_FOLDER = \"/home/pyuser/data/\"\n",
    "CSV_LABELS_FILE = \"Labeled_Data_RAW_Sample.csv\"\n",
    "CSV_SEPARATOR = \";\"  # Specify the CSV separator, e.g., ',' or '\\t'\n",
    "IMPORT_COLUMNS = []  # If empty, import all columns\n",
    "CHUNK_SIZE = 50000  # Number of rows per chunk\n",
    "\n",
    "DOWNLOAD_PATH = '../../data/Paradise_Images'\n",
    "\n",
    "# Conversion parameters\n",
    "BIT_DEPTH = 8\n",
    "CREATE_SUBFOLDERS = False\n",
    "DELETE_DICOM = True\n",
    "MONOCHROME = 1\n",
    "# RESIZE_X = 518\n",
    "RESIZE_Y = 750"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSI escape codes for colored output\n",
    "ANSI = {\n",
    "    'R' : '\\033[91m',  # Red\n",
    "    'G' : '\\033[92m',  # Green\n",
    "    'B' : '\\033[94m',  # Blue\n",
    "    'Y' : '\\033[93m',  # Yellow\n",
    "    'W' : '\\033[0m',  # White\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import CSVs to Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSV Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def import_csv_to_dataframe(file_path, separator=';', columns=None, chunk_size=None):\n",
    "    \"\"\"\n",
    "    Import CSV file into a pandas DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the CSV file\n",
    "        separator (str): CSV separator character\n",
    "        columns (list): List of columns to import (if None, import all)\n",
    "        chunk_size (int): Number of rows to read at a time (if None, read all at once)\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: The imported data\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Determine which columns to use\n",
    "        usecols = columns if columns and len(columns) > 0 else None\n",
    "        \n",
    "        if chunk_size:\n",
    "            # Read in chunks and concatenate\n",
    "            chunks = []\n",
    "            for chunk in pd.read_csv(file_path, sep=separator, usecols=usecols, chunksize=chunk_size):\n",
    "                chunks.append(chunk)\n",
    "            return pd.concat(chunks, ignore_index=True)\n",
    "        else:\n",
    "            # Read all at once\n",
    "            return pd.read_csv(file_path, sep=separator, usecols=usecols)\n",
    "    except Exception as e:\n",
    "        print(f\"{ANSI['R']}Error importing CSV: {e}{ANSI['W']}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import labeled data\n",
    "csv_path = os.path.join(CSV_FOLDER, CSV_LABELS_FILE)\n",
    "print(f\"{ANSI['B']}Importing labeled data from: {csv_path}{ANSI['W']}\")\n",
    "\n",
    "df_labeled_data = import_csv_to_dataframe(\n",
    "    file_path=csv_path,\n",
    "    separator=CSV_SEPARATOR,\n",
    "    columns=IMPORT_COLUMNS,\n",
    "    chunk_size=CHUNK_SIZE\n",
    ")\n",
    "\n",
    "if df_labeled_data is not None:\n",
    "    print(f\"{ANSI['G']}Successfully imported {len(df_labeled_data)} rows of labeled data{ANSI['W']}\")\n",
    "    display(df_labeled_data.head())\n",
    "else:\n",
    "    print(f\"{ANSI['R']}Failed to import labeled data{ANSI['W']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the files from ArchiMed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pydicom\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "import warnings\n",
    "\n",
    "def convert_dicom_to_png(\n",
    "    import_folder,\n",
    "    export_folder,\n",
    "    bit_depth=8,\n",
    "    create_subfolders=False,\n",
    "    resize_x=None,\n",
    "    resize_y=None,\n",
    "    monochrome=2,\n",
    "    delete_dicom=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Convert all DICOM files in import_folder (including subfolders) to PNG format.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    import_folder : str\n",
    "        Path to folder containing DICOM files to convert\n",
    "    export_folder : str\n",
    "        Path to folder where PNG files will be saved\n",
    "    bit_depth : int\n",
    "        Bit depth for output images (8, 12, or 16)\n",
    "    create_subfolders : bool\n",
    "        If True, create subfolders named after ExamCode for output files\n",
    "    resize_x : int or None\n",
    "        Width to resize images to (if None, no resizing unless resize_y is specified)\n",
    "    resize_y : int or None\n",
    "        Height to resize images to (if None, no resizing unless resize_x is specified)\n",
    "    monochrome : int\n",
    "        Default monochrome type (1 or 2) to use if not specified in DICOM header\n",
    "    delete_dicom : bool\n",
    "        If True, delete the DICOM file and its containing subfolder after conversion\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Summary of conversion process with counts of successful and failed conversions\n",
    "    \"\"\"\n",
    "    # Validate bit depth\n",
    "    if bit_depth not in [8, 12, 16]:\n",
    "        raise ValueError(\"bit_depth must be 8, 12, or 16\")\n",
    "    \n",
    "    # Validate monochrome\n",
    "    if monochrome not in [1, 2]:\n",
    "        raise ValueError(\"monochrome must be 1 or 2\")\n",
    "    \n",
    "    # Create export folder if it doesn't exist\n",
    "    os.makedirs(export_folder, exist_ok=True)\n",
    "    \n",
    "    # Find all DICOM files recursively\n",
    "    dicom_files = []\n",
    "    for ext in ['.dcm', '.DCM']:  # Common DICOM extensions\n",
    "        dicom_files.extend(glob.glob(os.path.join(import_folder, '**/*' + ext), recursive=True))\n",
    "    \n",
    "    # Initialize counters\n",
    "    successful = 0\n",
    "    failed = 0\n",
    "    skipped = 0\n",
    "    \n",
    "    print(f\"{ANSI['B']}Converting {len(dicom_files)} DICOM files to PNG...{ANSI['W']}\\n\")\n",
    "    \n",
    "    # Suppress specific pydicom warnings about character sets\n",
    "    warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"pydicom.charset\")\n",
    "    \n",
    "    # Process each DICOM file\n",
    "    for dicom_path in tqdm(dicom_files):\n",
    "        try:\n",
    "            # Try to read as DICOM\n",
    "            try:\n",
    "                ds = pydicom.dcmread(dicom_path)\n",
    "                pixel_array = ds.pixel_array\n",
    "            except Exception as e:\n",
    "                skipped += 1\n",
    "                continue  # Skip if not a valid DICOM file\n",
    "            \n",
    "            # Get metadata for subfolder creation if needed\n",
    "            exam_code = str(getattr(ds, 'StudyDescription', os.path.basename(os.path.dirname(dicom_path))))\n",
    "            \n",
    "            # Get file ID from the filename\n",
    "            file_id = os.path.splitext(os.path.basename(dicom_path))[0]\n",
    "            if file_id.endswith('.dcm'):\n",
    "                file_id = file_id[:-4]  # Remove .dcm if present\n",
    "            \n",
    "            # Check the PhotometricInterpretation from DICOM header\n",
    "            dicom_monochrome = monochrome  # Default value\n",
    "            \n",
    "            if hasattr(ds, 'PhotometricInterpretation'):\n",
    "                if ds.PhotometricInterpretation == 'MONOCHROME1':\n",
    "                    dicom_monochrome = 1\n",
    "                elif ds.PhotometricInterpretation == 'MONOCHROME2':\n",
    "                    dicom_monochrome = 2\n",
    "            \n",
    "            # Get bit depth information from DICOM header\n",
    "            bits_allocated = getattr(ds, 'BitsAllocated', 14)  # Default to 14 if not present\n",
    "            bits_stored = getattr(ds, 'BitsStored', bits_allocated)  # Default to bits_allocated if not present\n",
    "            high_bit = getattr(ds, 'HighBit', bits_stored - 1)  # Default to bits_stored-1 if not present\n",
    "            \n",
    "            print(f\"\\n{ANSI['B']}{file_id}{ANSI['W']} - {high_bit} - MONOCHROME{ANSI['B']}{dicom_monochrome}{ANSI['W']} - {ANSI['B']}Max pixel value:{ANSI['W']}{pixel_array.max()} - Bits Allocated:{ANSI['B']}{bits_allocated}{ANSI['W']} - Bits Stored:{ANSI['B']}{bits_stored}{ANSI['W']} - High Bit:{ANSI['B']}{high_bit}{ANSI['W']}\\n\")\n",
    "            \n",
    "            # Calculate the maximum possible value based on bits_stored\n",
    "            max_possible_value = (2 ** bits_stored) - 1\n",
    "            \n",
    "            # Normalize pixel values based on bit depth\n",
    "            output_max_value = (2 ** bit_depth) - 1  # Maximum value for the output bit depth\n",
    "            \n",
    "            # Scale to the appropriate range based on the output bit depth\n",
    "            if pixel_array.max() > 0:\n",
    "                # Use the actual bit depth for scaling\n",
    "                pixel_array = ((pixel_array / min(pixel_array.max(), max_possible_value)) * output_max_value)\n",
    "            \n",
    "            # Convert to appropriate data type based on bit depth\n",
    "            if bit_depth <= 8:\n",
    "                pixel_array = pixel_array.astype(np.uint8)\n",
    "            else:\n",
    "                pixel_array = pixel_array.astype(np.uint16)\n",
    "            \n",
    "            # Invert pixel values if needed to match the desired monochrome type\n",
    "            # If DICOM is MONOCHROME1 and we want MONOCHROME2, or vice versa, we need to invert\n",
    "            if dicom_monochrome != monochrome and dicom_monochrome in [1, 2] and monochrome in [1, 2]:\n",
    "                pixel_array = output_max_value - pixel_array\n",
    "            \n",
    "            # Convert to PIL Image\n",
    "            img = Image.fromarray(pixel_array)\n",
    "            \n",
    "            # Resize if specified, maintaining aspect ratio if only one dimension is provided\n",
    "            if resize_x is not None or resize_y is not None:\n",
    "                original_width, original_height = img.size\n",
    "                \n",
    "                if resize_x is not None and resize_y is not None:\n",
    "                    # Both dimensions specified, resize to exact dimensions\n",
    "                    new_size = (resize_x, resize_y)\n",
    "                elif resize_x is not None:\n",
    "                    # Only width specified, calculate height to maintain aspect ratio\n",
    "                    aspect_ratio = original_height / original_width\n",
    "                    new_size = (resize_x, int(resize_x * aspect_ratio))\n",
    "                else:\n",
    "                    # Only height specified, calculate width to maintain aspect ratio\n",
    "                    aspect_ratio = original_width / original_height\n",
    "                    new_size = (int(resize_y * aspect_ratio), resize_y)\n",
    "                \n",
    "                img = img.resize(new_size, Image.LANCZOS)\n",
    "            \n",
    "            # Determine output path\n",
    "            base_filename = os.path.splitext(os.path.basename(dicom_path))[0]\n",
    "            if create_subfolders:\n",
    "                subfolder_path = os.path.join(export_folder, exam_code)\n",
    "                os.makedirs(subfolder_path, exist_ok=True)\n",
    "                output_path = os.path.join(subfolder_path, f\"{base_filename}.png\")\n",
    "            else:\n",
    "                output_path = os.path.join(export_folder, f\"{base_filename}.png\")\n",
    "            \n",
    "            # Save as PNG\n",
    "            img.save(output_path)\n",
    "            successful += 1\n",
    "            \n",
    "            # Delete DICOM file and its containing folder if requested\n",
    "            if delete_dicom:\n",
    "                # Delete the DICOM file\n",
    "                os.remove(dicom_path)\n",
    "                \n",
    "                # Delete the containing subfolder if it's empty\n",
    "                dicom_folder = os.path.dirname(dicom_path)\n",
    "                if dicom_folder != import_folder:  # Don't delete the main import folder\n",
    "                    try:\n",
    "                        # Check if folder is empty\n",
    "                        if not os.listdir(dicom_folder):\n",
    "                            shutil.rmtree(dicom_folder)\n",
    "                    except Exception as e:\n",
    "                        print(f\"{ANSI['Y']}Warning: Could not delete folder {dicom_folder}: {str(e)}{ANSI['W']}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"{ANSI['R']}Error converting {dicom_path}: {str(e)}{ANSI['W']}\")\n",
    "            failed += 1\n",
    "    \n",
    "    # Summary\n",
    "    print(f\"\\n{ANSI['G']}Conversion complete:{ANSI['W']}\")\n",
    "    print(f\"  - {successful} files successfully converted\")\n",
    "    print(f\"  - {skipped} files skipped (not valid DICOM)\")\n",
    "    print(f\"  - {failed} files failed to convert\")\n",
    "    \n",
    "    return {\n",
    "        \"successful\": successful,\n",
    "        \"skipped\": skipped,\n",
    "        \"failed\": failed,\n",
    "        \"total\": len(dicom_files)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_archimed_files(dataframe, download_path, file_id_column='FileID', batch_size=20, convert=False):\n",
    "    \"\"\"\n",
    "    Downloads files from ArchiMed based on FileIDs in the dataframe.\n",
    "    \n",
    "    Args:\n",
    "        dataframe (pandas.DataFrame): DataFrame containing FileIDs\n",
    "        download_path (str): Path where to save downloaded files\n",
    "        file_id_column (str): Name of the column containing FileIDs (default: 'FileID')\n",
    "        batch_size (int): Number of files to process in each batch for progress reporting\n",
    "        convert (bool): If True, convert downloaded DICOM files to PNG after each batch (default: False)\n",
    "        \n",
    "    Returns:\n",
    "        list: List of successfully downloaded file paths\n",
    "    \"\"\"\n",
    "    from ArchiMedConnector.A3_Connector import A3_Connector\n",
    "    import os\n",
    "    \n",
    "    # Create download directory if it doesn't exist\n",
    "    os.makedirs(download_path, exist_ok=True)\n",
    "    \n",
    "    # Initialize ArchiMed connector\n",
    "    a3conn = A3_Connector()\n",
    "    \n",
    "    # Get user info for verification\n",
    "    user_info = a3conn.getUserInfos()\n",
    "    print(f\"{ANSI['B']}Connected to ArchiMed as:{ANSI['W']} {user_info.get('login', 'Unknown')}\")\n",
    "    \n",
    "    # Check if the FileID column exists\n",
    "    if file_id_column not in dataframe.columns:\n",
    "        print(f\"{ANSI['R']}Error: Column '{file_id_column}' not found in dataframe{ANSI['W']}\")\n",
    "        return []\n",
    "    \n",
    "    # Get unique FileIDs to avoid downloading duplicates\n",
    "    file_ids = dataframe[file_id_column].unique()\n",
    "    total_files = len(file_ids)\n",
    "    \n",
    "    print(f\"{ANSI['B']}Starting download of {total_files} files to{ANSI['W']} {download_path}\")\n",
    "    \n",
    "    downloaded_files = []\n",
    "    failed_files = []\n",
    "    batch_files = []\n",
    "    \n",
    "    # Process files in batches to show progress\n",
    "    for i, file_id in enumerate(file_ids):\n",
    "        if pd.isna(file_id):\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            # Convert to integer if needed\n",
    "            file_id = int(file_id)\n",
    "            \n",
    "            # Define output path for this file\n",
    "            file_output_path = os.path.join(download_path, f\"{file_id}\")\n",
    "            \n",
    "            # Download the file\n",
    "            result = a3conn.downloadFile(\n",
    "                file_id,\n",
    "                asStream=False,\n",
    "                destDir=file_output_path,\n",
    "                filename=f\"{file_id}.dcm\",\n",
    "                inWorklist=False\n",
    "            )\n",
    "            \n",
    "            downloaded_files.append(result)\n",
    "            batch_files.append(result)\n",
    "            \n",
    "            # Show progress every batch_size files\n",
    "            if (i + 1) % batch_size == 0 or (i + 1) == total_files:\n",
    "                print(f\"{ANSI['B']}Progress:{ANSI['W']} {i + 1}/{total_files} {ANSI['B']}files processed {ANSI['W']}({ANSI['B']}{((i + 1) / total_files * 100):.1f}%{ANSI['W']})\")\n",
    "                \n",
    "                # Convert batch if requested\n",
    "                if convert and batch_files:\n",
    "                    print(f\"{ANSI['B']}Converting batch of {ANSI['W']}{len(batch_files)}{ANSI['B']} DICOM files to PNG...{ANSI['W']}\")\n",
    "                    convert_dicom_to_png(\n",
    "                        import_folder=DOWNLOAD_PATH,\n",
    "                        export_folder=DOWNLOAD_PATH,\n",
    "                        bit_depth=BIT_DEPTH,\n",
    "                        create_subfolders=CREATE_SUBFOLDERS,\n",
    "                        delete_dicom=DELETE_DICOM,\n",
    "                        monochrome=MONOCHROME,\n",
    "                        resize_y=RESIZE_Y,\n",
    "                    )\n",
    "                    batch_files = []\n",
    "                \n",
    "        except Exception as e:\n",
    "            failed_files.append(file_id)\n",
    "            print(f\"{ANSI['R']}Error downloading file ID {file_id}: {str(e)}{ANSI['W']}\")\n",
    "    \n",
    "    # Summary\n",
    "    print(f\"{ANSI['G']}Download complete: {len(downloaded_files)} files downloaded successfully{ANSI['W']}\")\n",
    "    if failed_files:\n",
    "        print(f\"{ANSI['R']}Failed to download {len(failed_files)} files{ANSI['W']}\")\n",
    "    \n",
    "    return downloaded_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download files from the labeled data\n",
    "if df_labeled_data is not None:\n",
    "    print(f\"{ANSI['B']}Starting download of ArchiMed files...{ANSI['W']}\")\n",
    "    downloaded_files = download_archimed_files(\n",
    "        dataframe=df_labeled_data,\n",
    "        download_path=DOWNLOAD_PATH,\n",
    "        batch_size=2,\n",
    "        convert=True\n",
    "    )\n",
    "    print(f\"{ANSI['G']}Downloaded {len(downloaded_files)} files to {DOWNLOAD_PATH}{ANSI['W']}\")\n",
    "else:\n",
    "    print(f\"{ANSI['R']}Cannot download files: No labeled data available{ANSI['W']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert DICOM files to PNG files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "# convert_dicom_to_png(\n",
    "#     import_folder=DOWNLOAD_PATH,\n",
    "#     export_folder=DOWNLOAD_PATH,\n",
    "#     bit_depth=BIT_DEPTH,\n",
    "#     create_subfolders=CREATE_SUBFOLDERS,\n",
    "#     delete_dicom=DELETE_DICOM,\n",
    "#     monochrome=MONOCHROME,\n",
    "#     resize_y=RESIZE_Y,\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
