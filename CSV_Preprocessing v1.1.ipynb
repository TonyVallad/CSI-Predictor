{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h1 align=\"center\">CSV Preprocessing</h1>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Specific Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV Files\n",
    "CSV_FOLDER = \"../../data/\"\n",
    "# CSV_FOLDER = \"/home/pyuser/data/\"\n",
    "CSV_LABELS_FILE = \"paradise_csi.csv\"\n",
    "CSV_PATIENTS_FILE = \"PatientIds.csv\"\n",
    "CSV_ARCHIMED_DATA = \"ArchiMed_Data.csv\"\n",
    "CSV_TO_EXPLORE_1 = \"paradise_csi_drop_non_nan_w_classes.csv\"\n",
    "CSV_TO_EXPLORE_2 = \"paradise_csi_w_classes_w_non_nan.csv\"\n",
    "CSV_SEPARATOR = \",\"  # Specify the CSV separator, e.g., ',' or '\\t'\n",
    "IMPORT_COLUMNS = []  # If empty, import all columns\n",
    "CHUNK_SIZE = 50000  # Number of rows per chunk\n",
    "\n",
    "# Project Specific Variables\n",
    "EXAM_CODE_START = \"2020-128 01-\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSI escape codes for colored output\n",
    "ANSI = {\n",
    "    'R' : '\\033[91m',  # Red\n",
    "    'G' : '\\033[92m',  # Green\n",
    "    'B' : '\\033[94m',  # Blue\n",
    "    'Y' : '\\033[93m',  # Yellow\n",
    "    'W' : '\\033[0m',  # White\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import CSVs to Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSV Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Import CSV files into dataframes\n",
    "try:\n",
    "    # Import labels data\n",
    "    df_labels = pd.read_csv(\n",
    "        CSV_FOLDER + CSV_LABELS_FILE,\n",
    "        sep=CSV_SEPARATOR,\n",
    "        usecols=IMPORT_COLUMNS if IMPORT_COLUMNS else None,\n",
    "        chunksize=CHUNK_SIZE\n",
    "    )\n",
    "    df_labels = pd.concat(df_labels, ignore_index=True)\n",
    "    print(f\"{ANSI['G']}Successfully imported{ANSI['W']} {CSV_LABELS_FILE}\")\n",
    "    \n",
    "    # Import patient data  \n",
    "    # df_patients = pd.read_csv(\n",
    "    #     CSV_FOLDER + CSV_PATIENTS_FILE,\n",
    "    #     sep=CSV_SEPARATOR,\n",
    "    #     usecols=IMPORT_COLUMNS if IMPORT_COLUMNS else None,\n",
    "    #     chunksize=CHUNK_SIZE\n",
    "    # )\n",
    "    # df_patients = pd.concat(df_patients, ignore_index=True)\n",
    "    # print(f\"{ANSI['G']}Successfully imported{ANSI['W']} {CSV_PATIENTS_FILE}\")\n",
    "    \n",
    "    # Import ArchiMed CSV\n",
    "    df_archimed = pd.read_csv(\n",
    "        CSV_FOLDER + CSV_ARCHIMED_DATA,\n",
    "        sep=';',  # ArchiMed CSV separator is ';'\n",
    "        usecols=IMPORT_COLUMNS if IMPORT_COLUMNS else None,\n",
    "        chunksize=CHUNK_SIZE\n",
    "    )\n",
    "    df_archimed = pd.concat(df_archimed, ignore_index=True)\n",
    "    print(f\"{ANSI['G']}Successfully imported{ANSI['W']} {CSV_ARCHIMED_DATA}\")\n",
    "    \n",
    "    # Import CSV to explore 1\n",
    "    # df_to_explore_1 = pd.read_csv(\n",
    "    #     CSV_FOLDER + CSV_TO_EXPLORE_1,\n",
    "    #     sep=CSV_SEPARATOR,\n",
    "    #     usecols=IMPORT_COLUMNS if IMPORT_COLUMNS else None,\n",
    "    #     chunksize=CHUNK_SIZE\n",
    "    # )\n",
    "    # df_to_explore_1 = pd.concat(df_to_explore_1, ignore_index=True)\n",
    "    # print(f\"{ANSI['G']}Successfully imported{ANSI['W']} {CSV_TO_EXPLORE_1}\")\n",
    "    \n",
    "    # Import CSV to explore 2\n",
    "    # df_to_explore_2 = pd.read_csv(\n",
    "    #     CSV_FOLDER + CSV_TO_EXPLORE_2,\n",
    "    #     sep=CSV_SEPARATOR,\n",
    "    #     usecols=IMPORT_COLUMNS if IMPORT_COLUMNS else None,\n",
    "    #     chunksize=CHUNK_SIZE\n",
    "    # )\n",
    "    # df_to_explore_2 = pd.concat(df_to_explore_2, ignore_index=True)\n",
    "    # print(f\"{ANSI['G']}Successfully imported{ANSI['W']} {CSV_TO_EXPLORE_2}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"{ANSI['R']}Error importing CSV files: {str(e)}{ANSI['W']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rename columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ArchiMed CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns in df_archimed to be more readable\n",
    "df_archimed.rename(columns={\n",
    "    \"Exam\": 'ExamCode',  # Not really the patient's name but the exam code\n",
    "    'Instance Number': 'InstanceNumber',  # ID of the image series\n",
    "    # 'FileID': 'FileID',  # ID of the image file\n",
    "    'Admission ID - (0038,0010)': 'AdmissionID',  # ID of the patient's hospital stay\n",
    "    'Series Number - (0020,0011)': 'SeriesNumber',  # Number of the series in an exam\n",
    "    'Series Description - (0008,103E)': 'SeriesDescription',  # Description of the image series\n",
    "    'Image Type - (0008,0008)': 'ImageType',  # Type of the image (PRIMARY, etc.)\n",
    "    'Derivation Description - (0008,2111)': 'DerivationDescription',  # Description of the image derivation\n",
    "}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labeled Data CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns in df_labels to be more readable\n",
    "df_labels.rename(columns={\n",
    "    'number': 'ExamCodeEnd',  # Last 4 digits of the exam code\n",
    "    'id_number': 'AdmissionID',  # ID of the patient's hospital stay\n",
    "}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check ArchiMed data to merge with Labeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_primary_images_distribution(df_archimed):\n",
    "    \"\"\"\n",
    "    Analyzes the distribution of PRIMARY images across exams in the ArchiMed dataset.\n",
    "    \n",
    "    Args:\n",
    "        df_archimed (pd.DataFrame): ArchiMed dataframe containing exam and image information\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (primary_images_per_exam, exams_with_no_primary, exams_with_multiple)\n",
    "    \"\"\"\n",
    "    # Count number of primary images per exam\n",
    "    primary_images_per_exam = df_archimed[df_archimed['ImageType'].str.contains('PRIMARY', na=False)].groupby('ExamCode').size()\n",
    "\n",
    "    # Group exams by number of primary images they contain\n",
    "    exams_distribution = primary_images_per_exam.value_counts().sort_index()\n",
    "    print(f\"{ANSI['G']}Distribution of PRIMARY images per exam:{ANSI['W']}\")\n",
    "    for count, num_exams in exams_distribution.items():\n",
    "        print(f\"  {num_exams} exams have {count} PRIMARY images\")\n",
    "\n",
    "    # Get total number of unique exams\n",
    "    total_exams = df_archimed['ExamCode'].nunique()\n",
    "\n",
    "    # Calculate exams with no primary images\n",
    "    exams_with_no_primary = total_exams - len(primary_images_per_exam)\n",
    "    exams_with_multiple = len(primary_images_per_exam[primary_images_per_exam > 1])\n",
    "\n",
    "    print(f\"\\n{ANSI['G']}Summary:{ANSI['W']}\")\n",
    "    print(f\"  {len(primary_images_per_exam)} exams have at least one PRIMARY image\")\n",
    "    print(f\"  {exams_with_no_primary} exams have no PRIMARY images\")\n",
    "    print(f\"  {exams_with_multiple} exams have multiple PRIMARY images\")\n",
    "    \n",
    "    return primary_images_per_exam, exams_with_no_primary, exams_with_multiple\n",
    "\n",
    "# Call the function\n",
    "primary_images_per_exam, exams_with_no_primary, exams_with_multiple = analyze_primary_images_distribution(df_archimed)\n",
    "\n",
    "def filter_primary_images(df):\n",
    "    \"\"\"\n",
    "    Filters a dataframe to keep only PRIMARY images.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing image information with ImageType column\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Filtered dataframe containing only PRIMARY images\n",
    "    \"\"\"\n",
    "    df_primary = df[df['ImageType'].str.contains('PRIMARY', na=False)]\n",
    "    print(f\"\\n{ANSI['G']}Successfully filtered{ANSI['W']} PRIMARY images from ArchiMed data\")\n",
    "    return df_primary\n",
    "\n",
    "# Filter ArchiMed data to keep only PRIMARY images\n",
    "df_archimed_primary = filter_primary_images(df_archimed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_series_with_multiple_primary(primary_images_per_exam):\n",
    "    \"\"\"\n",
    "    Prints exam codes that have multiple primary images.\n",
    "    \n",
    "    Args:\n",
    "        primary_images_per_exam (pd.Series): Series containing count of primary images per exam\n",
    "    \"\"\"\n",
    "    series_with_multiple = primary_images_per_exam[primary_images_per_exam > 1].sort_values(ascending=False)\n",
    "    print(f\"{ANSI['G']}Series with multiple PRIMARY images:{ANSI['W']}\")\n",
    "    for exam_code, num_primary in series_with_multiple.items():\n",
    "        print(f\"  ExamCode: {exam_code} ({num_primary} primary images)\")\n",
    "\n",
    "# Call the function\n",
    "print_series_with_multiple_primary(primary_images_per_exam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge ArchiMed Data with Labeled Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_archimed_with_labels(df_archimed, df_labels):\n",
    "    \"\"\"\n",
    "    Merges ArchiMed data with Labeled data on ExamCodeEnd.\n",
    "    \n",
    "    Args:\n",
    "        df_archimed (pd.DataFrame): DataFrame containing ArchiMed data\n",
    "        df_labels (pd.DataFrame): DataFrame containing labeled data\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Merged dataframe with selected columns in specified order\n",
    "    \"\"\"\n",
    "    # Extract last 4 digits from ExamCode in df_archimed and convert to int\n",
    "    df_archimed['ExamCodeEnd'] = df_archimed['ExamCode'].str[-4:].astype(int)\n",
    "    \n",
    "    # Check if AdmissionID exists in both dataframes\n",
    "    admission_in_archimed = 'AdmissionID' in df_archimed.columns\n",
    "    admission_in_labels = 'AdmissionID' in df_labels.columns\n",
    "    \n",
    "    # Handle duplicate columns for the merge\n",
    "    if admission_in_archimed and admission_in_labels:\n",
    "        # Create temporary column names for the merge\n",
    "        df_archimed = df_archimed.rename(columns={'AdmissionID': 'AdmissionID_archimed'})\n",
    "        df_labels = df_labels.rename(columns={'AdmissionID': 'AdmissionID_labels'})\n",
    "    \n",
    "    # Merge dataframes on ExamCodeEnd\n",
    "    df_merged = pd.merge(df_archimed, df_labels, on='ExamCodeEnd', how='inner')\n",
    "    \n",
    "    # Handle AdmissionID after merge if it existed in both dataframes\n",
    "    if admission_in_archimed and admission_in_labels:\n",
    "        # Check if values are identical where both exist\n",
    "        mask_both_exist = df_merged['AdmissionID_archimed'].notna() & df_merged['AdmissionID_labels'].notna()\n",
    "        if mask_both_exist.any():\n",
    "            mismatch = df_merged.loc[mask_both_exist, 'AdmissionID_archimed'] != df_merged.loc[mask_both_exist, 'AdmissionID_labels']\n",
    "            if mismatch.any():\n",
    "                print(f\"\\n{ANSI['Y']}WARNING:{ANSI['W']} AdmissionID values differ between ArchiMed and Labels datasets for {mismatch.sum()} rows\")\n",
    "        \n",
    "        # Use archimed value if available, otherwise use labels value\n",
    "        df_merged['AdmissionID'] = df_merged['AdmissionID_archimed'].fillna(df_merged['AdmissionID_labels'])\n",
    "        \n",
    "        # Drop temporary columns\n",
    "        df_merged = df_merged.drop(['AdmissionID_archimed', 'AdmissionID_labels'], axis=1)\n",
    "    elif admission_in_archimed:\n",
    "        # AdmissionID only in archimed, no action needed\n",
    "        pass\n",
    "    elif admission_in_labels:\n",
    "        # AdmissionID only in labels, no action needed\n",
    "        pass\n",
    "    \n",
    "    # Get unmerged entries from both dataframes\n",
    "    df_unmerged_archimed = df_archimed[~df_archimed['ExamCodeEnd'].isin(df_merged['ExamCodeEnd'])]\n",
    "    df_unmerged_labels = df_labels[~df_labels['ExamCodeEnd'].isin(df_merged['ExamCodeEnd'])]\n",
    "    \n",
    "    print(f\"\\n{ANSI['G']}Successfully merged{ANSI['W']} ArchiMed data with Labeled data\")\n",
    "    print(f\"Number of rows in merged dataset: {len(df_merged)}\")\n",
    "    print(f\"Number of unmerged rows from ArchiMed: {len(df_unmerged_archimed)}\")\n",
    "    print(f\"Number of unmerged rows from Labels: {len(df_unmerged_labels)}\")\n",
    "    \n",
    "    # Select and reorder columns, checking if each column exists\n",
    "    columns_order = [\n",
    "        'ExamCode', 'SeriesNumber', 'SeriesDescription', 'FileID', 'Filename',\n",
    "        'ImageType', 'DerivationDescription', 'AdmissionID', 'csi_total', 'csi',\n",
    "        'right_sup', 'left_sup', 'right_mid', 'left_mid', 'right_inf', 'left_inf'\n",
    "    ]\n",
    "    \n",
    "    # Filter out columns that don't exist in the merged dataframe\n",
    "    available_columns = [col for col in columns_order if col in df_merged.columns]\n",
    "    \n",
    "    # Report any missing columns\n",
    "    missing_columns = set(columns_order) - set(available_columns)\n",
    "    if missing_columns:\n",
    "        print(f\"\\n{ANSI['Y']}WARNING:{ANSI['W']} The following columns are missing from the merged dataframe: {', '.join(missing_columns)}\")\n",
    "    \n",
    "    return df_merged[available_columns]\n",
    "\n",
    "# Call the function\n",
    "df_merged = merge_archimed_with_labels(df_archimed, df_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete Series that are not 'Chest' when there are multiple series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_non_chest_series(df_merged):\n",
    "    \"\"\"\n",
    "    Removes series that are not 'Chest' or 'LIT' when an exam has multiple series.\n",
    "    \n",
    "    Args:\n",
    "        df_merged (pd.DataFrame): DataFrame containing merged data\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (filtered_df, removed_df) - the filtered DataFrame and the removed rows\n",
    "    \"\"\"\n",
    "    # First, identify ExamCodes with multiple distinct Series\n",
    "    series_per_exam = df_merged.groupby('ExamCode')['SeriesNumber'].nunique()\n",
    "    exams_with_multiple = series_per_exam[series_per_exam > 1].index\n",
    "\n",
    "    # Filter rows where ExamCode has multiple series but SerieDescription is not 'Chest' or 'LIT'\n",
    "    mask = (\n",
    "        (df_merged['ExamCode'].isin(exams_with_multiple)) & \n",
    "        (~df_merged['SeriesDescription'].str.contains('Chest|LIT', case=False, na=False))\n",
    "    )\n",
    "\n",
    "    # Save rows that will be removed\n",
    "    df_lines_removed = df_merged[mask].copy()\n",
    "\n",
    "    # Remove these rows\n",
    "    df_filtered = df_merged[~mask].copy()\n",
    "\n",
    "    print(f\"\\n{ANSI['G']}Successfully removed non-Chest series{ANSI['W']} from exams with multiple series\")\n",
    "    print(f\"Number of rows removed: {mask.sum()}\")\n",
    "    print(f\"Number of rows remaining: {len(df_filtered)}\")\n",
    "    print(f\"Removed rows saved in df_lines_removed\")\n",
    "    \n",
    "    return df_filtered, df_lines_removed\n",
    "\n",
    "# Call the function\n",
    "df_merged, df_lines_removed = remove_non_chest_series(df_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_remaining_series_after_removal(df_merged, df_lines_removed):\n",
    "    \"\"\"\n",
    "    Checks that there is still at least 1 serie per exam after removing Series that are not 'Chest'\n",
    "    \n",
    "    Args:\n",
    "        df_merged (pd.DataFrame): DataFrame after filtering\n",
    "        df_lines_removed (pd.DataFrame): DataFrame containing removed rows\n",
    "        \n",
    "    Returns:\n",
    "        list: List of exam codes that have no remaining series\n",
    "    \"\"\"\n",
    "    # Get unique ExamCodes from removed rows\n",
    "    removed_exam_codes = df_lines_removed['ExamCode'].unique()\n",
    "\n",
    "    # Check how many distinct series remain for each of these ExamCodes\n",
    "    remaining_series = df_merged[df_merged['ExamCode'].isin(removed_exam_codes)].groupby('ExamCode')['SeriesNumber'].nunique()\n",
    "\n",
    "    # Check if any ExamCode has 0 remaining series\n",
    "    empty_exams = removed_exam_codes[~np.isin(removed_exam_codes, remaining_series.index)]\n",
    "\n",
    "    if len(empty_exams) > 0:\n",
    "        print(f\"\\n{ANSI['R']}Warning:{ANSI['W']} {len(empty_exams)} exams have no remaining series after removal:\")\n",
    "        print(empty_exams)\n",
    "    else:\n",
    "        print(f\"\\n{ANSI['G']}All exams still have at least one series{ANSI['W']} after removing non-Chest series\")\n",
    "        print(\"Minimum series per exam:\", remaining_series.min())\n",
    "        print(\"Maximum series per exam:\", remaining_series.max())\n",
    "        print(\"Average series per exam: {:.2f}\".format(remaining_series.mean()))\n",
    "    \n",
    "    return empty_exams\n",
    "\n",
    "# Call the function\n",
    "empty_exams = check_remaining_series_after_removal(df_merged, df_lines_removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_series_distribution(df):\n",
    "    \"\"\"\n",
    "    Displays the distribution of series per exam and images per series.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing exam data with ExamCode and SeriesNumber columns\n",
    "    \"\"\"\n",
    "    # Count number of series per exam\n",
    "    series_per_exam = df.groupby('ExamCode')['SeriesNumber'].nunique()\n",
    "    \n",
    "    # Get distribution of series per exam\n",
    "    distribution = series_per_exam.value_counts().sort_index()\n",
    "    \n",
    "    print(f\"\\n{ANSI['G']}Distribution of series per exam:{ANSI['W']}\")\n",
    "    for num_series, count in distribution.items():\n",
    "        print(f\"{count} exams have {num_series} series\")\n",
    "    \n",
    "    # Count number of images per series\n",
    "    images_per_series = df.groupby(['ExamCode', 'SeriesNumber'])['SOPInstanceUID'].count()\n",
    "    \n",
    "    # Get distribution of images per series\n",
    "    img_distribution = images_per_series.value_counts().sort_index()\n",
    "    \n",
    "    print(f\"\\n{ANSI['G']}Distribution of images per series:{ANSI['W']}\")\n",
    "    for num_images, count in img_distribution.items():\n",
    "        print(f\"{count} series have {num_images} images\")\n",
    "    \n",
    "    print(f\"\\n{ANSI['G']}Total exams:{ANSI['W']} {len(series_per_exam)}\")\n",
    "    print(f\"{ANSI['G']}Total images:{ANSI['W']} {df['SOPInstanceUID'].nunique()}\")\n",
    "    print(f\"{ANSI['G']}Average series per exam:{ANSI['W']} {series_per_exam.mean():.2f}\")\n",
    "\n",
    "# Call the function\n",
    "display_series_distribution(df_merged)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
