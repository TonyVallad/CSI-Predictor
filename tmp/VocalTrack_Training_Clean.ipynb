{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Vocal Tract Segmentation Training Pipeline\n",
        "\n",
        "This notebook provides a clean, organized implementation for training Mask R-CNN models on vocal tract medical imaging data.\n",
        "\n",
        "## Features\n",
        "- **Medical Image Segmentation**: Multi-class detection of vocal tract structures\n",
        "- **Mask R-CNN Architecture**: ResNet50-FPN backbone with custom modifications\n",
        "- **Data Pipeline**: Efficient loading from H5 files with augmentation\n",
        "- **Clinical Integration**: DICOM support and ROI file generation\n",
        "- **PyTorch Lightning**: Modern training framework with automatic logging\n",
        "\n",
        "## Workflow\n",
        "1. **Setup & Configuration**: Import libraries and define parameters\n",
        "2. **Data Loading**: Custom dataset classes for medical imaging\n",
        "3. **Model Definition**: Mask R-CNN with custom loss weighting\n",
        "4. **Training**: PyTorch Lightning training loop\n",
        "5. **Inference**: Model prediction and post-processing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 1. Setup & Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Core imports\n",
        "import os\n",
        "import sys\n",
        "import warnings\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Tuple, Optional, Any\n",
        "import random\n",
        "\n",
        "# Data science and numerical computing\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import h5py\n",
        "\n",
        "# Deep learning\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "import lightning as L\n",
        "\n",
        "# Image processing\n",
        "import cv2\n",
        "from PIL import Image\n",
        "from skimage import morphology\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Data augmentation\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "# FiftyOne for dataset management\n",
        "import fiftyone as fo\n",
        "from fiftyone.torch import FiftyOneTorchDataset\n",
        "\n",
        "# Medical imaging\n",
        "import pydicom\n",
        "\n",
        "# Suppress warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "def set_seed(seed: int = 42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(42)\n",
        "print(\"âœ… Libraries imported successfully\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ANSI escape codes for colored output\n",
        "ANSI = {\n",
        "    'R' : '\\033[91m',  # Red\n",
        "    'G' : '\\033[92m',  # Green\n",
        "    'B' : '\\033[94m',  # Blue\n",
        "    'Y' : '\\033[93m',  # Yellow\n",
        "    'W' : '\\033[0m',  # White\n",
        "}\n",
        "\n",
        "# Configuration\n",
        "CONFIG = {\n",
        "    # Data paths\n",
        "    \"data_dir\": \"/home/pyuser/data/RealTimeSwallowing\",\n",
        "    \"output_dir\": \"./outputs\",\n",
        "    \n",
        "    # Training parameters\n",
        "    \"batch_size\": 1,\n",
        "    \"num_workers\": 4,\n",
        "    \"learning_rate\": 1e-3,\n",
        "    \"max_epochs\": 100,\n",
        "    \"patience\": 10,\n",
        "    \n",
        "    # Model parameters\n",
        "    \"num_classes\": 34,  # Including background\n",
        "    \"trainable_backbone_layers\": 3,\n",
        "    \"image_size\": 480,\n",
        "    \n",
        "    # Loss weights\n",
        "    \"classification_loss_weight\": 1.0,\n",
        "    \"box_regression_loss_weight\": 1.0,\n",
        "    \"mask_loss_weight\": 2.0,\n",
        "    \"rpn_objectness_loss_weight\": 1.0,\n",
        "    \"rpn_box_loss_weight\": 1.0,\n",
        "    \n",
        "    # Data split\n",
        "    \"train_split\": 0.8,\n",
        "    \"val_split\": 0.2,\n",
        "    \n",
        "    # Augmentation\n",
        "    \"augmentation_prob\": 0.5,\n",
        "}\n",
        "\n",
        "# Anatomical class definitions - Focus on swallowing-related structures\n",
        "ANATOMICAL_CLASSES = [\n",
        "    'background', 'arytenoid-cartilage', 'brain-stem', 'c1', 'c2', 'c3', 'c4', 'c5', 'c6',\n",
        "    'cerebellum', 'chin', 'epiglottis', 'frontal-sinus', 'geniohyoid-muscle', 'head',\n",
        "    'incisior-hard-palate', 'lower-lip', 'mandible-incisior', 'mouth-end', 'nasal-root',\n",
        "    'nose-tip', 'pharynx', 'plate-link', 'sphenoid', 'soft-palate', 'soft-palate-midline',\n",
        "    'thyroid-cartilage', 'tongue', 'tongue-floor', 'tongue-muscle', 'upper-lip',\n",
        "    'vocal-folds', 'vocal-track'\n",
        "]\n",
        "\n",
        "# Patient and series data\n",
        "PATIENT_DATA = [\n",
        "    {'Patient': '2017-110^01-0196-V1MR', 'Serie': '7', 'Experimentday': '02072025'},\n",
        "    {'Patient': '2008-003^01-1791', 'Serie': '13', 'Experimentday': '03072025'},\n",
        "    {'Patient': '2008-003^01-1791', 'Serie': '15', 'Experimentday': '03072025'},\n",
        "    {'Patient': '2017-110_01-0170-V1MR', 'Serie': '115', 'Experimentday': '30062025'}\n",
        "]\n",
        "\n",
        "# Create output directory\n",
        "os.makedirs(CONFIG[\"output_dir\"], exist_ok=True)\n",
        "print(f\"âœ… Configuration loaded. Output directory: {CONFIG['output_dir']}\")\n",
        "print(f\"ðŸ“Š Number of anatomical classes: {len(ANATOMICAL_CLASSES)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 2. Data Loading & Preprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_h5_data(h5_filepath: str, frame_key: str) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Load data from H5 file for a specific frame.\n",
        "    \n",
        "    Args:\n",
        "        h5_filepath: Path to H5 file\n",
        "        frame_key: Frame identifier (e.g., 'frame_0001')\n",
        "        \n",
        "    Returns:\n",
        "        Dictionary containing image, masks, bboxes, and labels\n",
        "    \"\"\"\n",
        "    data = {\n",
        "        'image': None,\n",
        "        'masks': [],\n",
        "        'bboxes': [],\n",
        "        'labels': [],\n",
        "        'roi_names': []\n",
        "    }\n",
        "    \n",
        "    try:\n",
        "        with h5py.File(h5_filepath, 'r') as h5f:\n",
        "            # Get experiment group (first group in file)\n",
        "            exp_group_name = list(h5f.keys())[0]\n",
        "            exp_group = h5f[exp_group_name]\n",
        "            \n",
        "            if frame_key not in exp_group:\n",
        "                return data\n",
        "                \n",
        "            frame_group = exp_group[frame_key]\n",
        "            \n",
        "            # Load image (RGB or grayscale)\n",
        "            if 'image_rgb' in frame_group:\n",
        "                data['image'] = frame_group['image_rgb'][()]\n",
        "            elif 'image' in frame_group:\n",
        "                gray_img = frame_group['image'][()]\n",
        "                data['image'] = np.stack([gray_img, gray_img, gray_img], axis=-1)\n",
        "            \n",
        "            # Get ROI list\n",
        "            roi_list = exp_group['roi_list'][()]\n",
        "            if isinstance(roi_list[0], bytes):\n",
        "                roi_list = [roi.decode() for roi in roi_list]\n",
        "            \n",
        "            # Load masks and bboxes for each ROI\n",
        "            for roi_name in roi_list:\n",
        "                mask_key = f'{roi_name}_mask'\n",
        "                bbox_key = f'{roi_name}_bbox'\n",
        "                \n",
        "                if mask_key in frame_group and bbox_key in frame_group:\n",
        "                    mask = frame_group[mask_key][()]\n",
        "                    bbox = frame_group[bbox_key][()]\n",
        "                    \n",
        "                    # Convert bbox to Pascal VOC format (x_min, y_min, x_max, y_max)\n",
        "                    x, y, w, h = bbox\n",
        "                    bbox_pascal = [x, y, x + w, y + h]\n",
        "                    \n",
        "                    # Get class label\n",
        "                    if roi_name in ANATOMICAL_CLASSES:\n",
        "                        label = ANATOMICAL_CLASSES.index(roi_name)\n",
        "                        \n",
        "                        data['masks'].append(mask)\n",
        "                        data['bboxes'].append(bbox_pascal)\n",
        "                        data['labels'].append(label)\n",
        "                        data['roi_names'].append(roi_name)\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"Error loading H5 data: {e}\")\n",
        "    \n",
        "    return data\n",
        "\n",
        "\n",
        "def create_fiftyone_dataset(patient_data: List[Dict], data_dir: str, dataset_name: str = \"vocal_tract\") -> fo.Dataset:\n",
        "    \"\"\"\n",
        "    Create or load FiftyOne dataset from patient data.\n",
        "    \n",
        "    Args:\n",
        "        patient_data: List of patient information\n",
        "        data_dir: Base data directory\n",
        "        dataset_name: Name for the FiftyOne dataset\n",
        "        \n",
        "    Returns:\n",
        "        FiftyOne dataset\n",
        "    \"\"\"\n",
        "    # Delete existing dataset if it exists\n",
        "    if dataset_name in fo.list_datasets():\n",
        "        dataset = fo.load_dataset(dataset_name)\n",
        "        dataset.delete()\n",
        "    \n",
        "    # Create new dataset\n",
        "    dataset = fo.Dataset(dataset_name)\n",
        "    \n",
        "    for patient_info in patient_data:\n",
        "        patient = patient_info['Patient']\n",
        "        serie = patient_info['Serie']\n",
        "        exp_day = patient_info['Experimentday']\n",
        "        \n",
        "        export_dir = os.path.join(\n",
        "            data_dir, patient, 'OTHER', f\"S{serie}\", f\"Clean_{exp_day}\"\n",
        "        )\n",
        "        \n",
        "        if os.path.exists(export_dir):\n",
        "            print(f\"Adding data from: {export_dir}\")\n",
        "            dataset.add_dir(\n",
        "                dataset_dir=export_dir,\n",
        "                dataset_type=fo.types.FiftyOneDataset,\n",
        "                overwrite=True\n",
        "            )\n",
        "    \n",
        "    print(f\"âœ… FiftyOne dataset '{dataset_name}' created with {len(dataset)} samples\")\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def get_augmentation_pipeline(image_size: int = 480, prob: float = 0.5) -> A.Compose:\n",
        "    \"\"\"\n",
        "    Create augmentation pipeline for medical images.\n",
        "    \n",
        "    Args:\n",
        "        image_size: Target image size\n",
        "        prob: Probability of applying augmentations\n",
        "        \n",
        "    Returns:\n",
        "        Albumentations composition\n",
        "    \"\"\"\n",
        "    return A.Compose([\n",
        "        A.Resize(image_size, image_size),\n",
        "        A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=prob),\n",
        "        A.RandomGamma(gamma_limit=(80, 120), p=prob),\n",
        "        A.GaussNoise(var_limit=(10.0, 50.0), p=prob * 0.5),\n",
        "        A.HorizontalFlip(p=prob * 0.5),\n",
        "        A.ShiftScaleRotate(\n",
        "            shift_limit=0.1, \n",
        "            scale_limit=0.1, \n",
        "            rotate_limit=15, \n",
        "            p=prob,\n",
        "            border_mode=cv2.BORDER_CONSTANT\n",
        "        ),\n",
        "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        ToTensorV2(),\n",
        "    ], bbox_params=A.BboxParams(\n",
        "        format='pascal_voc',\n",
        "        label_fields=['labels'],\n",
        "        min_visibility=0.3\n",
        "    ))\n",
        "\n",
        "print(\"âœ… Data loading utilities defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class VocalTrackDataset(FiftyOneTorchDataset):\n",
        "    \"\"\"Clean dataset class for vocal tract segmentation.\"\"\"\n",
        "    \n",
        "    def __init__(\n",
        "        self,\n",
        "        fiftyone_dataset: fo.Dataset,\n",
        "        transform: Optional[A.Compose] = None,\n",
        "        classes: List[str] = None\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initialize dataset.\n",
        "        \n",
        "        Args:\n",
        "            fiftyone_dataset: FiftyOne dataset\n",
        "            transform: Augmentation pipeline\n",
        "            classes: List of class names\n",
        "        \"\"\"\n",
        "        self.classes = classes or ANATOMICAL_CLASSES\n",
        "        self.transform = transform\n",
        "        self.class_to_idx = {cls: idx for idx, cls in enumerate(self.classes)}\n",
        "        \n",
        "        super().__init__(\n",
        "            fiftyone_dataset,\n",
        "            gt_field=\"ground_truth.articulator_detections\",\n",
        "            classes=self.classes\n",
        "        )\n",
        "    \n",
        "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:\n",
        "        \"\"\"Get a sample from the dataset.\"\"\"\n",
        "        sample = super().__getitem__(idx)\n",
        "        image = np.array(sample[0])\n",
        "        target = sample[1]\n",
        "        \n",
        "        # Convert to format expected by Mask R-CNN\n",
        "        masks = []\n",
        "        boxes = []\n",
        "        labels = []\n",
        "        \n",
        "        if \"detections\" in target:\n",
        "            detections = target[\"detections\"]\n",
        "            \n",
        "            for detection in detections:\n",
        "                # Get mask\n",
        "                if hasattr(detection, 'mask') and detection.mask is not None:\n",
        "                    mask = detection.mask\n",
        "                    masks.append(mask)\n",
        "                    \n",
        "                    # Get bounding box in Pascal VOC format\n",
        "                    bbox = detection.bounding_box  # [x, y, w, h] in relative coords\n",
        "                    h, w = image.shape[:2]\n",
        "                    \n",
        "                    # Convert to absolute coordinates\n",
        "                    x_min = int(bbox[0] * w)\n",
        "                    y_min = int(bbox[1] * h)\n",
        "                    x_max = int((bbox[0] + bbox[2]) * w)\n",
        "                    y_max = int((bbox[1] + bbox[3]) * h)\n",
        "                    \n",
        "                    boxes.append([x_min, y_min, x_max, y_max])\n",
        "                    \n",
        "                    # Get label\n",
        "                    label_name = detection.label\n",
        "                    label_idx = self.class_to_idx.get(label_name, 0)\n",
        "                    labels.append(label_idx)\n",
        "        \n",
        "        # Apply augmentations if provided\n",
        "        if self.transform and len(masks) > 0:\n",
        "            try:\n",
        "                augmented = self.transform(\n",
        "                    image=image,\n",
        "                    masks=masks,\n",
        "                    bboxes=boxes,\n",
        "                    labels=labels\n",
        "                )\n",
        "                \n",
        "                image = augmented['image']\n",
        "                masks = augmented.get('masks', masks)\n",
        "                boxes = augmented.get('bboxes', boxes)\n",
        "                labels = augmented.get('labels', labels)\n",
        "                \n",
        "            except Exception as e:\n",
        "                print(f\"Augmentation failed: {e}\")\n",
        "                # Fallback to basic transform\n",
        "                basic_transform = A.Compose([\n",
        "                    A.Resize(CONFIG[\"image_size\"], CONFIG[\"image_size\"]),\n",
        "                    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "                    ToTensorV2()\n",
        "                ])\n",
        "                image = basic_transform(image=image)['image']\n",
        "        \n",
        "        # Ensure image is a tensor\n",
        "        if not isinstance(image, torch.Tensor):\n",
        "            image = torch.from_numpy(image).permute(2, 0, 1).float()\n",
        "        \n",
        "        # Convert to tensors\n",
        "        target_dict = {\n",
        "            \"boxes\": torch.as_tensor(boxes, dtype=torch.float32) if boxes else torch.zeros((0, 4)),\n",
        "            \"labels\": torch.as_tensor(labels, dtype=torch.int64) if labels else torch.zeros((0,), dtype=torch.int64),\n",
        "            \"masks\": torch.as_tensor(np.array(masks), dtype=torch.uint8) if masks else torch.zeros((0, CONFIG[\"image_size\"], CONFIG[\"image_size\"]), dtype=torch.uint8),\n",
        "            \"image_id\": torch.tensor([idx]),\n",
        "            \"area\": torch.tensor([(box[2] - box[0]) * (box[3] - box[1]) for box in boxes]) if boxes else torch.zeros((0,)),\n",
        "            \"iscrowd\": torch.zeros((len(boxes),), dtype=torch.int64) if boxes else torch.zeros((0,), dtype=torch.int64)\n",
        "        }\n",
        "        \n",
        "        return image, target_dict\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "    \"\"\"Custom collate function for DataLoader.\"\"\"\n",
        "    images, targets = zip(*batch)\n",
        "    images = torch.stack(images)\n",
        "    return images, list(targets)\n",
        "\n",
        "\n",
        "print(\"âœ… Dataset class defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class VocalTrackDataModule(L.LightningDataModule):\n",
        "    \"\"\"PyTorch Lightning DataModule for vocal tract segmentation.\"\"\"\n",
        "    \n",
        "    def __init__(\n",
        "        self,\n",
        "        data_dir: str,\n",
        "        patient_data: List[Dict],\n",
        "        batch_size: int = 1,\n",
        "        num_workers: int = 4,\n",
        "        train_split: float = 0.8,\n",
        "        augmentation_prob: float = 0.5,\n",
        "        image_size: int = 480\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initialize DataModule.\n",
        "        \n",
        "        Args:\n",
        "            data_dir: Base data directory\n",
        "            patient_data: List of patient information\n",
        "            batch_size: Batch size for training\n",
        "            num_workers: Number of data loading workers\n",
        "            train_split: Training data split ratio\n",
        "            augmentation_prob: Probability of applying augmentations\n",
        "            image_size: Target image size\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.data_dir = data_dir\n",
        "        self.patient_data = patient_data\n",
        "        self.batch_size = batch_size\n",
        "        self.num_workers = num_workers\n",
        "        self.train_split = train_split\n",
        "        self.augmentation_prob = augmentation_prob\n",
        "        self.image_size = image_size\n",
        "        \n",
        "        # Store classes\n",
        "        self.classes = ANATOMICAL_CLASSES\n",
        "        \n",
        "        # Initialize transforms\n",
        "        self.train_transform = get_augmentation_pipeline(\n",
        "            image_size=image_size, \n",
        "            prob=augmentation_prob\n",
        "        )\n",
        "        \n",
        "        self.val_transform = A.Compose([\n",
        "            A.Resize(image_size, image_size),\n",
        "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "            ToTensorV2(),\n",
        "        ])\n",
        "    \n",
        "    def prepare_data(self):\n",
        "        \"\"\"Create FiftyOne dataset.\"\"\"\n",
        "        self.fiftyone_dataset = create_fiftyone_dataset(\n",
        "            patient_data=self.patient_data,\n",
        "            data_dir=self.data_dir,\n",
        "            dataset_name=\"vocal_tract_clean\"\n",
        "        )\n",
        "    \n",
        "    def setup(self, stage: str = None):\n",
        "        \"\"\"Setup train/val datasets.\"\"\"\n",
        "        if stage == \"fit\" or stage is None:\n",
        "            # Create full dataset\n",
        "            full_dataset = VocalTrackDataset(\n",
        "                fiftyone_dataset=self.fiftyone_dataset,\n",
        "                classes=self.classes\n",
        "            )\n",
        "            \n",
        "            # Split into train/val\n",
        "            total_size = len(full_dataset)\n",
        "            train_size = int(self.train_split * total_size)\n",
        "            val_size = total_size - train_size\n",
        "            \n",
        "            # Create indices for train/val split\n",
        "            indices = list(range(total_size))\n",
        "            train_indices = indices[:train_size]\n",
        "            val_indices = indices[train_size:]\n",
        "            \n",
        "            # Create train dataset with augmentations\n",
        "            train_samples = [self.fiftyone_dataset[i] for i in train_indices]\n",
        "            train_fo_dataset = fo.Dataset()\n",
        "            train_fo_dataset.add_samples(train_samples)\n",
        "            \n",
        "            self.train_dataset = VocalTrackDataset(\n",
        "                fiftyone_dataset=train_fo_dataset,\n",
        "                transform=self.train_transform,\n",
        "                classes=self.classes\n",
        "            )\n",
        "            \n",
        "            # Create validation dataset without augmentations\n",
        "            val_samples = [self.fiftyone_dataset[i] for i in val_indices]\n",
        "            val_fo_dataset = fo.Dataset()\n",
        "            val_fo_dataset.add_samples(val_samples)\n",
        "            \n",
        "            self.val_dataset = VocalTrackDataset(\n",
        "                fiftyone_dataset=val_fo_dataset,\n",
        "                transform=self.val_transform,\n",
        "                classes=self.classes\n",
        "            )\n",
        "            \n",
        "            print(f\"âœ… Dataset split: {len(self.train_dataset)} train, {len(self.val_dataset)} val\")\n",
        "    \n",
        "    def train_dataloader(self):\n",
        "        \"\"\"Create training dataloader.\"\"\"\n",
        "        return DataLoader(\n",
        "            self.train_dataset,\n",
        "            batch_size=self.batch_size,\n",
        "            shuffle=True,\n",
        "            num_workers=self.num_workers,\n",
        "            collate_fn=collate_fn,\n",
        "            pin_memory=True\n",
        "        )\n",
        "    \n",
        "    def val_dataloader(self):\n",
        "        \"\"\"Create validation dataloader.\"\"\"\n",
        "        return DataLoader(\n",
        "            self.val_dataset,\n",
        "            batch_size=self.batch_size,\n",
        "            shuffle=False,\n",
        "            num_workers=self.num_workers,\n",
        "            collate_fn=collate_fn,\n",
        "            pin_memory=True\n",
        "        )\n",
        "    \n",
        "    def get_classes(self):\n",
        "        \"\"\"Return list of class names.\"\"\"\n",
        "        return self.classes\n",
        "\n",
        "print(\"âœ… DataModule class defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 3. Model Definition\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
        "\n",
        "\n",
        "class VocalTrackMaskRCNN(L.LightningModule):\n",
        "    \"\"\"Clean Mask R-CNN implementation for vocal tract segmentation.\"\"\"\n",
        "    \n",
        "    def __init__(\n",
        "        self,\n",
        "        num_classes: int,\n",
        "        learning_rate: float = 1e-3,\n",
        "        trainable_backbone_layers: int = 3,\n",
        "        class_weights: Optional[Dict[str, float]] = None,\n",
        "        **loss_weights\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initialize Mask R-CNN model.\n",
        "        \n",
        "        Args:\n",
        "            num_classes: Number of classes including background\n",
        "            learning_rate: Learning rate for optimizer\n",
        "            trainable_backbone_layers: Number of trainable backbone layers\n",
        "            class_weights: Weights for different classes\n",
        "            **loss_weights: Loss component weights\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        \n",
        "        self.num_classes = num_classes\n",
        "        self.learning_rate = learning_rate\n",
        "        self.class_weights = class_weights or {}\n",
        "        \n",
        "        # Loss weights\n",
        "        self.classification_loss_weight = loss_weights.get('classification_loss_weight', 1.0)\n",
        "        self.box_regression_loss_weight = loss_weights.get('box_regression_loss_weight', 1.0)\n",
        "        self.mask_loss_weight = loss_weights.get('mask_loss_weight', 1.0)\n",
        "        self.rpn_objectness_loss_weight = loss_weights.get('rpn_objectness_loss_weight', 1.0)\n",
        "        self.rpn_box_loss_weight = loss_weights.get('rpn_box_loss_weight', 1.0)\n",
        "        \n",
        "        # Initialize base model\n",
        "        self.model = torchvision.models.detection.maskrcnn_resnet50_fpn(\n",
        "            weights=torchvision.models.detection.MaskRCNN_ResNet50_FPN_Weights.DEFAULT,\n",
        "            trainable_backbone_layers=trainable_backbone_layers\n",
        "        )\n",
        "        \n",
        "        # Replace the classifier head\n",
        "        in_features = self.model.roi_heads.box_predictor.cls_score.in_features\n",
        "        self.model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "        \n",
        "        # Replace the mask predictor\n",
        "        in_features_mask = self.model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
        "        hidden_layer = 256\n",
        "        self.model.roi_heads.mask_predictor = MaskRCNNPredictor(\n",
        "            in_features_mask, hidden_layer, num_classes\n",
        "        )\n",
        "    \n",
        "    def forward(self, images, targets=None):\n",
        "        \"\"\"Forward pass.\"\"\"\n",
        "        if self.training and targets is not None:\n",
        "            return self.model(images, targets)\n",
        "        else:\n",
        "            return self.model(images)\n",
        "    \n",
        "    def training_step(self, batch, batch_idx):\n",
        "        \"\"\"Training step.\"\"\"\n",
        "        images, targets = batch\n",
        "        \n",
        "        # Forward pass\n",
        "        loss_dict = self.model(images, targets)\n",
        "        \n",
        "        # Apply custom loss weights\n",
        "        weighted_losses = {}\n",
        "        weighted_losses['loss_classifier'] = loss_dict['loss_classifier'] * self.classification_loss_weight\n",
        "        weighted_losses['loss_box_reg'] = loss_dict['loss_box_reg'] * self.box_regression_loss_weight\n",
        "        weighted_losses['loss_mask'] = loss_dict['loss_mask'] * self.mask_loss_weight\n",
        "        weighted_losses['loss_objectness'] = loss_dict['loss_objectness'] * self.rpn_objectness_loss_weight\n",
        "        weighted_losses['loss_rpn_box_reg'] = loss_dict['loss_rpn_box_reg'] * self.rpn_box_loss_weight\n",
        "        \n",
        "        # Total loss\n",
        "        total_loss = sum(weighted_losses.values())\n",
        "        \n",
        "        # Log losses\n",
        "        self.log('train_loss', total_loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
        "        for loss_name, loss_value in weighted_losses.items():\n",
        "            self.log(f'train_{loss_name}', loss_value, on_step=False, on_epoch=True, logger=True)\n",
        "        \n",
        "        return total_loss\n",
        "    \n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        \"\"\"Validation step.\"\"\"\n",
        "        images, targets = batch\n",
        "        \n",
        "        # Get losses\n",
        "        self.model.train()  # Need to be in train mode to get losses\n",
        "        with torch.no_grad():\n",
        "            loss_dict = self.model(images, targets)\n",
        "        \n",
        "        # Apply custom loss weights\n",
        "        weighted_losses = {}\n",
        "        weighted_losses['loss_classifier'] = loss_dict['loss_classifier'] * self.classification_loss_weight\n",
        "        weighted_losses['loss_box_reg'] = loss_dict['loss_box_reg'] * self.box_regression_loss_weight\n",
        "        weighted_losses['loss_mask'] = loss_dict['loss_mask'] * self.mask_loss_weight\n",
        "        weighted_losses['loss_objectness'] = loss_dict['loss_objectness'] * self.rpn_objectness_loss_weight\n",
        "        weighted_losses['loss_rpn_box_reg'] = loss_dict['loss_rpn_box_reg'] * self.rpn_box_loss_weight\n",
        "        \n",
        "        # Total loss\n",
        "        total_loss = sum(weighted_losses.values())\n",
        "        \n",
        "        # Log losses\n",
        "        self.log('val_loss', total_loss, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
        "        for loss_name, loss_value in weighted_losses.items():\n",
        "            self.log(f'val_{loss_name}', loss_value, on_step=False, on_epoch=True, logger=True)\n",
        "        \n",
        "        # Get predictions for visualization (first batch only)\n",
        "        if batch_idx == 0:\n",
        "            self.model.eval()\n",
        "            with torch.no_grad():\n",
        "                predictions = self.model(images)\n",
        "            self._log_predictions(images, predictions, targets)\n",
        "        \n",
        "        return total_loss\n",
        "    \n",
        "    def _log_predictions(self, images, predictions, targets):\n",
        "        \"\"\"Log prediction visualizations.\"\"\"\n",
        "        try:\n",
        "            # Take first image from batch\n",
        "            image = images[0]\n",
        "            prediction = predictions[0]\n",
        "            target = targets[0]\n",
        "            \n",
        "            # Convert image back to numpy for visualization\n",
        "            image_np = image.cpu().permute(1, 2, 0).numpy()\n",
        "            image_np = (image_np * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406]))\n",
        "            image_np = np.clip(image_np, 0, 1)\n",
        "            \n",
        "            # Create visualization\n",
        "            fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
        "            \n",
        "            # Ground truth\n",
        "            axes[0].imshow(image_np)\n",
        "            axes[0].set_title('Ground Truth')\n",
        "            axes[0].axis('off')\n",
        "            \n",
        "            # Predictions\n",
        "            axes[1].imshow(image_np)\n",
        "            if len(prediction['boxes']) > 0:\n",
        "                for i, (box, score) in enumerate(zip(prediction['boxes'], prediction['scores'])):\n",
        "                    if score > 0.5:  # Only show confident predictions\n",
        "                        x1, y1, x2, y2 = box.cpu().numpy()\n",
        "                        rect = plt.Rectangle((x1, y1), x2-x1, y2-y1, \n",
        "                                           fill=False, color='red', linewidth=2)\n",
        "                        axes[1].add_patch(rect)\n",
        "            axes[1].set_title('Predictions')\n",
        "            axes[1].axis('off')\n",
        "            \n",
        "            plt.tight_layout()\n",
        "            \n",
        "            # Log to tensorboard\n",
        "            self.logger.experiment.add_figure(\n",
        "                'predictions', fig, self.current_epoch\n",
        "            )\n",
        "            plt.close(fig)\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"Error logging predictions: {e}\")\n",
        "    \n",
        "    def configure_optimizers(self):\n",
        "        \"\"\"Configure optimizer and scheduler.\"\"\"\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate, weight_decay=1e-4)\n",
        "        \n",
        "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            optimizer, mode='min', factor=0.5, patience=5, verbose=True\n",
        "        )\n",
        "        \n",
        "        return {\n",
        "            \"optimizer\": optimizer,\n",
        "            \"lr_scheduler\": {\n",
        "                \"scheduler\": scheduler,\n",
        "                \"monitor\": \"val_loss\",\n",
        "                \"interval\": \"epoch\",\n",
        "                \"frequency\": 1\n",
        "            }\n",
        "        }\n",
        "\n",
        "print(\"âœ… Model class defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 4. Training Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize data module\n",
        "data_module = VocalTrackDataModule(\n",
        "    data_dir=CONFIG[\"data_dir\"],\n",
        "    patient_data=PATIENT_DATA,\n",
        "    batch_size=CONFIG[\"batch_size\"],\n",
        "    num_workers=CONFIG[\"num_workers\"],\n",
        "    train_split=CONFIG[\"train_split\"],\n",
        "    augmentation_prob=CONFIG[\"augmentation_prob\"],\n",
        "    image_size=CONFIG[\"image_size\"]\n",
        ")\n",
        "\n",
        "# Prepare data\n",
        "print(\"ðŸ”„ Preparing data...\")\n",
        "data_module.prepare_data()\n",
        "data_module.setup(\"fit\")\n",
        "\n",
        "print(f\"âœ… Data module initialized\")\n",
        "print(f\"ðŸ“Š Classes: {len(data_module.get_classes())}\")\n",
        "print(f\"ðŸ“Š Train samples: {len(data_module.train_dataset) if hasattr(data_module, 'train_dataset') else 'Not set up'}\")\n",
        "print(f\"ðŸ“Š Val samples: {len(data_module.val_dataset) if hasattr(data_module, 'val_dataset') else 'Not set up'}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize model\n",
        "model = VocalTrackMaskRCNN(\n",
        "    num_classes=len(ANATOMICAL_CLASSES),\n",
        "    learning_rate=CONFIG[\"learning_rate\"],\n",
        "    trainable_backbone_layers=CONFIG[\"trainable_backbone_layers\"],\n",
        "    classification_loss_weight=CONFIG[\"classification_loss_weight\"],\n",
        "    box_regression_loss_weight=CONFIG[\"box_regression_loss_weight\"],\n",
        "    mask_loss_weight=CONFIG[\"mask_loss_weight\"],\n",
        "    rpn_objectness_loss_weight=CONFIG[\"rpn_objectness_loss_weight\"],\n",
        "    rpn_box_loss_weight=CONFIG[\"rpn_box_loss_weight\"]\n",
        ")\n",
        "\n",
        "print(f\"âœ… Model initialized\")\n",
        "print(f\"ðŸ§  Number of classes: {len(ANATOMICAL_CLASSES)}\")\n",
        "print(f\"ðŸ§  Trainable backbone layers: {CONFIG['trainable_backbone_layers']}\")\n",
        "\n",
        "# Count parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"ðŸ§  Total parameters: {total_params:,}\")\n",
        "print(f\"ðŸ§  Trainable parameters: {trainable_params:,}\")\n",
        "\n",
        "# Check if CUDA is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"ðŸ–¥ï¸ Using device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"ðŸ–¥ï¸ GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"ðŸ–¥ï¸ CUDA memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training setup\n",
        "from lightning.pytorch.callbacks import ModelCheckpoint, EarlyStopping, LearningRateMonitor\n",
        "from lightning.pytorch.loggers import TensorBoardLogger\n",
        "\n",
        "# Create callbacks\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    dirpath=CONFIG[\"output_dir\"],\n",
        "    filename=\"vocal_tract_maskrcnn_{epoch:02d}_{val_loss:.2f}\",\n",
        "    monitor=\"val_loss\",\n",
        "    mode=\"min\",\n",
        "    save_top_k=3,\n",
        "    save_last=True,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor=\"val_loss\",\n",
        "    mode=\"min\",\n",
        "    patience=CONFIG[\"patience\"],\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "lr_monitor = LearningRateMonitor(logging_interval=\"epoch\")\n",
        "\n",
        "# Create logger\n",
        "logger = TensorBoardLogger(\n",
        "    save_dir=CONFIG[\"output_dir\"],\n",
        "    name=\"vocal_tract_logs\",\n",
        "    version=None\n",
        ")\n",
        "\n",
        "# Create trainer\n",
        "trainer = L.Trainer(\n",
        "    max_epochs=CONFIG[\"max_epochs\"],\n",
        "    accelerator=\"auto\",\n",
        "    devices=\"auto\",\n",
        "    logger=logger,\n",
        "    callbacks=[checkpoint_callback, early_stopping, lr_monitor],\n",
        "    val_check_interval=1.0,\n",
        "    log_every_n_steps=10,\n",
        "    gradient_clip_val=1.0,  # Gradient clipping for stability\n",
        "    precision=\"16-mixed\" if torch.cuda.is_available() else \"32\",  # Mixed precision for speed\n",
        "    enable_progress_bar=True,\n",
        "    enable_model_summary=True\n",
        ")\n",
        "\n",
        "print(\"âœ… Trainer configured\")\n",
        "print(f\"ðŸƒ Max epochs: {CONFIG['max_epochs']}\")\n",
        "print(f\"ðŸƒ Early stopping patience: {CONFIG['patience']}\")\n",
        "print(f\"ðŸƒ Log directory: {logger.log_dir}\")\n",
        "\n",
        "# Display training summary\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"TRAINING SUMMARY\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Dataset: {len(data_module.fiftyone_dataset)} total samples\")\n",
        "print(f\"Classes: {len(ANATOMICAL_CLASSES)} anatomical structures\")\n",
        "print(f\"Image size: {CONFIG['image_size']}x{CONFIG['image_size']}\")\n",
        "print(f\"Batch size: {CONFIG['batch_size']}\")\n",
        "print(f\"Learning rate: {CONFIG['learning_rate']}\")\n",
        "print(f\"Device: {device}\")\n",
        "print(\"=\"*50)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Start training\n",
        "print(\"ðŸš€ Starting training...\")\n",
        "\n",
        "try:\n",
        "    # Fit the model\n",
        "    trainer.fit(model, data_module)\n",
        "    \n",
        "    print(\"âœ… Training completed!\")\n",
        "    print(f\"ðŸ“Š Best model saved to: {checkpoint_callback.best_model_path}\")\n",
        "    print(f\"ðŸ“Š Final validation loss: {checkpoint_callback.best_model_score:.4f}\")\n",
        "    \n",
        "except KeyboardInterrupt:\n",
        "    print(\"âš ï¸ Training interrupted by user\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"âŒ Training failed: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "\n",
        "# Save final model\n",
        "final_model_path = os.path.join(CONFIG[\"output_dir\"], \"vocal_tract_maskrcnn_final.ckpt\")\n",
        "trainer.save_checkpoint(final_model_path)\n",
        "print(f\"ðŸ’¾ Final model saved to: {final_model_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 5. Inference & Visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_trained_model(checkpoint_path: str) -> VocalTrackMaskRCNN:\n",
        "    \"\"\"Load trained model from checkpoint.\"\"\"\n",
        "    model = VocalTrackMaskRCNN.load_from_checkpoint(checkpoint_path)\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "\n",
        "def predict_on_image(model: VocalTrackMaskRCNN, image: np.ndarray, device: str = \"cuda\") -> Dict:\n",
        "    \"\"\"Run inference on a single image.\"\"\"\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    \n",
        "    # Preprocess image\n",
        "    transform = A.Compose([\n",
        "        A.Resize(CONFIG[\"image_size\"], CONFIG[\"image_size\"]),\n",
        "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        ToTensorV2(),\n",
        "    ])\n",
        "    \n",
        "    if len(image.shape) == 3 and image.shape[2] == 3:\n",
        "        transformed = transform(image=image)\n",
        "        image_tensor = transformed['image'].unsqueeze(0).to(device)\n",
        "    else:\n",
        "        raise ValueError(\"Image must be RGB with shape (H, W, 3)\")\n",
        "    \n",
        "    # Run inference\n",
        "    with torch.no_grad():\n",
        "        predictions = model([image_tensor])\n",
        "    \n",
        "    return predictions[0]\n",
        "\n",
        "\n",
        "def visualize_predictions(\n",
        "    image: np.ndarray,\n",
        "    predictions: Dict,\n",
        "    classes: List[str],\n",
        "    score_threshold: float = 0.5,\n",
        "    show_masks: bool = True,\n",
        "    show_boxes: bool = True\n",
        "):\n",
        "    \"\"\"Visualize model predictions on an image.\"\"\"\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(15, 7))\n",
        "    \n",
        "    # Original image\n",
        "    axes[0].imshow(image)\n",
        "    axes[0].set_title('Original Image')\n",
        "    axes[0].axis('off')\n",
        "    \n",
        "    # Predictions\n",
        "    axes[1].imshow(image)\n",
        "    \n",
        "    boxes = predictions['boxes'].cpu().numpy()\n",
        "    scores = predictions['scores'].cpu().numpy()\n",
        "    labels = predictions['labels'].cpu().numpy()\n",
        "    masks = predictions['masks'].cpu().numpy()\n",
        "    \n",
        "    # Filter by score threshold\n",
        "    keep = scores > score_threshold\n",
        "    boxes = boxes[keep]\n",
        "    scores = scores[keep]\n",
        "    labels = labels[keep]\n",
        "    masks = masks[keep]\n",
        "    \n",
        "    print(f\"Found {len(boxes)} detections above threshold {score_threshold}\")\n",
        "    \n",
        "    # Create a colormap for different classes\n",
        "    colors = plt.cm.tab20(np.linspace(0, 1, len(classes)))\n",
        "    \n",
        "    # Draw predictions\n",
        "    for i, (box, score, label, mask) in enumerate(zip(boxes, scores, labels, masks)):\n",
        "        color = colors[label % len(colors)]\n",
        "        \n",
        "        if show_boxes:\n",
        "            # Draw bounding box\n",
        "            x1, y1, x2, y2 = box\n",
        "            rect = plt.Rectangle((x1, y1), x2-x1, y2-y1, \n",
        "                               fill=False, color=color, linewidth=2)\n",
        "            axes[1].add_patch(rect)\n",
        "            \n",
        "            # Add label\n",
        "            label_text = f\"{classes[label]}: {score:.2f}\"\n",
        "            axes[1].text(x1, y1-5, label_text, color=color, fontsize=8,\n",
        "                        bbox=dict(boxstyle=\"round,pad=0.3\", facecolor='white', alpha=0.7))\n",
        "        \n",
        "        if show_masks and len(mask.shape) == 3:\n",
        "            # Draw mask\n",
        "            mask_binary = mask[0] > 0.5\n",
        "            colored_mask = np.zeros((*mask_binary.shape, 4))\n",
        "            colored_mask[mask_binary] = [*color[:3], 0.3]  # Semi-transparent\n",
        "            axes[1].imshow(colored_mask)\n",
        "    \n",
        "    axes[1].set_title(f'Predictions (>{score_threshold} confidence)')\n",
        "    axes[1].axis('off')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    return fig\n",
        "\n",
        "\n",
        "def test_on_sample():\n",
        "    \"\"\"Test the trained model on a sample from the dataset.\"\"\"\n",
        "    try:\n",
        "        # Load best model\n",
        "        if hasattr(checkpoint_callback, 'best_model_path') and checkpoint_callback.best_model_path:\n",
        "            model_path = checkpoint_callback.best_model_path\n",
        "        else:\n",
        "            model_path = final_model_path\n",
        "        \n",
        "        print(f\"Loading model from: {model_path}\")\n",
        "        trained_model = load_trained_model(model_path)\n",
        "        \n",
        "        # Get a sample from validation dataset\n",
        "        val_dataloader = data_module.val_dataloader()\n",
        "        sample_batch = next(iter(val_dataloader))\n",
        "        images, targets = sample_batch\n",
        "        \n",
        "        # Take first image\n",
        "        sample_image = images[0]\n",
        "        sample_target = targets[0]\n",
        "        \n",
        "        # Convert back to numpy for visualization\n",
        "        image_np = sample_image.permute(1, 2, 0).cpu().numpy()\n",
        "        image_np = (image_np * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406]))\n",
        "        image_np = np.clip(image_np, 0, 1)\n",
        "        \n",
        "        # Run prediction\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        predictions = predict_on_image(trained_model, image_np, device=device.type)\n",
        "        \n",
        "        # Visualize\n",
        "        fig = visualize_predictions(\n",
        "            image_np, \n",
        "            predictions, \n",
        "            ANATOMICAL_CLASSES,\n",
        "            score_threshold=0.3,\n",
        "            show_masks=True,\n",
        "            show_boxes=True\n",
        "        )\n",
        "        \n",
        "        plt.show()\n",
        "        \n",
        "        # Print detection summary\n",
        "        scores = predictions['scores'].cpu().numpy()\n",
        "        labels = predictions['labels'].cpu().numpy()\n",
        "        \n",
        "        print(\"\\nDetection Summary:\")\n",
        "        print(\"-\" * 40)\n",
        "        for score, label in zip(scores, labels):\n",
        "            if score > 0.3:\n",
        "                print(f\"{ANATOMICAL_CLASSES[label]}: {score:.3f}\")\n",
        "        \n",
        "        return trained_model, predictions\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Error in testing: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return None, None\n",
        "\n",
        "\n",
        "print(\"âœ… Inference utilities defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test the trained model\n",
        "print(\"ðŸ” Testing trained model on sample data...\")\n",
        "trained_model, sample_predictions = test_on_sample()\n",
        "\n",
        "if trained_model is not None:\n",
        "    print(\"âœ… Model testing completed successfully!\")\n",
        "    print(f\"ðŸŽ¯ Model ready for inference\")\n",
        "    \n",
        "    # Print class summary\n",
        "    print(\"\\nSupported anatomical classes:\")\n",
        "    print(\"-\" * 30)\n",
        "    for i, class_name in enumerate(ANATOMICAL_CLASSES):\n",
        "        print(f\"{i:2d}: {class_name}\")\n",
        "else:\n",
        "    print(\"âŒ Model testing failed\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Summary\n",
        "\n",
        "This clean implementation provides:\n",
        "\n",
        "### ðŸŽ¯ **Key Improvements Over Original**\n",
        "- **Consistent bbox format**: Pascal VOC (x_min, y_min, x_max, y_max) throughout\n",
        "- **Simplified architecture**: Removed unnecessary complexity and debugging code\n",
        "- **Better organization**: Clear separation of concerns and modular design\n",
        "- **Error handling**: Robust error handling and fallback mechanisms\n",
        "- **Documentation**: Clear docstrings and comments\n",
        "\n",
        "### ðŸ”§ **Usage**\n",
        "1. **Configure**: Modify `CONFIG` and `PATIENT_DATA` for your setup\n",
        "2. **Prepare data**: Ensure H5 files are in the expected format\n",
        "3. **Train**: Run all cells to train the model\n",
        "4. **Inference**: Use the provided functions for prediction\n",
        "\n",
        "### ðŸ“Š **Outputs**\n",
        "- **Model checkpoints**: Saved in `./outputs/`\n",
        "- **TensorBoard logs**: Available for monitoring training\n",
        "- **Visualizations**: Automatic prediction visualization during validation\n",
        "\n",
        "### ðŸŽ¨ **Customization**\n",
        "- **Classes**: Modify `ANATOMICAL_CLASSES` for your specific use case\n",
        "- **Augmentation**: Adjust `get_augmentation_pipeline()` parameters\n",
        "- **Architecture**: Customize model parameters in `VocalTrackMaskRCNN`\n",
        "- **Training**: Modify `CONFIG` for different training settings\n",
        "\n",
        "### ðŸ“ **Notes**\n",
        "- Uses Pascal VOC bbox format for consistency\n",
        "- Designed for medical imaging workflows\n",
        "- Supports multi-class anatomical structure detection\n",
        "- Includes automatic data validation and preprocessing\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
